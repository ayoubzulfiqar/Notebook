{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "**Linear Regression** is one of the simplest and most widely used supervised machine learning algorithms. It is used for predicting a continuous target variable (also called the dependent variable) based on one or more input features (independent variables). The relationship between the input features and the target variable is assumed to be **linear**.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Concepts of Linear Regression:\n",
    "1. **Equation of a Line**:\n",
    "   - In simple linear regression (with one feature), the relationship is represented by the equation:\n",
    "     $\n",
    "     y = mx + b\n",
    "     $\n",
    "     - $ y $: Target variable (output).\n",
    "     - $ x $: Feature (input).\n",
    "     - $ m $: Slope (weight of the feature).\n",
    "     - $ b $: Intercept (bias term).\n",
    "\n",
    "   - In multiple linear regression (with multiple features), the equation becomes:\n",
    "     $\n",
    "     y = b_0 + b_1x_1 + b_2x_2 + \\dots + b_nx_n\n",
    "     $\n",
    "     - $ b_0 $: Intercept.\n",
    "     - $ b_1, b_2, \\dots, b_n $: Coefficients (weights) for each feature.\n",
    "     - $ x_1, x_2, \\dots, x_n $: Input features.\n",
    "\n",
    "2. **Objective**:\n",
    "   - The goal of linear regression is to find the best-fitting line (or hyperplane in higher dimensions) that minimizes the difference between the predicted values and the actual values.\n",
    "\n",
    "3. **Cost Function**:\n",
    "   - The most common cost function used in linear regression is the **Mean Squared Error (MSE)**:\n",
    "     $\n",
    "     \\text{MSE} = \\frac{1}{N} \\sum_{i=1}^N (y_i - \\hat{y}_i)^2\n",
    "     $\n",
    "     - $ y_i $: Actual value.\n",
    "     - $ \\hat{y}_i $: Predicted value.\n",
    "     - $ N $: Number of data points.\n",
    "\n",
    "4. **Optimization**:\n",
    "   - The coefficients ($ b_0, b_1, \\dots, b_n $) are optimized using techniques like:\n",
    "     - **Ordinary Least Squares (OLS)**: A closed-form solution that minimizes the MSE.\n",
    "     - **Gradient Descent**: An iterative optimization algorithm used for large datasets or complex models.\n",
    "\n",
    "---\n",
    "\n",
    "### Assumptions of Linear Regression:\n",
    "1. **Linearity**: The relationship between the features and the target variable is linear.\n",
    "2. **Independence**: Observations are independent of each other (no autocorrelation).\n",
    "3. **Homoscedasticity**: The residuals (errors) have constant variance across all levels of the independent variables.\n",
    "4. **Normality**: The residuals are normally distributed (important for confidence intervals and hypothesis testing).\n",
    "5. **No Multicollinearity**: The independent variables are not highly correlated with each other.\n",
    "\n",
    "---\n",
    "\n",
    "### Types of Linear Regression:\n",
    "1. **Simple Linear Regression**:\n",
    "   - Only one input feature is used to predict the target variable.\n",
    "   - Example: Predicting house prices based on square footage.\n",
    "\n",
    "2. **Multiple Linear Regression**:\n",
    "   - Multiple input features are used to predict the target variable.\n",
    "   - Example: Predicting house prices based on square footage, number of bedrooms, and location.\n",
    "\n",
    "3. **Polynomial Regression**:\n",
    "   - A form of linear regression where the relationship between the independent variable and the dependent variable is modeled as an $ n $-degree polynomial.\n",
    "   - Example: $ y = b_0 + b_1x + b_2x^2 + \\dots + b_nx^n $.\n",
    "\n",
    "---\n",
    "\n",
    "### Advantages of Linear Regression:\n",
    "- Simple to understand and interpret.\n",
    "- Computationally efficient.\n",
    "- Works well when the relationship between variables is linear.\n",
    "\n",
    "---\n",
    "\n",
    "### Disadvantages of Linear Regression:\n",
    "- Assumes a linear relationship, which may not hold in real-world scenarios.\n",
    "- Sensitive to outliers.\n",
    "- Cannot handle complex relationships between variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'MachineLearning/Datasets/CSVs/lr.csv'\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "# data\n",
    "\n",
    "# Drop the missing values\n",
    "data = data.dropna()\n",
    "\n",
    "# training dataset and labels\n",
    "inputData = np.array(data.x[0:500]).reshape(500, 1)\n",
    "outputData = np.array(data.y[0:500]).reshape(500, 1)\n",
    "\n",
    "# valid dataset and labels\n",
    "test_input = np.array(data.x[500:700]).reshape(199, 1)\n",
    "test_output = np.array(data.y[500:700]).reshape(199, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "\n",
    "class LinearRegression: \n",
    "    def __init__(self): \n",
    "        self.parameters = {} \n",
    "\n",
    "    def forwardPropagation(self, inputData): \n",
    "        m = self.parameters['m'] \n",
    "        c = self.parameters['c'] \n",
    "        predictions = np.multiply(m, inputData) + c \n",
    "        return predictions \n",
    "\n",
    "    def costFunction(self, predictions, outputData): \n",
    "        cost = np.mean((outputData - predictions) ** 2) \n",
    "        return cost \n",
    "\n",
    "    def backwardPropagation(self, inputData, outputData, predictions): \n",
    "        derivatives = {} \n",
    "        df = (predictions-outputData) \n",
    "        # dm= 2/n * mean of (predictions-actual) * input \n",
    "        dm = 2 * np.mean(np.multiply(inputData, df)) \n",
    "        # dc = 2/n * mean of (predictions-actual) \n",
    "        dc = 2 * np.mean(df) \n",
    "        derivatives['dm'] = dm \n",
    "        derivatives['dc'] = dc \n",
    "        return derivatives \n",
    "\n",
    "    def updateParameters(self, derivatives, learningRate): \n",
    "        self.parameters['m'] = self.parameters['m'] - learningRate * derivatives['dm'] \n",
    "        self.parameters['c'] = self.parameters['c'] - learningRate * derivatives['dc'] \n",
    "\n",
    "    def train(self, inputData, outputData, learningRate, iters): \n",
    "        # Initialize random parameters \n",
    "        self.parameters['m'] = np.random.uniform(0, 1) * -1\n",
    "        self.parameters['c'] = np.random.uniform(0, 1) * -1\n",
    "\n",
    "        # Initialize loss \n",
    "        self.loss = [] \n",
    "\n",
    "        # Initialize figure and axis for animation \n",
    "        fig, ax = plt.subplots() \n",
    "        xValues = np.linspace(min(inputData), max(inputData), 100) \n",
    "        line, = ax.plot(xValues, self.parameters['m'] * xValues +\n",
    "                        self.parameters['c'], color='red', label='Regression Line') \n",
    "        ax.scatter(inputData, outputData, marker='o', \n",
    "                color='green', label='Training Data') \n",
    "\n",
    "        # Set y-axis limits to exclude negative values \n",
    "        ax.set_ylim(0, max(outputData) + 1) \n",
    "\n",
    "        def update(frame): \n",
    "            # Forward propagation \n",
    "            predictions = self.forwardPropagation(inputData) \n",
    "\n",
    "            # Cost function \n",
    "            cost = self.costFunction(predictions, outputData) \n",
    "\n",
    "            # Back propagation \n",
    "            derivatives = self.backwardPropagation( \n",
    "                inputData, outputData, predictions) \n",
    "\n",
    "            # Update parameters \n",
    "            self.updateParameters(derivatives, learningRate) \n",
    "\n",
    "            # Update the regression line \n",
    "            line.set_ydata(self.parameters['m'] \n",
    "                        * xValues + self.parameters['c']) \n",
    "\n",
    "            # Append loss and print \n",
    "            self.loss.append(cost) \n",
    "            print(\"Iteration = {}, Loss = {}\".format(frame + 1, cost)) \n",
    "\n",
    "            return line, \n",
    "        # Create animation \n",
    "        ani = FuncAnimation(fig, update, frames=iters, interval=200, blit=True) \n",
    "\n",
    "        # Save the animation as a video file (e.g., MP4) \n",
    "        ani.save('linear_regression_A.gif', writer='ffmpeg') \n",
    "\n",
    "        plt.xlabel('Input') \n",
    "        plt.ylabel('Output') \n",
    "        plt.title('Linear Regression') \n",
    "        plt.legend() \n",
    "        plt.show() \n",
    "\n",
    "        return self.parameters, self.loss \n",
    "\n",
    "\n",
    "lg = LinearRegression()\n",
    "parameters, loss = lg.train(inputData, outputData, 0.0001, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Polynomial Regression** is a form of regression analysis in which the relationship between the independent variable $ x $ and the dependent variable $ y $ is modeled as an $ n $-th degree polynomial. Unlike linear regression, which assumes a linear relationship between the variables, polynomial regression can capture nonlinear relationships.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Concepts of Polynomial Regression:\n",
    "\n",
    "1. **Polynomial Equation**:\n",
    "   - The general form of a polynomial regression model is:\n",
    "     $\n",
    "     y = b_0 + b_1x + b_2x^2 + \\dots + b_nx^n\n",
    "     $\n",
    "     - $ y $: Dependent variable (target).\n",
    "     - $ x $: Independent variable (feature).\n",
    "     - $ b_0, b_1, \\dots, b_n $: Coefficients.\n",
    "     - $ n $: Degree of the polynomial.\n",
    "\n",
    "2. **Nonlinear Relationships**:\n",
    "   - Polynomial regression can model curves, making it more flexible than linear regression.\n",
    "   - For example, a quadratic polynomial ($ n = 2 $) can model a parabolic relationship.\n",
    "\n",
    "3. **Higher Degrees**:\n",
    "   - Higher-degree polynomials can fit the training data more closely, but they may also overfit, capturing noise rather than the underlying pattern.\n",
    "\n",
    "4. **Applications**:\n",
    "   - Used in scenarios where the relationship between variables is inherently nonlinear, such as:\n",
    "     - Predicting growth rates.\n",
    "     - Modeling economic trends.\n",
    "     - Analyzing biological data.\n",
    "\n",
    "---\n",
    "\n",
    "### How Polynomial Regression Works:\n",
    "1. **Feature Transformation**:\n",
    "   - Polynomial regression is essentially a special case of **multiple linear regression**.\n",
    "   - The original feature $ x $ is transformed into polynomial features ($ x, x^2, x^3, \\dots, x^n $).\n",
    "   - These transformed features are then used as inputs to a linear regression model.\n",
    "\n",
    "2. **Model Training**:\n",
    "   - The coefficients ($ b_0, b_1, \\dots, b_n $) are learned using the **Ordinary Least Squares (OLS)** method or gradient descent.\n",
    "\n",
    "3. **Prediction**:\n",
    "   - Once the model is trained, it can predict $ y $ for new values of $ x $.\n",
    "\n",
    "---\n",
    "\n",
    "### Advantages of Polynomial Regression:\n",
    "- Can model nonlinear relationships.\n",
    "- Flexible and can fit a wide range of data patterns.\n",
    "- Easy to implement using libraries like Scikit-learn.\n",
    "\n",
    "---\n",
    "\n",
    "### Disadvantages of Polynomial Regression:\n",
    "- **Overfitting**: Higher-degree polynomials can fit the training data too closely, leading to poor generalization on unseen data.\n",
    "- **Sensitive to Outliers**: Polynomial models can be heavily influenced by outliers.\n",
    "- **Computationally Expensive**: Higher-degree polynomials require more computation.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"E:/Projects/Notebook/MachineLearning/Datasets/CSVs/data.csv\")\n",
    "X = data.iloc[:, 1:2].values\n",
    "y = data.iloc[:, 2].values\n",
    " \n",
    "    # degree of polynomial\n",
    "degree = 2\n",
    "xPoly = np.column_stack([X ** i for i in range(1, degree + 1)])  # Create polynomial features\n",
    "xPoly = np.hstack([np.ones((xPoly.shape[0], 1)), xPoly])  # Add a column of ones for the intercept\n",
    "\n",
    "# Step 3: Implement Polynomial Regression using Ordinary Least Squares (OLS)\n",
    "def polynomial_regression(X, y):\n",
    "    coefficients = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "    return coefficients\n",
    "\n",
    "# Train the model\n",
    "coefficients = polynomial_regression(xPoly, y)\n",
    "\n",
    "# Step 4: Make predictions\n",
    "def predict(X, coefficients):\n",
    "    return X @ coefficients\n",
    "\n",
    "yPred = predict(xPoly, coefficients)\n",
    "\n",
    "# Step 5: Evaluate the model\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "mse = mean_squared_error(y, yPred)\n",
    "print(f\"Coefficients: {coefficients}\")\n",
    "print(f\"Predictions: {yPred}\")\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "# Step 6: Visualize the results\n",
    "plt.scatter(X, y, color='blue', label='Actual Data')  # Plot actual data points\n",
    "plt.plot(X, yPred, color='red', label='Polynomial Regression')  # Plot regression curve\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Polynomial Regression (Degree = 2)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ridge Regression** and **Lasso Regression** are two popular regularization techniques used in linear regression to prevent overfitting and improve model generalization. Both methods add a penalty term to the loss function, but they differ in the type of penalty applied.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **Ridge Regression (L2 Regularization)**:\n",
    "Ridge Regression adds a **L2 penalty** (squared magnitude of coefficients) to the loss function. This penalty shrinks the coefficients but does not set them to zero.\n",
    "\n",
    "#### Key Features:\n",
    "- **Loss Function**:\n",
    "  $\n",
    "  \\text{Loss} = \\text{Mean Squared Error (MSE)} + \\lambda \\sum_{i=1}^n b_i^2\n",
    "  $\n",
    "  - $ \\lambda $: Regularization parameter (controls the strength of the penalty).\n",
    "  - $ b_i $: Coefficients of the model.\n",
    "\n",
    "- **Effect**:\n",
    "  - Shrinks the coefficients toward zero but does not eliminate them entirely.\n",
    "  - Helps reduce multicollinearity (when features are highly correlated).\n",
    "\n",
    "- **Use Case**:\n",
    "  - When you have many features and want to keep all of them in the model.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Lasso Regression (L1 Regularization)**:\n",
    "Lasso Regression adds an **L1 penalty** (absolute magnitude of coefficients) to the loss function. This penalty can shrink some coefficients to exactly zero, effectively performing feature selection.\n",
    "\n",
    "#### Key Features:\n",
    "- **Loss Function**:\n",
    "  $\n",
    "  \\text{Loss} = \\text{Mean Squared Error (MSE)} + \\lambda \\sum_{i=1}^n |b_i|\n",
    "  $\n",
    "  - $ \\lambda $: Regularization parameter.\n",
    "  - $ b_i $: Coefficients of the model.\n",
    "\n",
    "- **Effect**:\n",
    "  - Shrinks some coefficients to zero, effectively removing those features from the model.\n",
    "  - Performs feature selection, making the model simpler and more interpretable.\n",
    "\n",
    "- **Use Case**:\n",
    "  - When you have many features and want to select only the most important ones.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Differences Between Ridge and Lasso:\n",
    "\n",
    "| **Aspect**              | **Ridge Regression**                          | **Lasso Regression**                        |\n",
    "|-------------------------|-----------------------------------------------|---------------------------------------------|\n",
    "| **Penalty Term**        | L2 penalty ($ \\sum b_i^2 $)                 | L1 penalty ($ \\sum |b_i| $)             |\n",
    "| **Effect on Coefficients** | Shrinks coefficients but does not set to zero | Shrinks coefficients and can set to zero    |\n",
    "| **Feature Selection**   | No                                            | Yes                                         |\n",
    "| **Use Case**            | When all features are relevant                | When some features are irrelevant           |\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "\n",
    "### Key Parameters:\n",
    "- **$ \\lambda $ (alpha)**: Controls the strength of regularization.\n",
    "  - Higher $ \\lambda $: More regularization (smaller coefficients).\n",
    "  - Lower $ \\lambda $: Less regularization (closer to ordinary least squares).\n",
    "\n",
    "---\n",
    "\n",
    "### When to Use Ridge vs. Lasso:\n",
    "- Use **Ridge Regression** when:\n",
    "  - You have many features, and all of them are potentially relevant.\n",
    "  - You want to reduce multicollinearity.\n",
    "- Use **Lasso Regression** when:\n",
    "  - You have many features, and you suspect some are irrelevant.\n",
    "  - You want a simpler, more interpretable model.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example dataset\n",
    "data = {\n",
    "    'X1': [1, 2, 3, 4, 5],\n",
    "    'X2': [2, 3, 4, 5, 6],\n",
    "    'y': [2, 4, 5, 4, 5]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Extract features (X) and target (y)\n",
    "X = df[['X1', 'X2']].values\n",
    "y = df['y'].values\n",
    "\n",
    "# Add a column of ones for the intercept term\n",
    "X = np.hstack([np.ones((X.shape[0], 1)), X])\n",
    "\n",
    "# Ridge Regression implementation\n",
    "def ridgeRegression(X, y, alpha):\n",
    "    identityMatrix = np.eye(X.shape[1])  # Identity matrix\n",
    "    identityMatrix[0, 0] = 0  # Do not penalize the intercept term\n",
    "    coefficients = np.linalg.inv(X.T @ X + alpha * identityMatrix) @ X.T @ y\n",
    "    return coefficients\n",
    "\n",
    "# Train the model\n",
    "alpha = 1.0  # Regularization parameter\n",
    "coefficients = ridgeRegression(X, y, alpha)\n",
    "\n",
    "# Output results\n",
    "print(\"Ridge Regression Coefficients:\", coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lassoRegression(X, y, alpha, max_iter=1000, tol=1e-4):\n",
    "    nSamples, nFeatures = X.shape\n",
    "    coefficients = np.zeros(nFeatures)  # Initialize coefficients\n",
    "    for _ in range(max_iter):\n",
    "        for j in range(nFeatures):\n",
    "            # Update each coefficient using coordinate descent\n",
    "            X_j = X[:, j]\n",
    "            y_pred = X @ coefficients\n",
    "            r = y - y_pred + coefficients[j] * X_j\n",
    "            coefficients[j] = np.sign(r.T @ X_j) * max(np.abs(r.T @ X_j) - alpha, 0) / (X_j.T @ X_j)\n",
    "        # Check for convergence\n",
    "        if np.linalg.norm(X @ coefficients - y) < tol:\n",
    "            break\n",
    "    return coefficients\n",
    "\n",
    "# Train the model\n",
    "alpha = 0.1  # Regularization parameter\n",
    "coefficients = lassoRegression(X, y, alpha)\n",
    "\n",
    "# Output results\n",
    "print(\"Lasso Regression Coefficients:\", coefficients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Logistic Regression in Machine Learning**  \n",
    "\n",
    "Logistic Regression is a **supervised learning algorithm** used for **binary classification** tasks. It predicts the probability that an instance belongs to a particular class using the **sigmoid function**. Unlike linear regression, which outputs continuous values, logistic regression maps predictions to probabilities between **0 and 1**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Mathematical Formulation**  \n",
    "\n",
    "### **1. Hypothesis Function**\n",
    "Logistic Regression models the probability of a binary outcome as:\n",
    "\n",
    "$\n",
    "h_{\\theta}(x) = \\frac{1}{1 + e^{-\\theta^T x}}\n",
    "$\n",
    "\n",
    "Where:  \n",
    "- $ h_{\\theta}(x) $ is the predicted probability that $ y = 1 $.  \n",
    "- $ \\theta $ is the weight vector (parameters).  \n",
    "- $ x $ is the input feature vector.  \n",
    "- The function $ \\frac{1}{1 + e^{-z}} $ is called the **sigmoid function**.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Cost Function (Log Loss)**\n",
    "The cost function for logistic regression is derived from the **likelihood function** and is given by:\n",
    "\n",
    "$\n",
    "J(\\theta) = - \\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log(h_{\\theta}(x^{(i)})) + (1 - y^{(i)}) \\log(1 - h_{\\theta}(x^{(i)})) \\right]\n",
    "$\n",
    "\n",
    "Where:  \n",
    "- $ m $ is the number of training examples.  \n",
    "- $ y^{(i)} $ is the actual label (0 or 1) of the $ i $-th training sample.  \n",
    "- $ h_{\\theta}(x^{(i)}) $ is the predicted probability.\n",
    "\n",
    "This function is minimized using **Gradient Descent**.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Gradient Descent Update Rule**\n",
    "To update the parameters $ \\theta $, we compute the gradient:\n",
    "\n",
    "$\n",
    "\\theta_j := \\theta_j - \\alpha \\frac{1}{m} \\sum_{i=1}^{m} (h_{\\theta}(x^{(i)}) - y^{(i)}) x_j^{(i)}\n",
    "$\n",
    "\n",
    "Where:  \n",
    "- $ \\alpha $ is the **learning rate**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Assumptions of Logistic Regression**\n",
    "1. **Linear Relationship**: The independent variables are assumed to have a linear relationship with the **log odds** of the dependent variable.\n",
    "2. **Independent Observations**: Observations should be independent of each other.\n",
    "3. **No Multicollinearity**: Features should not be highly correlated.\n",
    "4. **Large Sample Size**: Logistic Regression performs better when there is sufficient data.\n",
    "5. **Absence of Outliers**: Outliers can impact the model’s performance.\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "\n",
    "## **Summary**\n",
    "1. **Logistic Regression** is a classification algorithm that predicts probabilities using the **sigmoid function**.\n",
    "2. The model is trained using **Gradient Descent**, minimizing the **log loss function**.\n",
    "3. Assumptions include **linear relationships in log-odds**, **no multicollinearity**, and **independent observations**.\n",
    "4. We implemented **Logistic Regression from scratch**, covering **sigmoid function, cost function, gradient descent, and prediction**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid Function\n",
    "\n",
    "\n",
    "\n",
    "def sigmoid(z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "def costFunction(x, y, theta):\n",
    "    m = len(y)\n",
    "    h =sigmoid(x.dot(theta))\n",
    "    cost = (-1/m) * np.sum(y * np.log(h) + (1 - y) * np.log(1 - h))\n",
    "    return cost\n",
    "\n",
    "def gradientDecent(x, y, theta, alpha, iterations=100):\n",
    "    # alpha is learning rate\n",
    "    m = len(y)\n",
    "    costHistory = []\n",
    "    for _ in range(iterations):\n",
    "        h = sigmoid(X.dot(theta))\n",
    "        gradient = (1/m) * X.T.dot(h - y)\n",
    "        theta -= alpha * gradient\n",
    "        costHistory.append(costFunction(X, y, theta))\n",
    "        return theta, costHistory\n",
    "def predict(X, theta):\n",
    "    probabilities = sigmoid(X.dot(theta))\n",
    "    return (probabilities >= 0.5).astype(int)\n",
    "\n",
    "# yPred = predict(0, 45)\n",
    "\n",
    "# accuracy = np.mean(yPred == y) * 100\n",
    "# print(f\"Model Accuracy: {accuracy:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Support Vector Machines (SVM) in Machine Learning**  \n",
    "\n",
    "Support Vector Machines (SVM) is a **supervised learning algorithm** used for both **classification** and **regression** tasks. However, it is primarily used for **binary classification** problems.  \n",
    "\n",
    "SVM finds an **optimal decision boundary** (hyperplane) that maximizes the **margin** between the two classes.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Mathematical Formulation of SVM**  \n",
    "\n",
    "### **1.1 Hyperplane Equation**  \n",
    "For an **n-dimensional** feature space, a hyperplane is represented as:  \n",
    "\n",
    "$\n",
    "w^T x + b = 0\n",
    "$\n",
    "\n",
    "Where:  \n",
    "- $ w $ is the weight vector (parameters of the model).  \n",
    "- $ x $ is the input feature vector.  \n",
    "- $ b $ is the bias term.  \n",
    "\n",
    "A **decision boundary** in SVM separates the two classes such that:\n",
    "\n",
    "$\n",
    "w^T x + b > 0 \\quad \\text{for class } +1\n",
    "$\n",
    "\n",
    "$\n",
    "w^T x + b < 0 \\quad \\text{for class } -1\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "### **1.2 Margin Maximization**  \n",
    "SVM tries to **maximize the margin** between the closest data points from both classes. These closest points are called **support vectors**.\n",
    "\n",
    "The margin is defined as:\n",
    "\n",
    "$\n",
    "\\frac{2}{\\|w\\|}\n",
    "$\n",
    "\n",
    "The goal is to **maximize** $ \\frac{2}{\\|w\\|} $, which is equivalent to **minimizing** $ \\frac{1}{2} \\|w\\|^2 $.\n",
    "\n",
    "Thus, the optimization problem becomes:\n",
    "\n",
    "$\n",
    "\\min \\frac{1}{2} \\|w\\|^2\n",
    "$\n",
    "\n",
    "subject to:\n",
    "\n",
    "$\n",
    "y_i (w^T x_i + b) \\geq 1 \\quad \\forall i\n",
    "$\n",
    "\n",
    "where $ y_i $ is the class label ($+1$ or $-1$).\n",
    "\n",
    "---\n",
    "\n",
    "### **1.3 Soft Margin SVM (Handling Misclassifications)**  \n",
    "For **non-linearly separable data**, we introduce **slack variables** $ \\xi_i $ to allow some misclassifications.\n",
    "\n",
    "The modified optimization problem:\n",
    "\n",
    "$\n",
    "\\min \\frac{1}{2} \\|w\\|^2 + C \\sum_{i=1}^{m} \\xi_i\n",
    "$\n",
    "\n",
    "subject to:\n",
    "\n",
    "$\n",
    "y_i (w^T x_i + b) \\geq 1 - \\xi_i, \\quad \\xi_i \\geq 0\n",
    "$\n",
    "\n",
    "Where:  \n",
    "- $ C $ is a regularization parameter that controls the trade-off between **maximizing the margin** and **minimizing misclassifications**.\n",
    "\n",
    "---\n",
    "\n",
    "### **1.4 Kernel Trick for Non-Linearly Separable Data**  \n",
    "For complex datasets, **SVM uses kernel functions** to map data to a **higher-dimensional space**, where it becomes linearly separable.\n",
    "\n",
    "Common kernel functions:\n",
    "- **Linear Kernel**: $ K(x_i, x_j) = x_i^T x_j $\n",
    "- **Polynomial Kernel**: $ K(x_i, x_j) = (x_i^T x_j + c)^d $\n",
    "- **Radial Basis Function (RBF) Kernel**: $ K(x_i, x_j) = \\exp(-\\gamma \\|x_i - x_j\\|^2) $\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Assumptions of SVM**\n",
    "1. **Binary Classification**: SVM is primarily designed for binary classification tasks.\n",
    "2. **Independent Features**: Features should be **independent** and **not highly correlated**.\n",
    "3. **Linearly Separable Data (For Linear SVM)**: Works best when data is **linearly separable**.\n",
    "4. **Proper Kernel Selection**: For non-linear data, an appropriate **kernel function** must be chosen.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Summary**\n",
    "1. **SVM** finds the best **hyperplane** to separate data while maximizing the **margin**.\n",
    "2. **Mathematics**:  \n",
    "   - Finds the optimal $ w, b $ by solving a **convex optimization problem**.  \n",
    "   - Uses **Lagrange multipliers** to handle constraints.  \n",
    "3. **Implementation Steps**:  \n",
    "   - **Data handling with pandas**  \n",
    "   - **Feature standardization**  \n",
    "   - **Gradient-based optimization** for finding the best decision boundary  \n",
    "4. **Advantages of SVM**:\n",
    "   - Works well for **small to medium datasets**.\n",
    "   - Effective in **high-dimensional spaces**.\n",
    "   - Handles **non-linearly separable data** using **kernels**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM:\n",
    "    def __init__(self, learningRate=0.001, lambdaParam=0.01, iterations=1000):\n",
    "        self.lr = learningRate\n",
    "        self.lambda_param = lambdaParam  # Regularization\n",
    "        self.iterations = iterations\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        m, n = X.shape\n",
    "        self.w = np.zeros(n)\n",
    "        self.b = 0\n",
    "        y = y.flatten()  # Ensure y is a 1D array\n",
    "\n",
    "        for _ in range(self.iterations):\n",
    "            for i in range(m):\n",
    "                condition = y[i] * (np.dot(X[i], self.w) + self.b) >= 1\n",
    "                if condition:\n",
    "                    self.w -= self.lr * (2 * self.lambda_param * self.w)\n",
    "                else:\n",
    "                    self.w -= self.lr * (2 * self.lambda_param * self.w - np.dot(X[i], y[i]))\n",
    "                    self.b -= self.lr * y[i]\n",
    "\n",
    "    def predict(self, X):\n",
    "        approx = np.dot(X, self.w) + self.b\n",
    "        return np.sign(approx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **k-Nearest Neighbors (k-NN) Algorithm: Instance-Based Learning Using Distance Metrics**  \n",
    "\n",
    "### **1. Introduction to k-NN**\n",
    "k-Nearest Neighbors (k-NN) is a **supervised learning algorithm** used for **classification** and **regression**. It is an **instance-based** (or **lazy learning**) algorithm, meaning it **does not build a model during training**. Instead, it **memorizes the training data** and makes predictions based on the **k nearest data points**.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. How k-NN Works**\n",
    "1. **Store Training Data**: k-NN does not learn a model but keeps all training data in memory.\n",
    "2. **Calculate Distance**: When a new test point needs classification, k-NN computes the distance between this point and all training points.\n",
    "3. **Find k Nearest Neighbors**: It selects the **k closest training points** based on a chosen distance metric.\n",
    "4. **Vote (For Classification) or Average (For Regression)**:\n",
    "   - **Classification**: Assigns the **most common label** among the k neighbors.\n",
    "   - **Regression**: Takes the **mean (or weighted mean)** of the k neighbors' values.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Distance Metrics Used in k-NN**\n",
    "To determine the nearest neighbors, k-NN uses various distance metrics:\n",
    "\n",
    "1. **Euclidean Distance** (Most Common):\n",
    "   $\n",
    "   d(x, y) = \\sqrt{\\sum (x_i - y_i)^2}\n",
    "   $\n",
    "   - Works well for **continuous numerical features**.\n",
    "\n",
    "2. **Manhattan Distance**:\n",
    "   $\n",
    "   d(x, y) = \\sum |x_i - y_i|\n",
    "   $\n",
    "   - Suitable when **data has high dimensions**.\n",
    "\n",
    "3. **Minkowski Distance** (Generalization of Euclidean and Manhattan):\n",
    "   $\n",
    "   d(x, y) = \\left(\\sum |x_i - y_i|^p \\right)^{\\frac{1}{p}}\n",
    "   $\n",
    "   - When $ p=2 $, it is **Euclidean Distance**.\n",
    "   - When $ p=1 $, it is **Manhattan Distance**.\n",
    "\n",
    "4. **Cosine Similarity** (for Text & High-Dimensional Data):\n",
    "   $\n",
    "   \\cos(\\theta) = \\frac{x \\cdot y}{\\|x\\| \\|y\\|}\n",
    "   $\n",
    "   - Measures **angle** rather than absolute distance.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Assumptions in k-NN**\n",
    "1. **Locally Similar Data**: Assumes that similar points are close together.\n",
    "2. **No Explicit Model**: k-NN is a **lazy learning algorithm** and does not create a mathematical model.\n",
    "3. **Feature Scaling Matters**: Since k-NN uses distances, **features must be normalized**.\n",
    "4. **Computational Complexity**: k-NN requires storing the full dataset, making it **slow for large datasets**.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Summary**\n",
    "### **Key Features of k-NN**\n",
    "1. **Lazy Learning**: No training phase; the model just stores data.\n",
    "2. **Distance-Based**: Predictions are based on distance metrics like **Euclidean or Manhattan**.\n",
    "3. **Highly Interpretable**: Easy to understand but computationally expensive for large datasets.\n",
    "\n",
    "### **Pros and Cons**\n",
    "| Pros | Cons |\n",
    "|------|------|\n",
    "| Simple to implement | Slow for large datasets |\n",
    "| Works well for low-dimensional data | Sensitive to irrelevant features |\n",
    "| No assumptions about data distribution | Requires proper feature scaling |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "class KNN:\n",
    "    def __init__(self, k=3, distance_metric=\"euclidean\"):\n",
    "        self.k = k\n",
    "        self.distance_metric = distance_metric\n",
    "\n",
    "    def _compute_distance(self, x1, x2):\n",
    "        if self.distance_metric == \"euclidean\":\n",
    "            return np.sqrt(np.sum((x1 - x2) ** 2))\n",
    "        elif self.distance_metric == \"manhattan\":\n",
    "            return np.sum(np.abs(x1 - x2))\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported distance metric\")\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        predictions = []\n",
    "        for x in X_test:\n",
    "            # Compute distances from x to all training samples\n",
    "            distances = [self._compute_distance(x, x_train) for x_train in self.X_train]\n",
    "\n",
    "            # Get the indices of k nearest neighbors\n",
    "            k_indices = np.argsort(distances)[:self.k]\n",
    "\n",
    "            # Get labels of the k nearest neighbors\n",
    "            k_nearest_labels = [self.y_train[i] for i in k_indices]\n",
    "\n",
    "            # Majority vote for classification\n",
    "            most_common = Counter(k_nearest_labels).most_common(1)[0][0]\n",
    "            predictions.append(most_common)\n",
    "\n",
    "        return np.array(predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Extending k-NN: Weighted k-NN and k-NN Regression**\n",
    "\n",
    "We will now extend our **k-NN implementation** to include:  \n",
    "1. **Weighted k-NN**: Assigns **higher weight to closer neighbors**.  \n",
    "2. **k-NN Regression**: Predicts **continuous values** instead of discrete classes.  \n",
    "\n",
    "---\n",
    "\n",
    "## **1. Weighted k-NN (Classification)**\n",
    "In standard k-NN, all neighbors have **equal influence** on classification. In **weighted k-NN**, closer neighbors have a **higher influence** using an inverse distance weighting:\n",
    "\n",
    "$\n",
    "w_i = \\frac{1}{d(x, x_i) + \\epsilon}\n",
    "$\n",
    "\n",
    "Where:  \n",
    "- $ w_i $ is the weight for neighbor $ i $.  \n",
    "- $ d(x, x_i) $ is the distance to neighbor $ i $.  \n",
    "- $ \\epsilon $ is a small constant to prevent division by zero.\n",
    "\n",
    "\n",
    "## **2. k-NN Regression**\n",
    "Instead of **voting**, k-NN regression **averages** the k nearest values:\n",
    "\n",
    "$\n",
    "\\hat{y} = \\frac{1}{k} \\sum_{i=1}^{k} y_i\n",
    "$\n",
    "\n",
    "For **weighted k-NN regression**, the formula is:\n",
    "\n",
    "$\n",
    "\\hat{y} = \\frac{\\sum w_i y_i}{\\sum w_i}\n",
    "$\n",
    "\n",
    "where $ w_i $ is the weight (inverse of distance).\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Summary**\n",
    "| Variant | Description | Pros | Cons |\n",
    "|---------|-------------|------|------|\n",
    "| **Standard k-NN Classification** | Majority voting among k neighbors | Simple, interpretable | Sensitive to irrelevant features |\n",
    "| **Weighted k-NN Classification** | Closer neighbors have higher weight | Better performance on noisy data | Sensitive to outliers |\n",
    "| **k-NN Regression** | Predicts mean of k nearest neighbors | Works well for smooth functions | Struggles with non-uniform data |\n",
    "| **Weighted k-NN Regression** | Uses inverse distance weighting | More accurate for non-uniform data | Prone to overfitting |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedKNN(KNN):  # Inherit from our previous KNN class\n",
    "    def predict(self, X_test):\n",
    "        predictions = []\n",
    "        for x in X_test:\n",
    "            distances = [self._compute_distance(x, x_train) for x_train in self.X_train]\n",
    "            k_indices = np.argsort(distances)[:self.k]\n",
    "            k_nearest_labels = [self.y_train[i] for i in k_indices]\n",
    "            k_nearest_distances = [distances[i] for i in k_indices]\n",
    "\n",
    "            # Compute weights (inverse distance)\n",
    "            weights = [1 / (d + 1e-5) for d in k_nearest_distances]\n",
    "\n",
    "            # Weighted vote for classification\n",
    "            weighted_votes = {}\n",
    "            for label, weight in zip(k_nearest_labels, weights):\n",
    "                if label not in weighted_votes:\n",
    "                    weighted_votes[label] = 0\n",
    "                weighted_votes[label] += weight\n",
    "\n",
    "            # Get label with highest weighted vote\n",
    "            most_common = max(weighted_votes, key=weighted_votes.get)\n",
    "            predictions.append(most_common)\n",
    "\n",
    "        return np.array(predictions)\n",
    "\n",
    "\n",
    "\n",
    "class KNNRegressor:\n",
    "    def __init__(self, k=3, distance_metric=\"euclidean\", weighted=False):\n",
    "        self.k = k\n",
    "        self.distance_metric = distance_metric\n",
    "        self.weighted = weighted\n",
    "\n",
    "    def _compute_distance(self, x1, x2):\n",
    "        if self.distance_metric == \"euclidean\":\n",
    "            return np.sqrt(np.sum((x1 - x2) ** 2))\n",
    "        elif self.distance_metric == \"manhattan\":\n",
    "            return np.sum(np.abs(x1 - x2))\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported distance metric\")\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        predictions = []\n",
    "        for x in X_test:\n",
    "            distances = [self._compute_distance(x, x_train) for x_train in self.X_train]\n",
    "            k_indices = np.argsort(distances)[:self.k]\n",
    "            k_nearest_values = [self.y_train[i] for i in k_indices]\n",
    "\n",
    "            if self.weighted:\n",
    "                k_nearest_distances = [distances[i] for i in k_indices]\n",
    "                weights = [1 / (d + 1e-5) for d in k_nearest_distances]\n",
    "                prediction = np.sum(np.array(weights) * np.array(k_nearest_values)) / np.sum(weights)\n",
    "            else:\n",
    "                prediction = np.mean(k_nearest_values)\n",
    "\n",
    "            predictions.append(prediction)\n",
    "\n",
    "        return np.array(predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Naïve Bayes: Probabilistic Classifier with Feature Independence Assumption**  \n",
    "\n",
    "## **1. Introduction to Naïve Bayes**  \n",
    "Naïve Bayes is a **probabilistic classifier** based on **Bayes' Theorem**. It is called \"naïve\" because it **assumes that features are independent**, which is often not true in real-world data but simplifies computations.  \n",
    "\n",
    "Naïve Bayes is widely used in **text classification, spam filtering, sentiment analysis, and medical diagnosis**.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Bayes' Theorem**  \n",
    "Bayes’ theorem provides a way to update probabilities based on new evidence:\n",
    "\n",
    "$\n",
    "P(Y | X) = \\frac{P(X | Y) P(Y)}{P(X)}\n",
    "$\n",
    "\n",
    "Where:\n",
    "- $ P(Y | X) $ = **Posterior probability** (probability of class $ Y $ given features $ X $).\n",
    "- $ P(X | Y) $ = **Likelihood** (probability of features $ X $ given class $ Y $).\n",
    "- $ P(Y) $ = **Prior probability** (probability of class $ Y $ before seeing data).\n",
    "- $ P(X) $ = **Evidence** (probability of features $ X $ occurring).\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Assumptions of Naïve Bayes**\n",
    "1. **Feature Independence**: Each feature contributes independently to the probability of a class.\n",
    "2. **Equal Importance**: All features have the same weight in classification.\n",
    "3. **Fixed Probabilities**: Probabilities are estimated from training data and do not change dynamically.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Types of Naïve Bayes Classifiers**\n",
    "1. **Gaussian Naïve Bayes**: Assumes features are **normally distributed** (for continuous data).\n",
    "2. **Multinomial Naïve Bayes**: Used for **text classification** (word counts in documents).\n",
    "3. **Bernoulli Naïve Bayes**: Used for **binary features** (e.g., presence/absence of a word).\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### **Step 4: Implement Naïve Bayes (Gaussian)**\n",
    "#### **Gaussian Naïve Bayes Assumes Each Feature Follows a Normal Distribution**:\n",
    "$\n",
    "P(X | Y) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left(-\\frac{(X - \\mu)^2}{2 \\sigma^2}\\right)\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Summary**\n",
    "| Variant | Description | Use Cases |\n",
    "|---------|-------------|------------|\n",
    "| **Gaussian Naïve Bayes** | Assumes normal distribution of features | Medical diagnosis, image classification |\n",
    "| **Multinomial Naïve Bayes** | Used for word frequency data | Text classification, spam filtering |\n",
    "| **Bernoulli Naïve Bayes** | Works with binary features | Sentiment analysis, fraud detection |\n",
    "\n",
    "### **Pros and Cons**\n",
    "| Pros | Cons |\n",
    "|------|------|\n",
    "| Simple, fast, and scalable | Assumes feature independence |\n",
    "| Works well with high-dimensional data | Poor performance if features are correlated |\n",
    "| Requires very little training data | Sensitive to zero probabilities |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianNaiveBayes:\n",
    "    def __init__(self):\n",
    "        self.classes = None\n",
    "        self.class_priors = {}\n",
    "        self.means = {}\n",
    "        self.variances = {}\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        self.classes = np.unique(y_train)  # Get unique class labels\n",
    "        \n",
    "        for cls in self.classes:\n",
    "            X_c = X_train[y_train == cls]  # Subset data for each class\n",
    "            \n",
    "            # Calculate mean and variance for each feature\n",
    "            self.means[cls] = np.mean(X_c, axis=0)\n",
    "            self.variances[cls] = np.var(X_c, axis=0)\n",
    "            \n",
    "            # Calculate prior probability P(Y)\n",
    "            self.class_priors[cls] = X_c.shape[0] / X_train.shape[0]\n",
    "\n",
    "    def _calculate_likelihood(self, x, mean, var):\n",
    "        \"\"\"Compute Gaussian likelihood using Normal Distribution formula.\"\"\"\n",
    "        eps = 1e-9  # Avoid division by zero\n",
    "        coef = 1 / np.sqrt(2 * np.pi * var + eps)\n",
    "        exponent = np.exp(- (x - mean) ** 2 / (2 * var + eps))\n",
    "        return coef * exponent\n",
    "\n",
    "    def _calculate_posterior(self, x):\n",
    "        \"\"\"Calculate P(Y | X) for each class.\"\"\"\n",
    "        posteriors = {}\n",
    "        \n",
    "        for cls in self.classes:\n",
    "            prior = np.log(self.class_priors[cls])  # Log probability for numerical stability\n",
    "            likelihoods = np.sum(np.log(self._calculate_likelihood(x, self.means[cls], self.variances[cls])))\n",
    "            posteriors[cls] = prior + likelihoods  # Apply Bayes' rule\n",
    "            \n",
    "        return max(posteriors, key=posteriors.get)  # Return class with max probability\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        return np.array([self._calculate_posterior(x) for x in X_test])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Multinomial Naïve Bayes (For Text Classification)**  \n",
    "In Multinomial Naïve Bayes, we assume that **features represent counts** (e.g., word frequencies in text classification). The likelihood is calculated as:\n",
    "\n",
    "$\n",
    "P(X | Y) = \\prod_{i=1}^{n} P(x_i | Y)\n",
    "$\n",
    "\n",
    "Where $ P(x_i | Y) $ is estimated as:\n",
    "\n",
    "$\n",
    "P(x_i | Y) = \\frac{count(x_i, Y) + \\alpha}{\\sum_{j} count(x_j, Y) + \\alpha V}\n",
    "$\n",
    "\n",
    "- $ count(x_i, Y) $ = number of times feature $ x_i $ appears in class $ Y $  \n",
    "- $ V $ = vocabulary size  \n",
    "- $ \\alpha $ = Laplace smoothing parameter (usually $ \\alpha = 1 $)  \n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Laplace Smoothing**\n",
    "Laplace smoothing prevents zero probabilities. The formula for **adjusted probability** is:\n",
    "\n",
    "$\n",
    "P(x_i | Y) = \\frac{count(x_i, Y) + \\alpha}{\\sum_{j} count(x_j, Y) + \\alpha V}\n",
    "$\n",
    "\n",
    "Where $ \\alpha > 0 $ ensures all probabilities are non-zero.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class MultinomialNaiveBayes:\n",
    "    def __init__(self, alpha=1.0):\n",
    "        self.alpha = alpha  # Laplace smoothing parameter\n",
    "        self.class_priors = {}\n",
    "        self.word_counts = {}\n",
    "        self.vocab = set()\n",
    "        self.class_word_totals = {}\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        self.classes = np.unique(y_train)\n",
    "        self.class_priors = {cls: np.mean(y_train == cls) for cls in self.classes}\n",
    "        self.word_counts = {cls: defaultdict(int) for cls in self.classes}\n",
    "        self.class_word_totals = {cls: 0 for cls in self.classes}\n",
    "\n",
    "        for text, cls in zip(X_train, y_train):\n",
    "            words = text.split()\n",
    "            for word in words:\n",
    "                self.word_counts[cls][word] += 1\n",
    "                self.class_word_totals[cls] += 1\n",
    "                self.vocab.add(word)\n",
    "\n",
    "    def _calculate_posterior(self, text, cls):\n",
    "        words = text.split()\n",
    "        log_prob = np.log(self.class_priors[cls])\n",
    "        V = len(self.vocab)\n",
    "\n",
    "        for word in words:\n",
    "            word_freq = self.word_counts[cls].get(word, 0)\n",
    "            prob = (word_freq + self.alpha) / (self.class_word_totals[cls] + self.alpha * V)\n",
    "            log_prob += np.log(prob)\n",
    "\n",
    "        return log_prob\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        predictions = []\n",
    "        for text in X_test:\n",
    "            posteriors = {cls: self._calculate_posterior(text, cls) for cls in self.classes}\n",
    "            predictions.append(max(posteriors, key=posteriors.get))\n",
    "        return np.array(predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Decision Trees: Hierarchical Classification Based on Feature Splits**\n",
    "\n",
    "### **1. Mathematical Foundation**  \n",
    "\n",
    "A **Decision Tree** is a hierarchical structure where data is recursively split based on feature values to form a tree-like structure. The goal is to **maximize information gain** at each split.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Splitting Criteria**  \n",
    "\n",
    "At each node, we choose the best feature to split using **Impurity Measures**:\n",
    "\n",
    "#### **(a) Gini Impurity (for Classification)**\n",
    "$\n",
    "Gini(D) = 1 - \\sum_{i=1}^{c} p_i^2\n",
    "$\n",
    "Where:\n",
    "- $ p_i $ is the proportion of class $ i $ in dataset $ D $.  \n",
    "- Lower **Gini** means purer nodes.\n",
    "\n",
    "#### **(b) Entropy (for Classification)**\n",
    "$\n",
    "Entropy(D) = -\\sum_{i=1}^{c} p_i \\log_2 p_i\n",
    "$\n",
    "- Measures **disorder** in the dataset.\n",
    "- Lower entropy means purer splits.\n",
    "\n",
    "#### **(c) Mean Squared Error (for Regression)**\n",
    "$\n",
    "MSE = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y})^2\n",
    "$\n",
    "- Used in **Regression Trees**.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Information Gain (IG)**\n",
    "The best split is chosen by **maximizing Information Gain**:\n",
    "\n",
    "$\n",
    "IG = I(D) - \\sum_{j} \\frac{|D_j|}{|D|} I(D_j)\n",
    "$\n",
    "\n",
    "Where:\n",
    "- $ I(D) $ = Impurity of the parent node.\n",
    "- $ D_j $ = Subsets after the split.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### **6. Summary**\n",
    "\n",
    "| **Criterion** | **Formula** | **Use Case** |\n",
    "|--------------|------------|-------------|\n",
    "| **Gini Impurity** | $ 1 - \\sum p_i^2 $ | Faster computation, common in classification |\n",
    "| **Entropy** | $ -\\sum p_i \\log_2 p_i $ | More interpretable, used in information theory |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=5, min_samples_split=2, criterion=\"gini\"):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.criterion = criterion\n",
    "        self.tree = None\n",
    "\n",
    "    def _gini(self, y):\n",
    "        classes, counts = np.unique(y, return_counts=True)\n",
    "        p = counts / counts.sum()\n",
    "        return 1 - np.sum(p ** 2)\n",
    "\n",
    "    def _entropy(self, y):\n",
    "        classes, counts = np.unique(y, return_counts=True)\n",
    "        p = counts / counts.sum()\n",
    "        return -np.sum(p * np.log2(p + 1e-9))\n",
    "\n",
    "    def _best_split(self, X, y):\n",
    "        best_gain, best_feature, best_value = 0, None, None\n",
    "        parent_impurity = self._gini(y) if self.criterion == \"gini\" else self._entropy(y)\n",
    "        \n",
    "        for feature in range(X.shape[1]):\n",
    "            values = np.unique(X[:, feature])\n",
    "            for value in values:\n",
    "                left_mask, right_mask = X[:, feature] <= value, X[:, feature] > value\n",
    "                if sum(left_mask) < self.min_samples_split or sum(right_mask) < self.min_samples_split:\n",
    "                    continue\n",
    "                \n",
    "                left_impurity = self._gini(y[left_mask]) if self.criterion == \"gini\" else self._entropy(y[left_mask])\n",
    "                right_impurity = self._gini(y[right_mask]) if self.criterion == \"gini\" else self._entropy(y[right_mask])\n",
    "                \n",
    "                weighted_impurity = (sum(left_mask) * left_impurity + sum(right_mask) * right_impurity) / len(y)\n",
    "                info_gain = parent_impurity - weighted_impurity\n",
    "\n",
    "                if info_gain > best_gain:\n",
    "                    best_gain, best_feature, best_value = info_gain, feature, value\n",
    "        \n",
    "        return best_feature, best_value\n",
    "\n",
    "    def _build_tree(self, X, y, depth=0):\n",
    "        if len(np.unique(y)) == 1 or depth >= self.max_depth:\n",
    "            return np.argmax(np.bincount(y))\n",
    "\n",
    "        feature, value = self._best_split(X, y)\n",
    "        if feature is None:\n",
    "            return np.argmax(np.bincount(y))\n",
    "\n",
    "        left_mask, right_mask = X[:, feature] <= value, X[:, feature] > value\n",
    "        left_branch = self._build_tree(X[left_mask], y[left_mask], depth + 1)\n",
    "        right_branch = self._build_tree(X[right_mask], y[right_mask], depth + 1)\n",
    "\n",
    "        return {\"feature\": feature, \"value\": value, \"left\": left_branch, \"right\": right_branch}\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.tree = self._build_tree(X, y)\n",
    "\n",
    "    def _predict_sample(self, x, node):\n",
    "        if isinstance(node, int):\n",
    "            return node\n",
    "        return self._predict_sample(x, node[\"left\"] if x[node[\"feature\"]] <= node[\"value\"] else node[\"right\"])\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._predict_sample(x, self.tree) for x in X])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Random Forest: Ensemble of Decision Trees for Improved Accuracy and Robustness**  \n",
    "\n",
    "### **1. Mathematical Foundation**  \n",
    "\n",
    "**Random Forest** is an **ensemble learning method** that combines multiple **Decision Trees** to improve accuracy and reduce overfitting. It uses **Bagging (Bootstrap Aggregation)** and **Random Feature Selection** to build diverse trees.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Key Concepts**  \n",
    "\n",
    "#### **(a) Bagging (Bootstrap Aggregation)**\n",
    "Each tree is trained on a **random subset** of the training data (sampling with replacement).\n",
    "\n",
    "$\n",
    "D_i \\sim \\text{Bootstrap Sample from } D\n",
    "$\n",
    "\n",
    "#### **(b) Random Feature Selection**\n",
    "Each tree is trained using only a **random subset of features**. If there are $ M $ total features, a subset of size $ \\sqrt{M} $ (for classification) or $ M/3 $ (for regression) is used.\n",
    "\n",
    "#### **(c) Final Prediction (Voting/Averaging)**\n",
    "For **classification**, the majority vote is taken:\n",
    "\n",
    "$\n",
    "\\hat{Y} = \\arg\\max_k \\sum_{i=1}^{T} \\mathbb{1}(h_i(X) = k)\n",
    "$\n",
    "\n",
    "For **regression**, the average prediction is taken:\n",
    "\n",
    "$\n",
    "\\hat{Y} = \\frac{1}{T} \\sum_{i=1}^{T} h_i(X)\n",
    "$\n",
    "\n",
    "Where $ T $ is the number of trees, and $ h_i(X) $ is the prediction from the $ i $-th tree.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Summary**\n",
    "| **Concept** | **Explanation** |\n",
    "|------------|----------------|\n",
    "| **Bagging** | Trains each tree on a different random sample of data |\n",
    "| **Random Feature Selection** | Each tree is trained on a subset of features |\n",
    "| **Voting (Classification)** | Final prediction is based on majority vote |\n",
    "| **Averaging (Regression)** | Final prediction is the average output of all trees |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest:\n",
    "    def __init__(self, n_trees=10, max_depth=5, min_samples_split=2, criterion=\"gini\"):\n",
    "        self.n_trees = n_trees\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.criterion = criterion\n",
    "        self.trees = []\n",
    "\n",
    "    def _bootstrap_sample(self, X, y):\n",
    "        indices = np.random.choice(len(X), size=len(X), replace=True)\n",
    "        return X[indices], y[indices]\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.trees = []\n",
    "        for _ in range(self.n_trees):\n",
    "            X_sample, y_sample = self._bootstrap_sample(X, y)\n",
    "            # Using DecisionTree\n",
    "            tree = DecisionTree(max_depth=self.max_depth, min_samples_split=self.min_samples_split, criterion=self.criterion)\n",
    "            tree.fit(X_sample, y_sample)\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = np.array([tree.predict(X) for tree in self.trees])\n",
    "        return np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=0, arr=predictions)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
