{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Algorithms for Data Processing**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Data Cleaning**  \n",
    "- Handling Missing Values:  \n",
    "  - Mean/Median/Mode Imputation  \n",
    "  - K-Nearest Neighbors (KNN) Imputation  \n",
    "  - Interpolation  \n",
    "- Outlier Detection and Removal:  \n",
    "  - Z-score Method  \n",
    "  - IQR (Inter-quartile Range) Method  \n",
    "  - Isolation Forest  \n",
    "  - Local Outlier Factor (LOF)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Mean Imputation**\n",
    "Mean imputation is a technique used to handle missing values in a dataset by replacing them with the **mean** (average) of the available data for that feature. It is commonly applied to numerical data. The mean is calculated as the sum of all values divided by the number of values. This method assumes that the data is normally distributed and does not have significant skewness or outliers.\n",
    "\n",
    "---\n",
    "\n",
    "## **Median Imputation**\n",
    "Median imputation replaces missing values with the **median** (middle value) of the available data for that feature. The median is the value that separates the higher half of the data from the lower half. This method is particularly useful for numerical data that is skewed or contains outliers, as the median is less sensitive to extreme values compared to the mean.\n",
    "\n",
    "---\n",
    "\n",
    "## **Mode Imputation**\n",
    "Mode imputation is used to replace missing values with the **mode** (most frequent value) of the available data for that feature. This technique is typically applied to categorical data or discrete numerical data where a clear dominant value exists. The mode represents the value that appears most frequently in the dataset.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Median Mode Imputations\n",
    "\n",
    "# Sample data\n",
    "data = {'A': [1, 2, np.nan, 4, 5]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Mean imputation\n",
    "meanVal = df['A'].mean()\n",
    "df['A']= df['A'].fillna(meanVal)\n",
    "print(\"Mean:\\n\",df)\n",
    "\n",
    "# Median Imputation\n",
    "medianVal = df['A'].median()\n",
    "df['A']= df['A'].fillna(medianVal)\n",
    "print(\"Median:\\n\",df)\n",
    "\n",
    "# Mode\n",
    "modVal = df['A'].mod(0)\n",
    "df['A']= df['A'].fillna(modVal)\n",
    "print(\"Mode:\\n\",df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### **K-Nearest Neighbors (KNN) Imputation**\n",
    "KNN imputation is a technique used to handle missing values by replacing them with values from the **k-nearest neighbors** in the dataset. It leverages the idea that similar data points (rows) should have similar values for their features.\n",
    "\n",
    "---\n",
    "\n",
    "### **How KNN Imputation Works**\n",
    "1. **Step 1: Identify Missing Values**  \n",
    "   Locate the missing values in the dataset.\n",
    "\n",
    "2. **Step 2: Compute Distances**  \n",
    "   For each row with missing values, compute the distance (e.g., Euclidean, Manhattan) to all other rows using the available features.\n",
    "\n",
    "3. **Step 3: Select Nearest Neighbors**  \n",
    "   Identify the **k-nearest neighbors** (rows) with the smallest distances.\n",
    "\n",
    "4. **Step 4: Impute Missing Values**  \n",
    "   - For **numerical features**: Replace the missing value with the **mean** or **median** of the corresponding feature values from the k-nearest neighbors.\n",
    "   - For **categorical features**: Replace the missing value with the **mode** (most frequent value) of the corresponding feature values from the k-nearest neighbors.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Parameters**\n",
    "- **n_neighbors (k)**: The number of neighbors to consider. A smaller k may overfit, while a larger k may smooth out patterns.\n",
    "- **Distance Metric**: Common metrics include Euclidean, Manhattan, or Minkowski distance.\n",
    "- **Weights**: Neighbors can be weighted by their distance (closer neighbors have more influence).\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### **Advantages of KNN Imputation**\n",
    "- Preserves relationships between features.\n",
    "- Works well for datasets with complex patterns.\n",
    "- Flexible for both numerical and categorical data.\n",
    "\n",
    "---\n",
    "\n",
    "### **Disadvantages of KNN Imputation**\n",
    "- Computationally expensive for large datasets.\n",
    "- Sensitive to the choice of k and distance metric.\n",
    "- Requires scaling of features for accurate distance calculations.\n",
    "\n",
    "---\n",
    "\n",
    "[Code](https://www.geeksforgeeks.org/k-nearest-neighbours/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def euclideanDistance(a, b):\n",
    "    \"\"\"Compute Euclidean distance between two vectors, ignoring NaNs.\"\"\"\n",
    "    mask = ~np.isnan(a) & ~np.isnan(b)  # Consider only non-NaN values\n",
    "    if not np.any(mask):\n",
    "        return np.inf  # If all values are NaN, return infinity\n",
    "    return np.sqrt(np.sum((a[mask] - b[mask])**2))\n",
    "\n",
    "def kNN(X, k=3):\n",
    "    \"\"\"KNN Imputation for missing values in a NumPy array.\"\"\"\n",
    "    xImputed = X.copy()  # Copy of original data\n",
    "    nRows, nColumns = X.shape\n",
    "\n",
    "    for rowIndex in range(nRows):\n",
    "        for columnIndex in range(nColumns):\n",
    "            if np.isnan(X[rowIndex, columnIndex]):  # Check for missing value\n",
    "                distances = []\n",
    "                \n",
    "                # Find K nearest neighbors\n",
    "                for neighborIndex in range(nRows):\n",
    "                    if neighborIndex != rowIndex and not np.isnan(X[neighborIndex, columnIndex]):\n",
    "                        dist = euclideanDistance(X[rowIndex], X[neighborIndex])\n",
    "                        distances.append((dist, X[neighborIndex, columnIndex]))\n",
    "\n",
    "                # Sort by distance and select K closest\n",
    "                distances.sort(key=lambda x: x[0])\n",
    "                kNeighbors = [val for _, val in distances[:k]]\n",
    "\n",
    "                # Impute missing value with mean of K neighbors\n",
    "                if kNeighbors:\n",
    "                    xImputed[rowIndex, columnIndex] = np.mean(kNeighbors)\n",
    "\n",
    "    return xImputed\n",
    "\n",
    "# Example dataset with missing values (NaN)\n",
    "X = np.array([[1.0, 2.0, np.nan],\n",
    "              [2.0, np.nan, 3.0],\n",
    "              [np.nan, 4.0, 5.0],\n",
    "              [3.0, 4.0, 6.0]])\n",
    "\n",
    "# Apply KNN imputation\n",
    "X_imputed = kNN(X, k=2)\n",
    "\n",
    "print(\"Original Data with Missing Values:\")\n",
    "print(X)\n",
    "\n",
    "print(\"\\nImputed Data:\")\n",
    "print(X_imputed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Algorithm forInterpolation**  \n",
    "\n",
    "Interpolation in a **matrix (2D data)** depends on the nature of missing values and the structure of the data. The best algorithm depends on:  \n",
    "\n",
    "1. **Smoothness of Data**  \n",
    "   - If data follows a smooth trend → **Spline Interpolation or Bicubic Interpolation**  \n",
    "   - If data has sudden jumps → **Nearest-Neighbor Interpolation**  \n",
    "\n",
    "2. **Computational Efficiency**  \n",
    "   - If speed is important → **Linear Interpolation or Nearest-Neighbor**  \n",
    "   - If accuracy is important → **Polynomial or Spline Interpolation**  \n",
    "\n",
    "---\n",
    "\n",
    "### **Common Matrix Interpolation Methods**  \n",
    "\n",
    "| Algorithm | Description | Best For |\n",
    "|-----------|-------------|----------|\n",
    "| **Bilinear Interpolation** | Uses linear interpolation in both row and column directions | Image resizing, smooth surfaces |\n",
    "| **Bicubic Interpolation** | Uses cubic polynomials for smoother results | High-quality image scaling |\n",
    "| **Nearest-Neighbor Interpolation** | Uses the closest available value | Discrete data, quick estimation |\n",
    "| **Spline Interpolation** | Fits smooth curves across the data points | Geospatial data, scientific applications |\n",
    "| **Kriging Interpolation** | A geostatistical method that models spatial correlation | Geographic and environmental data |\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### **Best Choice for Matrix Interpolation**\n",
    "| **Scenario** | **Best Algorithm** | **Best Library** |\n",
    "|-------------|----------------|---------------|\n",
    "| **Image Processing** | Bicubic Interpolation | OpenCV (`cv2`) |\n",
    "| **Smooth Data (Geospatial, Climate)** | Kriging, Spline | SciPy (`scipy.interpolate`) |\n",
    "| **Quick Estimation** | Nearest-Neighbor | NumPy (`numpy.interp`) |\n",
    "| **General Purpose** | Bilinear, Bicubic | SciPy (`griddata`) |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def linearInterpolation(X):\n",
    "    \"\"\"Perform linear interpolation on 1D NumPy array with missing values (NaN).\"\"\"\n",
    "    n = len(X)\n",
    "    for i in range(n):\n",
    "        if np.isnan(X[i]):  # If missing value found\n",
    "            left, right = None, None\n",
    "\n",
    "            # Find the nearest left non-NaN value\n",
    "            for k in range(i - 1, -1, -1):\n",
    "                if not np.isnan(X[k]):\n",
    "                    left = (k, X[k])\n",
    "                    break\n",
    "\n",
    "            # Find the nearest right non-NaN value\n",
    "            for k in range(i + 1, n):\n",
    "                if not np.isnan(X[k]):\n",
    "                    right = (k, X[k])\n",
    "                    break\n",
    "\n",
    "            # If both left and right exist, apply linear interpolation\n",
    "            if left and right:\n",
    "                x1, y1 = left\n",
    "                x2, y2 = right\n",
    "                X[i] = y1 + (y2 - y1) * (i - x1) / (x2 - x1)\n",
    "\n",
    "    return X\n",
    "\n",
    "def nearestNeighborInterpolation(X):\n",
    "    \"\"\"Perform nearest-neighbor interpolation on 1D NumPy array with missing values.\"\"\"\n",
    "    n = len(X)\n",
    "    for i in range(n):\n",
    "        if np.isnan(X[i]):  # If missing value found\n",
    "            left, right = None, None\n",
    "\n",
    "            # Find the nearest left non-NaN value\n",
    "            for k in range(i - 1, -1, -1):\n",
    "                if not np.isnan(X[k]):\n",
    "                    left = X[k]\n",
    "                    break\n",
    "\n",
    "            # Find the nearest right non-NaN value\n",
    "            for k in range(i + 1, n):\n",
    "                if not np.isnan(X[k]):\n",
    "                    right = X[k]\n",
    "                    break\n",
    "\n",
    "            # Use the nearest available value\n",
    "            if left is not None and right is not None:\n",
    "                X[i] = left if (i - k) < (k - i) else right\n",
    "            elif left is not None:\n",
    "                X[i] = left\n",
    "            elif right is not None:\n",
    "                X[i] = right\n",
    "\n",
    "    return X\n",
    "\n",
    "def polynomialInterpolation(X, degree=2):\n",
    "    \"\"\"Perform polynomial interpolation on a 1D NumPy array with missing values.\"\"\"\n",
    "    xKnown = np.where(~np.isnan(X))[0]  # Indices of known values\n",
    "    yKnown = X[xKnown]  # Known values\n",
    "    xMissing = np.where(np.isnan(X))[0]  # Indices of missing values\n",
    "\n",
    "    # Fit a polynomial curve to the known data points\n",
    "    polyCoefficients = np.polyfit(xKnown, yKnown, degree)\n",
    "    poly_func = np.poly1d(polyCoefficients)\n",
    "\n",
    "    # Predict missing values using the polynomial function\n",
    "    X[xMissing] = poly_func(xMissing)\n",
    "\n",
    "    return X\n",
    "\n",
    "# Example dataset with missing values\n",
    "X = np.array([1.0, np.nan, 3.0, 4.0, np.nan, 6.0, np.nan, 8.0])\n",
    "\n",
    "# Apply interpolation methods\n",
    "XLinear = linearInterpolation(X.copy())\n",
    "XNearest = nearestNeighborInterpolation(X.copy())\n",
    "XPoly = polynomialInterpolation(X.copy(), degree=2)\n",
    "\n",
    "print(\"Original Data with Missing Values:\")\n",
    "print(X)\n",
    "\n",
    "print(\"\\nLinear Interpolation:\")\n",
    "print(XLinear)\n",
    "\n",
    "print(\"\\nNearest-Neighbor Interpolation:\")\n",
    "print(XNearest)\n",
    "\n",
    "print(\"\\nPolynomial Interpolation (Degree 2):\")\n",
    "print(XPoly)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Lib\n",
    "feature = np.array([0, 1, 2, 3, 4])\n",
    "target = np.array([0, 1, np.nan, 3, 4])\n",
    "\n",
    "# Interpolate missing value\n",
    "yIntercept = np.interp(feature, feature[~np.isnan(target)], target[~np.isnan(target)])\n",
    "print(yIntercept)\n",
    "\n",
    "# Use Scipy for 2D and OpenCV for matrix - use cases are define above\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Outlier Detection and Removal**  \n",
    "\n",
    "### **What is an Outlier?**  \n",
    "An **outlier** is a data point that significantly differs from the rest of the dataset. Outliers can occur due to errors, variability in data, or rare events. Detecting and removing outliers is essential in **data preprocessing** for improving machine learning model performance.  \n",
    "\n",
    "---\n",
    "\n",
    "## **Methods for Outlier Detection and Removal**  \n",
    "\n",
    "### **1. Z-score Method (Standard Score Method)**  \n",
    " **Concept:**  \n",
    "- Measures how many standard deviations a data point is from the mean.  \n",
    "- If a data point's **Z-score is too high or too low**, it's considered an outlier.  \n",
    "\n",
    " **Formula:**  \n",
    "$\n",
    "Z = \\frac{(X - \\mu)}{\\sigma}\n",
    "$\n",
    "Where:  \n",
    "- $ X $ = data point  \n",
    "- $ \\mu $ = mean of the dataset  \n",
    "- $ \\sigma $ = standard deviation  \n",
    "\n",
    " **Threshold:**  \n",
    "- Commonly used thresholds: **Z > 3 or Z < -3** (99.7% of data falls within 3 standard deviations).  \n",
    "\n",
    " **Steps:**  \n",
    "1. Compute the mean ($\\mu$) and standard deviation ($\\sigma$).  \n",
    "2. Calculate the Z-score for each data point.  \n",
    "3. Remove points where **Z-score > threshold (usually 3 or -3)**.  \n",
    "\n",
    "---\n",
    "\n",
    "### **2. IQR (Interquartile Range) Method**  \n",
    " **Concept:**  \n",
    "- Uses the **middle 50% of the data** to detect outliers.  \n",
    "- Any value **below the lower bound or above the upper bound** is considered an outlier.  \n",
    "\n",
    " **Formula:**  \n",
    "$\n",
    "IQR = Q3 - Q1\n",
    "$\n",
    "$\n",
    "\\text{Lower Bound} = Q1 - 1.5 \\times IQR\n",
    "$\n",
    "$\n",
    "\\text{Upper Bound} = Q3 + 1.5 \\times IQR\n",
    "$\n",
    "Where:  \n",
    "- **Q1 (25th percentile)** = First quartile  \n",
    "- **Q3 (75th percentile)** = Third quartile  \n",
    "- **IQR (Interquartile Range)** = Spread of middle 50% of data  \n",
    "\n",
    " **Steps:**  \n",
    "1. Compute **Q1 (25th percentile)** and **Q3 (75th percentile)**.  \n",
    "2. Compute **IQR = Q3 - Q1**.  \n",
    "3. Compute **upper** and **lower bounds**.  \n",
    "4. Remove values outside these bounds.  \n",
    "\n",
    "---\n",
    "\n",
    "### **3. Isolation Forest (IForest)**  \n",
    " **Concept:**  \n",
    "- A machine learning algorithm that isolates anomalies by **randomly selecting features** and **splitting data points**.  \n",
    "- Outliers get isolated faster because they lie in low-density regions.  \n",
    "\n",
    " **How It Works?**  \n",
    "1. Randomly select a feature and split it at a random value.  \n",
    "2. Build a tree where normal points need **more splits** to be isolated, while **outliers are isolated quickly**.  \n",
    "3. Compute an **anomaly score**, where higher values indicate outliers.  \n",
    "\n",
    " **Pros:**  \n",
    " Works on **high-dimensional data**  \n",
    " **Unsupervised** (no need for labeled data)  \n",
    " Efficient for large datasets  \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Local Outlier Factor (LOF)**  \n",
    " **Concept:**  \n",
    "- Measures how **isolated a data point is** compared to its neighbors.  \n",
    "- Uses **density comparison**—if a point is in a low-density region, it's an outlier.  \n",
    "\n",
    " **How It Works?**  \n",
    "1. Compute the **local density** of each point by looking at its **k nearest neighbors**.  \n",
    "2. Compare each point’s density with its neighbors.  \n",
    "3. If a point’s density is **significantly lower** than its neighbors, it's an outlier.  \n",
    "\n",
    " **Pros:**  \n",
    " Works well in **clusters**  \n",
    " **Unsupervised** (no need for labeled data)  \n",
    " Detects **local anomalies** (useful when outliers are not globally different but locally different)  \n",
    "\n",
    "---\n",
    "\n",
    "## **Comparison of Outlier Detection Methods**  \n",
    "\n",
    "| Method | Type | Best For | Pros | Cons |\n",
    "|--------|------|----------|------|------|\n",
    "| **Z-score** | Statistical | Normally distributed data | Simple, fast | Sensitive to skewed data |\n",
    "| **IQR Method** | Statistical | Skewed data, small datasets | Robust to skewed data | Ignores data distribution |\n",
    "| **Isolation Forest** | Machine Learning | Large, high-dimensional datasets | Works well on large datasets | Needs tuning |\n",
    "| **LOF** | Machine Learning | Clustering-based outlier detection | Detects **local** anomalies | Computationally expensive |\n",
    "\n",
    "---\n",
    "\n",
    "## **Final Recommendation**\n",
    "- **For Normally Distributed Data** → **Z-score**  \n",
    "- **For Skewed Data / Small Data** → **IQR**  \n",
    "- **For Large & High-Dimensional Data** → **Isolation Forest**  \n",
    "- **For Clustered Data** → **Local Outlier Factor (LOF)**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sample dataset\n",
    "data = np.array([10, 12, 14, 15, 16, 100])  # 100 is an outlier\n",
    "\n",
    "# Compute mean and standard deviation\n",
    "mean = np.mean(data)\n",
    "standardDeviation = np.std(data)\n",
    "\n",
    "# Compute Z-scores\n",
    "zScores = (data - mean) / standardDeviation\n",
    "\n",
    "# Remove outliers (Z > 3 or Z < -3)\n",
    "filteredData = data[np.abs(zScores) < 3]\n",
    "print(filteredData)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## ** Raw Implementation of Isolation Forest**\n",
    "\n",
    "### **How It Works**\n",
    "- Randomly selects a feature and a split value.\n",
    "- Constructs a tree by recursively splitting the data.\n",
    "- Outliers are isolated quickly in shallow trees.\n",
    "- The average depth of a point determines its anomaly score.\n",
    "\n",
    "---\n",
    "\n",
    "## **Comparison of Isolation Forest vs Local Outlier Factor (LOF)**\n",
    "\n",
    "| **Feature**             | **Isolation Forest**            | **Local Outlier Factor (LOF)**    |\n",
    "|------------------------|---------------------------------|-----------------------------------|\n",
    "| **Type**              | Tree-based method               | Density-based method             |\n",
    "| **Best for**          | High-dimensional data           | Small and structured datasets    |\n",
    "| **Computational Cost**| Fast (O(n log n))               | Slow (O(n²) for large datasets)  |\n",
    "| **Interpretable**     | Yes (Tree splits)               | Hard to interpret densities      |\n",
    "| **Works with Clusters** | No (assumes global outliers)  | Yes (detects local outliers)     |\n",
    "\n",
    "---\n",
    "\n",
    "## **Final Thoughts**\n",
    "- **Use Isolation Forest** when working with **large, high-dimensional datasets**.\n",
    "- **Use LOF** when outliers are **locally different from neighbors**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class IsolationTree:\n",
    "    def __init__(self, maxDepth):\n",
    "        self.maxDepth = maxDepth\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.splitFeature = None\n",
    "        self.splitValue = None\n",
    "        self.size = 0\n",
    "\n",
    "    def fit(self, X, depth=0):\n",
    "        if depth >= self.maxDepth or X.shape[0] <= 1:\n",
    "            return\n",
    "        \n",
    "        self.splitFeature = np.random.randint(0, X.shape[1])\n",
    "        minValue, maxValue = np.min(X[:, self.splitFeature]), np.max(X[:, self.splitFeature])\n",
    "\n",
    "        if minValue == maxValue:\n",
    "            return\n",
    "\n",
    "        self.splitValue = np.random.uniform(minValue, maxValue)\n",
    "        \n",
    "        leftMask = X[:, self.splitFeature] < self.splitValue\n",
    "        xLeft, xRight = X[leftMask], X[~leftMask]\n",
    "\n",
    "        self.left = IsolationTree(self.maxDepth)\n",
    "        self.right = IsolationTree(self.maxDepth)\n",
    "\n",
    "        self.left.fit(xLeft, depth + 1)\n",
    "        self.right.fit(xRight, depth + 1)\n",
    "\n",
    "class IsolationForest:\n",
    "    def __init__(self, nTrees=100, maxDepth=10):\n",
    "        self.nTrees = nTrees\n",
    "        self.maxDepth = maxDepth\n",
    "        self.trees = []\n",
    "\n",
    "    def fit(self, X):\n",
    "        self.trees = [IsolationTree(self.maxDepth) for _ in range(self.nTrees)]\n",
    "        for tree in self.trees:\n",
    "            sampleIndices = np.random.choice(X.shape[0], size=min(256, X.shape[0]), replace=False)\n",
    "            tree.fit(X[sampleIndices])\n",
    "\n",
    "    def pathLength(self, X, tree, depth=0):\n",
    "        if tree is None or (tree.splitFeature is None and tree.splitValue is None):\n",
    "            return depth\n",
    "\n",
    "        if X[tree.splitFeature] < tree.splitValue:\n",
    "            return self.pathLength(X, tree.left, depth + 1)\n",
    "        else:\n",
    "            return self.pathLength(X, tree.right, depth + 1)\n",
    "\n",
    "    def anomalyScore(self, X):\n",
    "        pathLengths = np.array([np.mean([self.pathLength(x, tree) for tree in self.trees]) for x in X])\n",
    "        c = 2 * (np.log(X.shape[0] - 1) + 0.5772156649) - (2 * (X.shape[0] - 1) / X.shape[0])\n",
    "        scores = 2 ** (-pathLengths / c)\n",
    "        return scores\n",
    "\n",
    "    def predict(self, X, threshold=0.6):\n",
    "        scores = self.anomalyScore(X)\n",
    "        return np.where(scores > threshold, -1, 1)  # -1 for outliers, 1 for normal points\n",
    "\n",
    "# Example usage\n",
    "X = np.array([[10], [12], [14], [15], [16], [100]])  # 100 is an outlier\n",
    "\n",
    "model = IsolationForest(nTrees=50, maxDepth=8)\n",
    "model.fit(X)\n",
    "\n",
    "predictions = model.predict(X)\n",
    "print(predictions)  # -1 indicates outlier, 1 indicates normal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LocalOutlierFactor:\n",
    "    def __init__(self, nNeighbors=3):\n",
    "        self.nNeighbors = nNeighbors\n",
    "        self.X = None\n",
    "\n",
    "    def fit(self, X):\n",
    "        self.X = X\n",
    "\n",
    "    def euclideanDistance(self, a, b):\n",
    "        return np.sqrt(np.sum((a - b) ** 2))\n",
    "\n",
    "    def kDistance(self, point):\n",
    "        distances = np.array([self.euclideanDistance(point, x) for x in self.X])\n",
    "        sortedDistances = np.sort(distances)\n",
    "        return sortedDistances[self.nNeighbors]\n",
    "\n",
    "    def reachabilityDistance(self, point, neighbor):\n",
    "        return max(self.kDistance(neighbor), self.euclideanDistance(point, neighbor))\n",
    "\n",
    "    def localReachabilityDensity(self, point):\n",
    "        distances = np.array([self.reachabilityDistance(point, neighbor) for neighbor in self.X])\n",
    "        return 1 / (np.mean(distances) + 1e-10)  # Avoid division by zero\n",
    "\n",
    "    def localOutlierFactor(self, point):\n",
    "        lrd_point = self.localReachabilityDensity(point)\n",
    "        lrd_neighbors = np.array([self.localReachabilityDensity(neighbor) for neighbor in self.X])\n",
    "        return np.mean(lrd_neighbors) / (lrd_point + 1e-10)  # Avoid division by zero\n",
    "\n",
    "    def predict(self, X, threshold=1.5):\n",
    "        lof_scores = np.array([self.localOutlierFactor(point) for point in X])\n",
    "        return np.where(lof_scores > threshold, -1, 1)  # -1 for outliers, 1 for normal points\n",
    "\n",
    "# Example usage\n",
    "X = np.array([[10], [12], [14], [15], [16], [100]])  # 100 is an outlier\n",
    "\n",
    "lof_model = LocalOutlierFactor(nNeighbors=2)\n",
    "lof_model.fit(X)\n",
    "\n",
    "predictions = lof_model.predict(X)\n",
    "print(predictions)  # -1 indicates outlier, 1 indicates normal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Feature Scaling in Machine Learning**  \n",
    "\n",
    "## **What is Feature Scaling?**  \n",
    "Feature scaling is a **data preprocessing technique** used to normalize or standardize numerical features in a dataset. Many machine learning algorithms, especially those relying on **distance-based calculations** (e.g., K-Nearest Neighbors, SVM, PCA), perform better when features are on a similar scale.  \n",
    "\n",
    "---\n",
    "\n",
    "## **1. Standardization (Z-score Normalization)**  \n",
    " **Concept:**  \n",
    "- Transforms data to have a **mean of 0** and a **standard deviation of 1**.  \n",
    "- Helps in **normalizing** features with **different units** or **ranges**.  \n",
    "\n",
    " **Formula:**  \n",
    "$\n",
    "X_{\\text{scaled}} = \\frac{X - \\mu}{\\sigma}\n",
    "$\n",
    "Where:  \n",
    "- $ X $ = Original value  \n",
    "- $ \\mu $ = Mean of the feature  \n",
    "- $ \\sigma $ = Standard deviation of the feature  \n",
    "\n",
    " **Best for:**  \n",
    " **Normally distributed data**  \n",
    " Models that assume **zero-mean and unit variance** (e.g., PCA, Logistic Regression, SVM)  \n",
    "\n",
    "---\n",
    "\n",
    "## **2. Min-Max Scaling (Normalization)**  \n",
    " **Concept:**  \n",
    "- Rescales data to a fixed range, usually **[0,1]** or **[-1,1]**.  \n",
    "- Retains the **original distribution** but compresses values into a small range.  \n",
    "\n",
    " **Formula:**  \n",
    "$\n",
    "X_{\\text{scaled}} = \\frac{X - X_{\\min}}{X_{\\max} - X_{\\min}}\n",
    "$\n",
    "Where:  \n",
    "- $ X $ = Original value  \n",
    "- $ X_{\\min} $, $ X_{\\max} $ = Minimum and maximum values in the dataset  \n",
    "\n",
    " **Best for:**  \n",
    " **Algorithms that require bounded values** (e.g., Neural Networks, K-Means Clustering)  \n",
    " **Features with different units**  \n",
    "\n",
    "---\n",
    "\n",
    "## **3. Robust Scaling (for Handling Outliers)**  \n",
    " **Concept:**  \n",
    "- Uses the **median** and **interquartile range (IQR)** instead of the mean and standard deviation.  \n",
    "- **Reduces the effect of outliers** by scaling data based on robust statistics.  \n",
    "\n",
    " **Formula:**  \n",
    "$\n",
    "X_{\\text{scaled}} = \\frac{X - Q1}{Q3 - Q1}\n",
    "$\n",
    "Where:  \n",
    "- $ Q1 $ = 25th percentile (First quartile)  \n",
    "- $ Q3 $ = 75th percentile (Third quartile)  \n",
    "\n",
    " **Best for:**  \n",
    " **Datasets with extreme outliers**  \n",
    " **Skewed distributions**  \n",
    "\n",
    "---\n",
    "\n",
    "## **4. Log Transformation**  \n",
    " **Concept:**  \n",
    "- **Reduces right-skewed distributions** by applying a logarithmic function.  \n",
    "- **Compresses large values** and expands small values.  \n",
    "- Helps **normalize** data that follows a power-law distribution.  \n",
    "\n",
    " **Formula:**  \n",
    "$\n",
    "X_{\\text{scaled}} = \\log(X + c)\n",
    "$\n",
    "Where:  \n",
    "- $ c $ is a small constant (to avoid log(0) errors).  \n",
    "\n",
    " **Best for:**  \n",
    " **Data with exponential growth** (e.g., income, population, price distributions).  \n",
    " **Handling skewness and heteroscedasticity (unequal variance)**.  \n",
    "\n",
    "---\n",
    "\n",
    "## **5. Power Transformation (Box-Cox & Yeo-Johnson)**  \n",
    " **Concept:**  \n",
    "- **Box-Cox and Yeo-Johnson** are transformations that make data more **normally distributed**.  \n",
    "- Unlike log transformation, these methods work with **negative** and **zero** values.  \n",
    "\n",
    "### **(a) Box-Cox Transformation**  \n",
    "- Works only on **positive data** ($ X > 0 $).  \n",
    "- Uses a parameter **$ \\lambda $** to transform data.  \n",
    "\n",
    " **Formula:**  \n",
    "$\n",
    "X_{\\text{scaled}} =\n",
    "\\begin{cases} \n",
    "\\frac{X^{\\lambda} - 1}{\\lambda}, & \\lambda \\neq 0 \\\\\n",
    "\\log(X), & \\lambda = 0\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "### **(b) Yeo-Johnson Transformation**  \n",
    "- Works on **both positive and negative** data.  \n",
    "- Similar to Box-Cox but designed for datasets containing **zero or negative values**.  \n",
    "\n",
    " **Formula:**  \n",
    "$\n",
    "X_{\\text{scaled}} =\n",
    "\\begin{cases} \n",
    "\\frac{(X + 1)^{\\lambda} - 1}{\\lambda}, & X \\geq 0, \\lambda \\neq 0 \\\\\n",
    "\\log(X + 1), & X \\geq 0, \\lambda = 0 \\\\\n",
    "-\\frac{(-X + 1)^{2 - \\lambda} - 1}{2 - \\lambda}, & X < 0, \\lambda \\neq 2 \\\\\n",
    "-\\log(-X + 1), & X < 0, \\lambda = 2\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "\n",
    "## **Comparison of Feature Scaling Methods**\n",
    "\n",
    "| **Method**               | **Best For**                  | **Handles Outliers?** | **Works with Negative Values?** |\n",
    "|-------------------------|-----------------------------|----------------------|--------------------------------|\n",
    "| **Standardization (Z-score)** | Normal distribution, SVM, PCA | ❌ No |  Yes |\n",
    "| **Min-Max Scaling**      | Neural networks, bounded data | ❌ No |  Yes |\n",
    "| **Robust Scaling**       | Data with outliers |  Yes |  Yes |\n",
    "| **Log Transformation**   | Skewed data, large values | ❌ No | ❌ No (Must be positive) |\n",
    "| **Box-Cox Transformation** | Normalizing non-normal data | ❌ No | ❌ No (Must be positive) |\n",
    "| **Yeo-Johnson Transformation** | Normalizing non-normal data |  Yes |  Yes |\n",
    "\n",
    "---\n",
    "\n",
    "## **Final Recommendations**\n",
    "- **Use Standardization (Z-score)** when working with **normally distributed data**.\n",
    "- **Use Min-Max Scaling** when data should be **bounded within a fixed range**.\n",
    "- **Use Robust Scaling** if the data contains **outliers**.\n",
    "- **Use Log Transformation** for **right-skewed data**.\n",
    "- **Use Box-Cox or Yeo-Johnson** for **non-normal distributions**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z we already Implemented above \n",
    "\n",
    "\n",
    "# Min Max Scaling (Normalization)\n",
    "\n",
    "data = np.array([50, 60, 70, 80, 90])\n",
    "\n",
    "\n",
    "def minMaxScaling(data):\n",
    "    minValue = np.min(data)\n",
    "    maxValue = np.max(data)\n",
    "    normalizedData = (data - minValue) / (maxValue - minValue)\n",
    "    return normalizedData\n",
    "\n",
    "print(minMaxScaling(data))\n",
    "\n",
    "\n",
    "## Robust Scaling (Outliers)\n",
    "\n",
    "def robustScaling(data):\n",
    "    Q1 = np.percentile(data, 25)\n",
    "    Q3 = np.percentile(data, 75)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    robustScaledData = (data - Q1) / IQR\n",
    "    return robustScaledData\n",
    "print(robustScaling(data))\n",
    "\n",
    "\n",
    "\n",
    "# Logged Transformation\n",
    "\n",
    "def loggedTransformation(data):\n",
    "    loggedTransformData = np.log(data + 1)\n",
    "    return loggedTransformData\n",
    "\n",
    "print(loggedTransformation(data))\n",
    "\n",
    "\n",
    "\n",
    "# BoxCox\n",
    "def boxCox(data):\n",
    "    return stats.boxcox(data)\n",
    "\n",
    "\n",
    "#  Yeo-Johnson\n",
    "\n",
    "def yeoJohnson(data):\n",
    "    return stats.yeojohnson(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Encoding categorical data** is a process used to convert categorical variables into numerical values so that machine learning algorithms can understand them. Here’s an overview of the common techniques used:\n",
    "\n",
    "1. **One-Hot Encoding**:\n",
    "   - This method creates new binary columns for each category in a feature. If a category exists, the corresponding column is marked with a `1`, and others are marked with `0`.\n",
    "   - **Example**: For the \"Color\" feature with categories `['Red', 'Green', 'Blue']`, it would create 3 new columns: `Color_Red`, `Color_Green`, `Color_Blue`. A row with `Color = 'Red'` would become `[1, 0, 0]`.\n",
    "\n",
    "2. **Label Encoding**:\n",
    "   - Label encoding assigns a unique integer to each category. The categories are mapped to numerical values, usually starting from `0`.\n",
    "   - **Example**: For `['Red', 'Green', 'Blue']`, the encoding might look like this: `Red -> 0`, `Green -> 1`, `Blue -> 2`.\n",
    "\n",
    "3. **Target Encoding (Mean Encoding)**:\n",
    "   - This method replaces each category with the mean of the target variable for that category. It is particularly useful when dealing with high cardinality features.\n",
    "   - **Example**: If you are predicting house prices and have a feature `Neighborhood`, you would replace each neighborhood with the average price of houses in that neighborhood.\n",
    "\n",
    "4. **Frequency Encoding**:\n",
    "   - This technique encodes categories based on the frequency of their occurrence in the dataset. Each category is replaced with the number of times it appears.\n",
    "   - **Example**: For `['Red', 'Green', 'Blue', 'Red', 'Red']`, `Red` would be encoded as `3`, `Green` as `1`, and `Blue` as `1`.\n",
    "\n",
    "5. **Binary Encoding**:\n",
    "   - Binary encoding is a mix of one-hot encoding and label encoding. It first converts the category labels to integers and then transforms those integers into binary code. Each digit of the binary code is represented by a column.\n",
    "   - **Example**: For categories `['Red', 'Green', 'Blue']`, label encoding would first map them to integers: `Red -> 0`, `Green -> 1`, `Blue -> 2`. Then, the binary encoding of these integers would be:\n",
    "     - `0 -> 00`\n",
    "     - `1 -> 01`\n",
    "     - `2 -> 10`\n",
    "     These values would be split across two binary columns.\n",
    "\n",
    "6. **Ordinal Encoding**:\n",
    "   - This technique is used when the categories have a meaningful order. Each category is assigned an integer based on its position in the order.\n",
    "   - **Example**: For the feature `Size` with categories `['Small', 'Medium', 'Large']`, ordinal encoding would map them as `Small -> 0`, `Medium -> 1`, `Large -> 2`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data\n",
    "df = pd.DataFrame({\n",
    "    'Color': ['Red', 'Green', 'Blue', 'Green', 'Red'],\n",
    "    'Price': [100, 200, 150, 180, 120]\n",
    "})\n",
    "def oneHotEncoding(data):\n",
    "\n",
    "    oHE = pd.get_dummies(data, columns=['Color'])\n",
    "    return oHE\n",
    "\n",
    "print(oneHotEncoding(df))\n",
    "\n",
    "\n",
    "def targetEncoding():\n",
    "    tE = df.groupby('Color')['Price'].mean()\n",
    "    f= df['Color_encoded'] = df['Color'].map(tE)\n",
    "    return f\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Feature Selection**\n",
    "\n",
    "Feature selection is a crucial step in the data preprocessing pipeline that aims to reduce the number of input features to the model while retaining the most important ones. By removing irrelevant or redundant features, it improves model performance, reduces over-fitting, speeds up computation, and provides better interpretability.\n",
    "\n",
    "There are three main types of feature selection methods: **Filter Methods**, **Wrapper Methods**, and **Embedded Methods**.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Filter Methods**\n",
    "Filter methods evaluate the importance of features independently of any machine learning model. They rely on statistical tests to measure the relevance of features to the target variable. These methods are typically fast and can be used as a preprocessing step before applying more complex models.\n",
    "\n",
    "- **Mutual Information**: Measures the dependency between two variables. Higher mutual information means the feature provides more relevant information about the target variable.\n",
    "- **Chi-Square Test**: A statistical test used to determine if two categorical variables are independent. It is often used for evaluating categorical features in relation to the target variable.\n",
    "- **ANOVA (Analysis of Variance) Test**: Used to determine if there are significant differences between the means of different groups. It is useful when the target is continuous, and the features are categorical.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Wrapper Methods**\n",
    "Wrapper methods evaluate subsets of features by training a machine learning model on them and assessing its performance. These methods are computationally expensive but can potentially find the best subset of features.\n",
    "\n",
    "- **Recursive Feature Elimination (RFE)**: A technique that recursively removes the least important features based on the performance of the model. The model is trained repeatedly on different subsets of features to identify the most important ones.\n",
    "- **Forward/Backward Feature Selection**:\n",
    "  - **Forward Selection**: Starts with no features and iteratively adds features that improve the model's performance.\n",
    "  - **Backward Selection**: Starts with all features and iteratively removes features that have the least impact on model performance.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Embedded Methods**\n",
    "Embedded methods perform feature selection during the model training process. These methods are efficient because they combine feature selection and model training into one process.\n",
    "\n",
    "- **LASSO (L1 Regularization)**: LASSO adds a penalty to the model that encourages the coefficients of less important features to be zero, effectively performing feature selection. This is commonly used in linear regression models.\n",
    "- **Decision Tree Feature Importance**: Decision trees and tree-based algorithms, like Random Forest and XGBoost, can compute the importance of each feature based on how well they reduce impurity (e.g., Gini index or entropy).\n",
    "- **SHAP (SHapley Additive exPlanations)**: SHAP values provide a unified measure of feature importance for any machine learning model. They are based on cooperative game theory and explain how much each feature contributes to a model's prediction.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Dimensionality Reduction**\n",
    "\n",
    "Dimensionality reduction is the process of reducing the number of features or variables in a dataset, while retaining as much information as possible. This is particularly useful for improving computational efficiency, reducing noise, and mitigating the curse of dimensionality in machine learning models. Dimensionality reduction techniques are broadly categorized into linear and non-linear methods.\n",
    "\n",
    "Here are the most commonly used dimensionality reduction techniques:\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Principal Component Analysis (PCA)**\n",
    "PCA is a **linear** dimensionality reduction technique that transforms the original features into a smaller set of new features, called **principal components**. These principal components are linear combinations of the original features and are ordered in such a way that the first few components retain most of the variance (information) in the dataset.\n",
    "\n",
    "#### Key Concepts:\n",
    "- PCA aims to maximize the variance in the dataset while reducing its dimensionality.\n",
    "- It does this by identifying the directions (principal components) in which the data varies the most.\n",
    "- It involves an eigenvalue decomposition of the covariance matrix of the data.\n",
    "\n",
    "#### Advantages:\n",
    "- PCA is widely used for feature extraction and noise reduction.\n",
    "- It reduces over-fitting by removing correlated features.\n",
    "- PCA can help visualize high-dimensional data by projecting it onto two or three principal components.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Linear Discriminant Analysis (LDA)**\n",
    "LDA is a **supervised** dimensionality reduction technique used to find a linear combination of features that best separates two or more classes in the dataset. Unlike PCA, which focuses on variance, LDA maximizes the separation between classes.\n",
    "\n",
    "#### Key Concepts:\n",
    "- LDA tries to maximize the **between-class variance** and minimize the **within-class variance**.\n",
    "- It is commonly used in classification tasks to reduce the dimensionality of data while preserving class separability.\n",
    "- The number of dimensions after LDA is at most the number of classes minus one.\n",
    "\n",
    "#### Advantages:\n",
    "- LDA is particularly useful when the dataset has labeled data and is used for class separation.\n",
    "- It can improve classification performance by projecting data onto a lower-dimensional space that is more suitable for classification.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. t-Distributed Stochastic Neighbor Embedding (t-SNE)**\n",
    "t-SNE is a **non-linear** dimensionality reduction technique primarily used for the visualization of high-dimensional data in a lower-dimensional space (typically 2D or 3D). It is effective in preserving local structure, making it ideal for visualizing clusters and patterns in complex datasets.\n",
    "\n",
    "#### Key Concepts:\n",
    "- t-SNE minimizes the divergence between probability distributions that represent pairwise similarities between data points in high-dimensional and low-dimensional spaces.\n",
    "- It models data points in the high-dimensional space as probability distributions and seeks to map them to a lower-dimensional space with similar probability distributions.\n",
    "\n",
    "#### Advantages:\n",
    "- t-SNE is particularly effective in visualizing clusters or groupings in high-dimensional data.\n",
    "- It is widely used in fields like bio-informatics, NLP, and computer vision for exploring data.\n",
    "\n",
    "#### Limitations:\n",
    "- t-SNE is computationally expensive and does not preserve global structure (e.g., distances between clusters may not be preserved well).\n",
    "- It is typically used for visualization rather than for actual model training or feature extraction.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. AutoEncoders**\n",
    "AutoEncoders are a type of **neural network** used for unsupervised dimensionality reduction. An auto-encoder consists of an encoder and a decoder. The encoder compresses the input data into a smaller representation (latent space), while the decoder attempts to reconstruct the original data from this compressed form.\n",
    "\n",
    "#### Key Concepts:\n",
    "- AutoEncoders learn to encode data into a lower-dimensional space by training the network to minimize the reconstruction error between the input and the output.\n",
    "- The middle layer (latent space) represents the reduced-dimensionality version of the data.\n",
    "- They can be used for both dimensionality reduction and anomaly detection.\n",
    "\n",
    "#### Advantages:\n",
    "- AutoEncoders can capture complex, non-linear relationships in the data.\n",
    "- They are flexible and can be trained on different types of data, including images, text, and time-series data.\n",
    "\n",
    "#### Limitations:\n",
    "- AutoEncoders can be computationally expensive, especially for large datasets.\n",
    "- The quality of dimensionality reduction depends on the architecture and the choice of hyper-parameters.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Independent Component Analysis (ICA)**\n",
    "ICA is a **non-linear** dimensionality reduction technique that aims to find statistically independent components in the data. Unlike PCA, which seeks components with maximum variance, ICA looks for components that are as statistically independent as possible.\n",
    "\n",
    "#### Key Concepts:\n",
    "- ICA is useful when the data consists of mixed signals, such as in **blind source separation** problems, where the goal is to recover the original sources from their mixtures (e.g., separating audio signals from a mixture of sounds).\n",
    "- It is based on the assumption that the observed data is a linear combination of statistically independent sources.\n",
    "- ICA uses higher-order statistical moments to separate the signals.\n",
    "\n",
    "#### Advantages:\n",
    "- ICA is particularly useful in signal processing, especially for problems like separating mixed audio signals.\n",
    "- It can uncover hidden factors that are independent of each other.\n",
    "\n",
    "#### Limitations:\n",
    "- ICA can be sensitive to noise and may not work well with highly correlated data.\n",
    "- It assumes that the underlying sources are independent, which may not always be true for all types of data.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary of Differences:\n",
    "- **PCA** focuses on variance and is linear, making it suitable for data with linear relationships.\n",
    "- **LDA** is supervised and focuses on class separability, often used in classification problems.\n",
    "- **t-SNE** is non-linear and best for visualization of high-dimensional data, especially to uncover clusters.\n",
    "- **AutoEncoders** are neural networks that can model complex, non-linear relationships for dimensionality reduction.\n",
    "- **ICA** focuses on finding statistically independent components and is often used in signal processing applications.\n",
    "\n",
    "Each method has its strengths and weaknesses, and the choice of technique depends on the nature of the data and the problem at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Sample data\n",
    "df = pd.DataFrame({\n",
    "    'Feature1': [1, 2, 3, 4, 5],\n",
    "    'Feature2': [10, 20, 30, 40, 50],\n",
    "    'Feature3': [100, 200, 300, 400, 500]\n",
    "})\n",
    "def principalComponentAnalysis(data):\n",
    "    # Step 1: Standardize the data\n",
    "    data = df.values\n",
    "    mean = np.mean(data, axis=0)\n",
    "    standardDeviation = np.std(data, axis=0)\n",
    "    dataStandardized = (data - mean) / standardDeviation\n",
    "\n",
    "    # Step 2: Compute the covariance matrix\n",
    "    covarianceMatrix = np.cov(dataStandardized, rowvar=False)\n",
    "\n",
    "    # Step 3: Compute eigenvalues and eigenvectors\n",
    "    eigenValues, eigenVectors = np.linalg.eigh(covarianceMatrix)\n",
    "\n",
    "    # Step 4: Sort eigenvalues and eigenvectors\n",
    "    sortedIndices = np.argsort(eigenValues)[::-1]\n",
    "    eigenValuesSorted = eigenValues[sortedIndices]\n",
    "    eigenVectorsSorted = eigenVectors[:, sortedIndices]\n",
    "\n",
    "    # Step 5: Select top k eigenvectors (k=2 for 2D projection)\n",
    "    k = 2\n",
    "    topEigenVectors = eigenVectorsSorted[:, :k]\n",
    "\n",
    "    # Step 6: Project data onto the new subspace\n",
    "    pcaResult = np.dot(dataStandardized, topEigenVectors)\n",
    "    print(f\"PCA Result (2D Projection):\\n{pcaResult}\")\n",
    "    return pcaResult\n",
    "\n",
    "principalComponentAnalysis(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data (3 classes)\n",
    "df = pd.DataFrame({\n",
    "    'Feature1': [1, 2, 3, 6, 7, 8, 10, 11, 12],\n",
    "    'Feature2': [4, 5, 6, 9, 10, 11, 15, 16, 17],\n",
    "    'Target': [0, 0, 0, 1, 1, 1, 2, 2, 2]\n",
    "})\n",
    "\n",
    "\n",
    "def linearDiscriminantAnalysis(data):\n",
    "    # Step 1: Compute class means\n",
    "    classMeans = df.groupby('Target').mean().drop(columns=['Target'])\n",
    "\n",
    "    # Step 2: Compute the within-class scatter matrix (Sw)\n",
    "    Sw = np.zeros((2, 2))\n",
    "    for target in df['Target'].unique():\n",
    "        classData = df[df['Target'] == target].drop(columns=['Target'])\n",
    "        classMean = classMeans.loc[target]\n",
    "        Sw += np.dot((classData - classMean).T, (classData - classMean))\n",
    "\n",
    "    # Step 3: Compute the between-class scatter matrix (Sb)\n",
    "    overallMean = df.drop(columns=['Target']).mean()\n",
    "    Sb = np.zeros((2, 2))\n",
    "    for target in df['Target'].unique():\n",
    "        classData = df[df['Target'] == target].drop(columns=['Target'])\n",
    "        classMean = classMeans.loc[target]\n",
    "        n = classData.shape[0]\n",
    "        Sb += n * np.outer(classMean - overallMean, classMean - overallMean)\n",
    "\n",
    "    # Step 4: Solve the generalized eigenvalue problem\n",
    "    eigenValues, eigenVectors = np.linalg.eig(np.linalg.inv(Sw).dot(Sb))\n",
    "\n",
    "    # Step 5: Sort eigenvalues and eigenvectors\n",
    "    sorted_indices = np.argsort(eigenValues)[::-1]\n",
    "    eigvalsSorted = eigenValues[sorted_indices]\n",
    "    eigenVectorsSorted = eigenVectors[:, sorted_indices]\n",
    "\n",
    "    # Step 6: Project data onto the new subspace\n",
    "    ldaResult = np.dot(df.drop(columns=['Target']), eigenVectorsSorted[:, :2])\n",
    "    print(f\"LDA Result (2D Projection):\\n{ldaResult}\")\n",
    "    return ldaResult\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data Transformation & Augmentation**\n",
    "\n",
    "Data transformation and augmentation are techniques used to improve the quality and diversity of data for machine learning models. **Data transformation** involves modifying data to make it more suitable for modeling, while **data augmentation** refers to artificially increasing the size of the dataset by generating new data points through various methods.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Polynomial Features**\n",
    "Polynomial features are used to add interaction terms between features in a dataset, thereby allowing linear models to capture non-linear relationships. This transformation is especially useful when you want to apply a linear model but your data exhibits polynomial behavior.\n",
    "\n",
    "#### **Mathematical Explanation:**\n",
    "- If your original feature is $ x $, the polynomial transformation will generate higher-degree features. For example, for a degree of 2, the transformation would create $ x^2 $.\n",
    "- For two features $ x_1 $ and $ x_2 $, the polynomial transformation of degree $ d $ would include features such as $ x_1^2 $, $ x_2^2 $, and $ x_1x_2 $, etc.\n",
    "\n",
    "Given a dataset with features $ x_1, x_2, \\dots, x_n $, a polynomial transformation of degree $ d $ will create the following features:\n",
    "\n",
    "$\n",
    "x_1, x_2, \\dots, x_n, x_1^2, x_2^2, \\dots, x_n^2, x_1x_2, x_1x_3, \\dots\n",
    "$\n",
    "\n",
    "#### **Purpose:**\n",
    "- Polynomial features help linear models like linear regression to approximate more complex relationships between features.\n",
    "- The expanded feature space can capture non-linear relationships, even though the model itself is linear.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Discretization (Binning)**\n",
    "Discretization (also called binning) is a process that converts continuous data into discrete bins or intervals. This is useful when you want to convert continuous variables into categorical ones for modeling or when data exhibits certain groupings.\n",
    "\n",
    "#### **Mathematical Explanation:**\n",
    "- Discretization works by defining intervals (bins) in the feature space and assigning each data point to one of those bins. For example, you might define bins for ages: $ [0, 18) $, $ [18, 30) $, $ [30, 50) $, and $ [50, \\infty) $.\n",
    "- Let the feature $ x $ be a continuous variable. The discretized value $ b $ can be computed by:\n",
    "\n",
    "$\n",
    "b = \\text{bin}(x) \n",
    "$\n",
    "\n",
    "where $ \\text{bin}(x) $ maps $ x $ into one of the defined bins based on predefined thresholds.\n",
    "\n",
    "#### **Purpose:**\n",
    "- Discretization is helpful when the underlying data has inherent categories or thresholds, such as age groups or income brackets.\n",
    "- It also reduces the influence of outliers and can improve the performance of certain machine learning models by simplifying the relationship between features.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Data Augmentation**\n",
    "Data augmentation refers to artificially increasing the size of the dataset by generating new data points. It is commonly used in tasks like image and text classification, where generating additional data helps the model generalize better and prevents overfitting.\n",
    "\n",
    "#### **For Image Data:**\n",
    "In the context of images, augmentation techniques involve creating modified versions of the images to add variety to the dataset. Common transformations include:\n",
    "\n",
    "##### **a) Flipping:**\n",
    "Flipping an image horizontally or vertically. This helps the model generalize better, as many objects look similar when flipped (e.g., faces, vehicles).\n",
    "\n",
    "- **Horizontal flip**: Reflects the image across a vertical axis.\n",
    "- **Vertical flip**: Reflects the image across a horizontal axis.\n",
    "\n",
    "##### **b) Rotation:**\n",
    "Rotating an image by a certain degree (e.g., 90°, 180°, or any arbitrary angle). This introduces variety into the dataset, especially useful for cases where objects can appear at different orientations.\n",
    "\n",
    "- **Rotation matrix**: If the image is represented by a matrix, rotating it involves applying a transformation matrix to the image coordinates.\n",
    "\n",
    "##### **c) Scaling:**\n",
    "Scaling (zooming in or out) changes the size of an image. This helps the model learn invariances in object size, making it robust to different object scales in the input data.\n",
    "\n",
    "- **Scaling formula**: The scaling factor $ \\alpha $ scales the image pixels according to:\n",
    "\n",
    "$\n",
    "\\text{scaled image} = \\alpha \\times \\text{original image}\n",
    "$\n",
    "\n",
    "#### **For Text Data:**\n",
    "Text data can also be augmented to increase diversity and prevent overfitting. Some common techniques include:\n",
    "\n",
    "##### **a) Synonym Replacement:**\n",
    "This involves replacing words in the text with their synonyms. By using thesauruses or pre-trained word embeddings, we can find synonyms and replace them in a way that does not change the overall meaning of the text.\n",
    "\n",
    "- **Example**: Replacing the word \"happy\" with \"joyful\".\n",
    "\n",
    "##### **b) Back Translation:**\n",
    "Back translation involves translating the text into another language and then translating it back into the original language. This process introduces variation in the phrasing of the sentences, which can help improve the model's robustness.\n",
    "\n",
    "- **Example**: Translating \"I love programming\" to French (\"J'adore la programmation\") and then back to English (\"I adore programming\").\n",
    "\n",
    "---\n",
    "\n",
    "### **Purpose of Data Augmentation:**\n",
    "- **For Image Data:**\n",
    "  - It prevents overfitting by introducing more variations of the images.\n",
    "  - It allows models to be invariant to transformations like rotations, translations, or changes in scale, which is useful in real-world applications.\n",
    "  \n",
    "- **For Text Data:**\n",
    "  - It helps with increasing the diversity of text, especially when data is scarce.\n",
    "  - It can help capture a wider range of expressions and increase the model's generalization ability.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary:\n",
    "\n",
    "- **Polynomial Features**: Transform features into polynomial forms, capturing non-linear relationships.\n",
    "- **Discretization (Binning)**: Convert continuous features into discrete bins.\n",
    "- **Data Augmentation**:\n",
    "  - **Image Augmentation**: Techniques like flipping, rotation, and scaling to create diverse versions of the same image.\n",
    "  - **Text Augmentation**: Methods like synonym replacement and back translation to generate diverse text representations.\n",
    "\n",
    "These methods are integral for enhancing model performance, improving generalization, and overcoming issues like overfitting, especially when data is limited."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
