{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Algorithms for Data Processing**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Data Cleaning**  \n",
    "- Handling Missing Values:  \n",
    "  - Mean/Median/Mode Imputation  \n",
    "  - K-Nearest Neighbors (KNN) Imputation  \n",
    "  - Interpolation  \n",
    "- Outlier Detection and Removal:  \n",
    "  - Z-score Method  \n",
    "  - IQR (Inter-quartile Range) Method  \n",
    "  - Isolation Forest  \n",
    "  - Local Outlier Factor (LOF)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Mean Imputation**\n",
    "Mean imputation is a technique used to handle missing values in a dataset by replacing them with the **mean** (average) of the available data for that feature. It is commonly applied to numerical data. The mean is calculated as the sum of all values divided by the number of values. This method assumes that the data is normally distributed and does not have significant skewness or outliers.\n",
    "\n",
    "---\n",
    "\n",
    "## **Median Imputation**\n",
    "Median imputation replaces missing values with the **median** (middle value) of the available data for that feature. The median is the value that separates the higher half of the data from the lower half. This method is particularly useful for numerical data that is skewed or contains outliers, as the median is less sensitive to extreme values compared to the mean.\n",
    "\n",
    "---\n",
    "\n",
    "## **Mode Imputation**\n",
    "Mode imputation is used to replace missing values with the **mode** (most frequent value) of the available data for that feature. This technique is typically applied to categorical data or discrete numerical data where a clear dominant value exists. The mode represents the value that appears most frequently in the dataset.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Median Mode Imputations\n",
    "\n",
    "# Sample data\n",
    "data = {'A': [1, 2, np.nan, 4, 5]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Mean imputation\n",
    "meanVal = df['A'].mean()\n",
    "df['A']= df['A'].fillna(meanVal)\n",
    "print(\"Mean:\\n\",df)\n",
    "\n",
    "# Median Imputation\n",
    "medianVal = df['A'].median()\n",
    "df['A']= df['A'].fillna(medianVal)\n",
    "print(\"Median:\\n\",df)\n",
    "\n",
    "# Mode\n",
    "modVal = df['A'].mod(0)\n",
    "df['A']= df['A'].fillna(modVal)\n",
    "print(\"Mode:\\n\",df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### **K-Nearest Neighbors (KNN) Imputation**\n",
    "KNN imputation is a technique used to handle missing values by replacing them with values from the **k-nearest neighbors** in the dataset. It leverages the idea that similar data points (rows) should have similar values for their features.\n",
    "\n",
    "---\n",
    "\n",
    "### **How KNN Imputation Works**\n",
    "1. **Step 1: Identify Missing Values**  \n",
    "   Locate the missing values in the dataset.\n",
    "\n",
    "2. **Step 2: Compute Distances**  \n",
    "   For each row with missing values, compute the distance (e.g., Euclidean, Manhattan) to all other rows using the available features.\n",
    "\n",
    "3. **Step 3: Select Nearest Neighbors**  \n",
    "   Identify the **k-nearest neighbors** (rows) with the smallest distances.\n",
    "\n",
    "4. **Step 4: Impute Missing Values**  \n",
    "   - For **numerical features**: Replace the missing value with the **mean** or **median** of the corresponding feature values from the k-nearest neighbors.\n",
    "   - For **categorical features**: Replace the missing value with the **mode** (most frequent value) of the corresponding feature values from the k-nearest neighbors.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Parameters**\n",
    "- **n_neighbors (k)**: The number of neighbors to consider. A smaller k may overfit, while a larger k may smooth out patterns.\n",
    "- **Distance Metric**: Common metrics include Euclidean, Manhattan, or Minkowski distance.\n",
    "- **Weights**: Neighbors can be weighted by their distance (closer neighbors have more influence).\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### **Advantages of KNN Imputation**\n",
    "- Preserves relationships between features.\n",
    "- Works well for datasets with complex patterns.\n",
    "- Flexible for both numerical and categorical data.\n",
    "\n",
    "---\n",
    "\n",
    "### **Disadvantages of KNN Imputation**\n",
    "- Computationally expensive for large datasets.\n",
    "- Sensitive to the choice of k and distance metric.\n",
    "- Requires scaling of features for accurate distance calculations.\n",
    "\n",
    "---\n",
    "\n",
    "[Code](https://www.geeksforgeeks.org/k-nearest-neighbours/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def euclideanDistance(a, b):\n",
    "    \"\"\"Compute Euclidean distance between two vectors, ignoring NaNs.\"\"\"\n",
    "    mask = ~np.isnan(a) & ~np.isnan(b)  # Consider only non-NaN values\n",
    "    if not np.any(mask):\n",
    "        return np.inf  # If all values are NaN, return infinity\n",
    "    return np.sqrt(np.sum((a[mask] - b[mask])**2))\n",
    "\n",
    "def kNN(X, k=3):\n",
    "    \"\"\"KNN Imputation for missing values in a NumPy array.\"\"\"\n",
    "    xImputed = X.copy()  # Copy of original data\n",
    "    nRows, nColumns = X.shape\n",
    "\n",
    "    for rowIndex in range(nRows):\n",
    "        for columnIndex in range(nColumns):\n",
    "            if np.isnan(X[rowIndex, columnIndex]):  # Check for missing value\n",
    "                distances = []\n",
    "                \n",
    "                # Find K nearest neighbors\n",
    "                for neighborIndex in range(nRows):\n",
    "                    if neighborIndex != rowIndex and not np.isnan(X[neighborIndex, columnIndex]):\n",
    "                        dist = euclideanDistance(X[rowIndex], X[neighborIndex])\n",
    "                        distances.append((dist, X[neighborIndex, columnIndex]))\n",
    "\n",
    "                # Sort by distance and select K closest\n",
    "                distances.sort(key=lambda x: x[0])\n",
    "                kNeighbors = [val for _, val in distances[:k]]\n",
    "\n",
    "                # Impute missing value with mean of K neighbors\n",
    "                if kNeighbors:\n",
    "                    xImputed[rowIndex, columnIndex] = np.mean(kNeighbors)\n",
    "\n",
    "    return xImputed\n",
    "\n",
    "# Example dataset with missing values (NaN)\n",
    "X = np.array([[1.0, 2.0, np.nan],\n",
    "              [2.0, np.nan, 3.0],\n",
    "              [np.nan, 4.0, 5.0],\n",
    "              [3.0, 4.0, 6.0]])\n",
    "\n",
    "# Apply KNN imputation\n",
    "X_imputed = kNN(X, k=2)\n",
    "\n",
    "print(\"Original Data with Missing Values:\")\n",
    "print(X)\n",
    "\n",
    "print(\"\\nImputed Data:\")\n",
    "print(X_imputed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Best Algorithm for Matrix Interpolation**  \n",
    "\n",
    "Interpolation in a **matrix (2D data)** depends on the nature of missing values and the structure of the data. The best algorithm depends on:  \n",
    "\n",
    "1. **Smoothness of Data**  \n",
    "   - If data follows a smooth trend → **Spline Interpolation or Bicubic Interpolation**  \n",
    "   - If data has sudden jumps → **Nearest-Neighbor Interpolation**  \n",
    "\n",
    "2. **Computational Efficiency**  \n",
    "   - If speed is important → **Linear Interpolation or Nearest-Neighbor**  \n",
    "   - If accuracy is important → **Polynomial or Spline Interpolation**  \n",
    "\n",
    "---\n",
    "\n",
    "### **Common Matrix Interpolation Methods**  \n",
    "\n",
    "| Algorithm | Description | Best For |\n",
    "|-----------|-------------|----------|\n",
    "| **Bilinear Interpolation** | Uses linear interpolation in both row and column directions | Image resizing, smooth surfaces |\n",
    "| **Bicubic Interpolation** | Uses cubic polynomials for smoother results | High-quality image scaling |\n",
    "| **Nearest-Neighbor Interpolation** | Uses the closest available value | Discrete data, quick estimation |\n",
    "| **Spline Interpolation** | Fits smooth curves across the data points | Geospatial data, scientific applications |\n",
    "| **Kriging Interpolation** | A geostatistical method that models spatial correlation | Geographic and environmental data |\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### **Best Choice for Matrix Interpolation**\n",
    "| **Scenario** | **Best Algorithm** | **Best Library** |\n",
    "|-------------|----------------|---------------|\n",
    "| **Image Processing** | Bicubic Interpolation | OpenCV (`cv2`) |\n",
    "| **Smooth Data (Geospatial, Climate)** | Kriging, Spline | SciPy (`scipy.interpolate`) |\n",
    "| **Quick Estimation** | Nearest-Neighbor | NumPy (`numpy.interp`) |\n",
    "| **General Purpose** | Bilinear, Bicubic | SciPy (`griddata`) |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def linearInterpolation(X):\n",
    "    \"\"\"Perform linear interpolation on 1D NumPy array with missing values (NaN).\"\"\"\n",
    "    n = len(X)\n",
    "    for i in range(n):\n",
    "        if np.isnan(X[i]):  # If missing value found\n",
    "            left, right = None, None\n",
    "\n",
    "            # Find the nearest left non-NaN value\n",
    "            for k in range(i - 1, -1, -1):\n",
    "                if not np.isnan(X[k]):\n",
    "                    left = (k, X[k])\n",
    "                    break\n",
    "\n",
    "            # Find the nearest right non-NaN value\n",
    "            for k in range(i + 1, n):\n",
    "                if not np.isnan(X[k]):\n",
    "                    right = (k, X[k])\n",
    "                    break\n",
    "\n",
    "            # If both left and right exist, apply linear interpolation\n",
    "            if left and right:\n",
    "                x1, y1 = left\n",
    "                x2, y2 = right\n",
    "                X[i] = y1 + (y2 - y1) * (i - x1) / (x2 - x1)\n",
    "\n",
    "    return X\n",
    "\n",
    "def nearestNeighborInterpolation(X):\n",
    "    \"\"\"Perform nearest-neighbor interpolation on 1D NumPy array with missing values.\"\"\"\n",
    "    n = len(X)\n",
    "    for i in range(n):\n",
    "        if np.isnan(X[i]):  # If missing value found\n",
    "            left, right = None, None\n",
    "\n",
    "            # Find the nearest left non-NaN value\n",
    "            for k in range(i - 1, -1, -1):\n",
    "                if not np.isnan(X[k]):\n",
    "                    left = X[k]\n",
    "                    break\n",
    "\n",
    "            # Find the nearest right non-NaN value\n",
    "            for k in range(i + 1, n):\n",
    "                if not np.isnan(X[k]):\n",
    "                    right = X[k]\n",
    "                    break\n",
    "\n",
    "            # Use the nearest available value\n",
    "            if left is not None and right is not None:\n",
    "                X[i] = left if (i - k) < (k - i) else right\n",
    "            elif left is not None:\n",
    "                X[i] = left\n",
    "            elif right is not None:\n",
    "                X[i] = right\n",
    "\n",
    "    return X\n",
    "\n",
    "def polynomialInterpolation(X, degree=2):\n",
    "    \"\"\"Perform polynomial interpolation on a 1D NumPy array with missing values.\"\"\"\n",
    "    xKnown = np.where(~np.isnan(X))[0]  # Indices of known values\n",
    "    yKnown = X[xKnown]  # Known values\n",
    "    xMissing = np.where(np.isnan(X))[0]  # Indices of missing values\n",
    "\n",
    "    # Fit a polynomial curve to the known data points\n",
    "    polyCoefficients = np.polyfit(xKnown, yKnown, degree)\n",
    "    poly_func = np.poly1d(polyCoefficients)\n",
    "\n",
    "    # Predict missing values using the polynomial function\n",
    "    X[xMissing] = poly_func(xMissing)\n",
    "\n",
    "    return X\n",
    "\n",
    "# Example dataset with missing values\n",
    "X = np.array([1.0, np.nan, 3.0, 4.0, np.nan, 6.0, np.nan, 8.0])\n",
    "\n",
    "# Apply interpolation methods\n",
    "XLinear = linearInterpolation(X.copy())\n",
    "XNearest = nearestNeighborInterpolation(X.copy())\n",
    "XPoly = polynomialInterpolation(X.copy(), degree=2)\n",
    "\n",
    "print(\"Original Data with Missing Values:\")\n",
    "print(X)\n",
    "\n",
    "print(\"\\nLinear Interpolation:\")\n",
    "print(XLinear)\n",
    "\n",
    "print(\"\\nNearest-Neighbor Interpolation:\")\n",
    "print(XNearest)\n",
    "\n",
    "print(\"\\nPolynomial Interpolation (Degree 2):\")\n",
    "print(XPoly)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Lib\n",
    "x = np.array([0, 1, 2, 3, 4])\n",
    "y = np.array([0, 1, np.nan, 3, 4])\n",
    "\n",
    "# Interpolate missing value\n",
    "yIntercept = np.interp(x, x[~np.isnan(y)], y[~np.isnan(y)])\n",
    "print(yIntercept)\n",
    "\n",
    "# Use Scipy for 2D and OpenCV for matrix - use cases are define above\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Outlier Detection and Removal**  \n",
    "\n",
    "### **What is an Outlier?**  \n",
    "An **outlier** is a data point that significantly differs from the rest of the dataset. Outliers can occur due to errors, variability in data, or rare events. Detecting and removing outliers is essential in **data preprocessing** for improving machine learning model performance.  \n",
    "\n",
    "---\n",
    "\n",
    "## **Methods for Outlier Detection and Removal**  \n",
    "\n",
    "### **1. Z-score Method (Standard Score Method)**  \n",
    " **Concept:**  \n",
    "- Measures how many standard deviations a data point is from the mean.  \n",
    "- If a data point's **Z-score is too high or too low**, it's considered an outlier.  \n",
    "\n",
    " **Formula:**  \n",
    "$\n",
    "Z = \\frac{(X - \\mu)}{\\sigma}\n",
    "$\n",
    "Where:  \n",
    "- $ X $ = data point  \n",
    "- $ \\mu $ = mean of the dataset  \n",
    "- $ \\sigma $ = standard deviation  \n",
    "\n",
    " **Threshold:**  \n",
    "- Commonly used thresholds: **Z > 3 or Z < -3** (99.7% of data falls within 3 standard deviations).  \n",
    "\n",
    " **Steps:**  \n",
    "1. Compute the mean ($\\mu$) and standard deviation ($\\sigma$).  \n",
    "2. Calculate the Z-score for each data point.  \n",
    "3. Remove points where **Z-score > threshold (usually 3 or -3)**.  \n",
    "\n",
    "---\n",
    "\n",
    "### **2. IQR (Interquartile Range) Method**  \n",
    " **Concept:**  \n",
    "- Uses the **middle 50% of the data** to detect outliers.  \n",
    "- Any value **below the lower bound or above the upper bound** is considered an outlier.  \n",
    "\n",
    " **Formula:**  \n",
    "$\n",
    "IQR = Q3 - Q1\n",
    "$\n",
    "$\n",
    "\\text{Lower Bound} = Q1 - 1.5 \\times IQR\n",
    "$\n",
    "$\n",
    "\\text{Upper Bound} = Q3 + 1.5 \\times IQR\n",
    "$\n",
    "Where:  \n",
    "- **Q1 (25th percentile)** = First quartile  \n",
    "- **Q3 (75th percentile)** = Third quartile  \n",
    "- **IQR (Interquartile Range)** = Spread of middle 50% of data  \n",
    "\n",
    " **Steps:**  \n",
    "1. Compute **Q1 (25th percentile)** and **Q3 (75th percentile)**.  \n",
    "2. Compute **IQR = Q3 - Q1**.  \n",
    "3. Compute **upper** and **lower bounds**.  \n",
    "4. Remove values outside these bounds.  \n",
    "\n",
    "---\n",
    "\n",
    "### **3. Isolation Forest (IForest)**  \n",
    " **Concept:**  \n",
    "- A machine learning algorithm that isolates anomalies by **randomly selecting features** and **splitting data points**.  \n",
    "- Outliers get isolated faster because they lie in low-density regions.  \n",
    "\n",
    " **How It Works?**  \n",
    "1. Randomly select a feature and split it at a random value.  \n",
    "2. Build a tree where normal points need **more splits** to be isolated, while **outliers are isolated quickly**.  \n",
    "3. Compute an **anomaly score**, where higher values indicate outliers.  \n",
    "\n",
    " **Pros:**  \n",
    " Works on **high-dimensional data**  \n",
    " **Unsupervised** (no need for labeled data)  \n",
    " Efficient for large datasets  \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Local Outlier Factor (LOF)**  \n",
    " **Concept:**  \n",
    "- Measures how **isolated a data point is** compared to its neighbors.  \n",
    "- Uses **density comparison**—if a point is in a low-density region, it's an outlier.  \n",
    "\n",
    " **How It Works?**  \n",
    "1. Compute the **local density** of each point by looking at its **k nearest neighbors**.  \n",
    "2. Compare each point’s density with its neighbors.  \n",
    "3. If a point’s density is **significantly lower** than its neighbors, it's an outlier.  \n",
    "\n",
    " **Pros:**  \n",
    " Works well in **clusters**  \n",
    " **Unsupervised** (no need for labeled data)  \n",
    " Detects **local anomalies** (useful when outliers are not globally different but locally different)  \n",
    "\n",
    "---\n",
    "\n",
    "## **Comparison of Outlier Detection Methods**  \n",
    "\n",
    "| Method | Type | Best For | Pros | Cons |\n",
    "|--------|------|----------|------|------|\n",
    "| **Z-score** | Statistical | Normally distributed data | Simple, fast | Sensitive to skewed data |\n",
    "| **IQR Method** | Statistical | Skewed data, small datasets | Robust to skewed data | Ignores data distribution |\n",
    "| **Isolation Forest** | Machine Learning | Large, high-dimensional datasets | Works well on large datasets | Needs tuning |\n",
    "| **LOF** | Machine Learning | Clustering-based outlier detection | Detects **local** anomalies | Computationally expensive |\n",
    "\n",
    "---\n",
    "\n",
    "## **Final Recommendation**\n",
    "- **For Normally Distributed Data** → **Z-score**  \n",
    "- **For Skewed Data / Small Data** → **IQR**  \n",
    "- **For Large & High-Dimensional Data** → **Isolation Forest**  \n",
    "- **For Clustered Data** → **Local Outlier Factor (LOF)**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sample dataset\n",
    "data = np.array([10, 12, 14, 15, 16, 100])  # 100 is an outlier\n",
    "\n",
    "# Compute mean and standard deviation\n",
    "mean = np.mean(data)\n",
    "standardDeviation = np.std(data)\n",
    "\n",
    "# Compute Z-scores\n",
    "zScores = (data - mean) / standardDeviation\n",
    "\n",
    "# Remove outliers (Z > 3 or Z < -3)\n",
    "filteredData = data[np.abs(zScores) < 3]\n",
    "print(filteredData)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## ** Raw Implementation of Isolation Forest**\n",
    "\n",
    "### **How It Works**\n",
    "- Randomly selects a feature and a split value.\n",
    "- Constructs a tree by recursively splitting the data.\n",
    "- Outliers are isolated quickly in shallow trees.\n",
    "- The average depth of a point determines its anomaly score.\n",
    "\n",
    "---\n",
    "\n",
    "## **Comparison of Isolation Forest vs Local Outlier Factor (LOF)**\n",
    "\n",
    "| **Feature**             | **Isolation Forest**            | **Local Outlier Factor (LOF)**    |\n",
    "|------------------------|---------------------------------|-----------------------------------|\n",
    "| **Type**              | Tree-based method               | Density-based method             |\n",
    "| **Best for**          | High-dimensional data           | Small and structured datasets    |\n",
    "| **Computational Cost**| Fast (O(n log n))               | Slow (O(n²) for large datasets)  |\n",
    "| **Interpretable**     | Yes (Tree splits)               | Hard to interpret densities      |\n",
    "| **Works with Clusters** | No (assumes global outliers)  | Yes (detects local outliers)     |\n",
    "\n",
    "---\n",
    "\n",
    "## **Final Thoughts**\n",
    "- **Use Isolation Forest** when working with **large, high-dimensional datasets**.\n",
    "- **Use LOF** when outliers are **locally different from neighbors**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class IsolationTree:\n",
    "    def __init__(self, maxDepth):\n",
    "        self.maxDepth = maxDepth\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.splitFeature = None\n",
    "        self.splitValue = None\n",
    "        self.size = 0\n",
    "\n",
    "    def fit(self, X, depth=0):\n",
    "        if depth >= self.maxDepth or X.shape[0] <= 1:\n",
    "            return\n",
    "        \n",
    "        self.splitFeature = np.random.randint(0, X.shape[1])\n",
    "        minValue, maxValue = np.min(X[:, self.splitFeature]), np.max(X[:, self.splitFeature])\n",
    "\n",
    "        if minValue == maxValue:\n",
    "            return\n",
    "\n",
    "        self.splitValue = np.random.uniform(minValue, maxValue)\n",
    "        \n",
    "        leftMask = X[:, self.splitFeature] < self.splitValue\n",
    "        xLeft, xRight = X[leftMask], X[~leftMask]\n",
    "\n",
    "        self.left = IsolationTree(self.maxDepth)\n",
    "        self.right = IsolationTree(self.maxDepth)\n",
    "\n",
    "        self.left.fit(xLeft, depth + 1)\n",
    "        self.right.fit(xRight, depth + 1)\n",
    "\n",
    "class IsolationForest:\n",
    "    def __init__(self, nTrees=100, maxDepth=10):\n",
    "        self.nTrees = nTrees\n",
    "        self.maxDepth = maxDepth\n",
    "        self.trees = []\n",
    "\n",
    "    def fit(self, X):\n",
    "        self.trees = [IsolationTree(self.maxDepth) for _ in range(self.nTrees)]\n",
    "        for tree in self.trees:\n",
    "            sampleIndices = np.random.choice(X.shape[0], size=min(256, X.shape[0]), replace=False)\n",
    "            tree.fit(X[sampleIndices])\n",
    "\n",
    "    def pathLength(self, X, tree, depth=0):\n",
    "        if tree is None or (tree.splitFeature is None and tree.splitValue is None):\n",
    "            return depth\n",
    "\n",
    "        if X[tree.splitFeature] < tree.splitValue:\n",
    "            return self.pathLength(X, tree.left, depth + 1)\n",
    "        else:\n",
    "            return self.pathLength(X, tree.right, depth + 1)\n",
    "\n",
    "    def anomalyScore(self, X):\n",
    "        pathLengths = np.array([np.mean([self.pathLength(x, tree) for tree in self.trees]) for x in X])\n",
    "        c = 2 * (np.log(X.shape[0] - 1) + 0.5772156649) - (2 * (X.shape[0] - 1) / X.shape[0])\n",
    "        scores = 2 ** (-pathLengths / c)\n",
    "        return scores\n",
    "\n",
    "    def predict(self, X, threshold=0.6):\n",
    "        scores = self.anomalyScore(X)\n",
    "        return np.where(scores > threshold, -1, 1)  # -1 for outliers, 1 for normal points\n",
    "\n",
    "# Example usage\n",
    "X = np.array([[10], [12], [14], [15], [16], [100]])  # 100 is an outlier\n",
    "\n",
    "model = IsolationForest(nTrees=50, maxDepth=8)\n",
    "model.fit(X)\n",
    "\n",
    "predictions = model.predict(X)\n",
    "print(predictions)  # -1 indicates outlier, 1 indicates normal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  1  1  1  1 -1]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class LocalOutlierFactor:\n",
    "    def __init__(self, nNeighbors=3):\n",
    "        self.nNeighbors = nNeighbors\n",
    "        self.X = None\n",
    "\n",
    "    def fit(self, X):\n",
    "        self.X = X\n",
    "\n",
    "    def euclideanDistance(self, a, b):\n",
    "        return np.sqrt(np.sum((a - b) ** 2))\n",
    "\n",
    "    def kDistance(self, point):\n",
    "        distances = np.array([self.euclideanDistance(point, x) for x in self.X])\n",
    "        sortedDistances = np.sort(distances)\n",
    "        return sortedDistances[self.nNeighbors]\n",
    "\n",
    "    def reachabilityDistance(self, point, neighbor):\n",
    "        return max(self.kDistance(neighbor), self.euclideanDistance(point, neighbor))\n",
    "\n",
    "    def localReachabilityDensity(self, point):\n",
    "        distances = np.array([self.reachabilityDistance(point, neighbor) for neighbor in self.X])\n",
    "        return 1 / (np.mean(distances) + 1e-10)  # Avoid division by zero\n",
    "\n",
    "    def localOutlierFactor(self, point):\n",
    "        lrd_point = self.localReachabilityDensity(point)\n",
    "        lrd_neighbors = np.array([self.localReachabilityDensity(neighbor) for neighbor in self.X])\n",
    "        return np.mean(lrd_neighbors) / (lrd_point + 1e-10)  # Avoid division by zero\n",
    "\n",
    "    def predict(self, X, threshold=1.5):\n",
    "        lof_scores = np.array([self.localOutlierFactor(point) for point in X])\n",
    "        return np.where(lof_scores > threshold, -1, 1)  # -1 for outliers, 1 for normal points\n",
    "\n",
    "# Example usage\n",
    "X = np.array([[10], [12], [14], [15], [16], [100]])  # 100 is an outlier\n",
    "\n",
    "lof_model = LocalOutlierFactor(nNeighbors=2)\n",
    "lof_model.fit(X)\n",
    "\n",
    "predictions = lof_model.predict(X)\n",
    "print(predictions)  # -1 indicates outlier, 1 indicates normal\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
