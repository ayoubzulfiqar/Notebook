{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculus\n",
    "\n",
    "I will skip the basics because all the basic material about calculus are easily available in the internet and i you want to learn the basic i would recommend this video that helped me to understand the basic of calculus theoretically.\n",
    "\n",
    "Here: I would do only challenges and implementations that are important in machine learning and I will Implement Numerically but for finding the differentiation Equation i did symbolically.\n",
    "\n",
    " [Calculus](https://www.youtube.com/watch?v=MO-AExWdl4Q)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Calculus challenges**\n",
    "\n",
    "---\n",
    "\n",
    "### **Basic Differentiation**\n",
    "1. Write a Python function to compute the derivative of a quadratic function.  \n",
    "2. Calculate the derivative of a polynomial function of degree 3.  \n",
    "3. Implement the chain rule for composite functions.  \n",
    "4. Compute the gradient of $ f(x) = x^2 + 3x + 5 $.  \n",
    "5. Differentiate $ f(x) = e^x $ and verify your result numerically.  \n",
    "6. Find the derivative of $ f(x) = \\ln(x) $ for $ x > 0 $.  \n",
    "7. Compute the derivative of $ f(x) = \\sin(x) $ and $ f(x) = \\cos(x) $.  \n",
    "8. Implement numerical differentiation using the finite difference method.  \n",
    "9. Differentiate $ f(x) = x^3 \\sin(x) $ using the product rule.  \n",
    "10. Differentiate $ f(x) = \\frac{x^2}{x+1} $ using the quotient rule.\n",
    "\n",
    "---\n",
    "\n",
    "### **Partial Derivatives**\n",
    "11. Compute the partial derivative of $ f(x, y) = x^2 + y^2 $ with respect to $ x $.  \n",
    "12. Calculate the gradient vector of $ f(x, y) = x^2 + 3xy + y^2 $.  \n",
    "13. Find the partial derivatives of $ f(x, y) = e^{x+y} $.  \n",
    "14. Write a function to compute the Jacobian matrix for $ f(x, y) = [x^2 + y, xy] $.  \n",
    "15. Compute second-order partial derivatives of $ f(x, y) = \\sin(xy) $.  \n",
    "16. Implement a function to calculate the Hessian matrix for $ f(x, y) = x^2 + y^2 + xy $.  \n",
    "17. Visualize the gradient vector field of $ f(x, y) = x^2 - y^2 $ using Python.  \n",
    "18. Find the gradient of $ f(x, y, z) = x^2 + y^2 + z^2 $.  \n",
    "19. Solve $ \\nabla f(x, y) = 0 $ for $ f(x, y) = x^2 + 4xy + y^2 $.  \n",
    "20. Use Python to compute higher-order partial derivatives.\n",
    "\n",
    "---\n",
    "\n",
    "### **Optimization**\n",
    "21. Write a Python function to find the minimum of $ f(x) = x^2 + 4x + 4 $ using derivatives.  \n",
    "22. Implement gradient descent for a 2D quadratic function $ f(x, y) = x^2 + y^2 $.  \n",
    "23. Perform gradient descent to minimize $ f(x) = \\ln(x) + x^2 $.  \n",
    "24. Solve $ f'(x) = 0 $ for $ f(x) = 3x^3 - 4x^2 + 5 $ to find critical points.  \n",
    "25. Visualize the optimization path of gradient descent on $ f(x, y) = x^2 + y^2 $.  \n",
    "26. Use the Hessian matrix to check the convexity of $ f(x, y) = x^2 + y^2 $.  \n",
    "27. Implement stochastic gradient descent for $ f(x) = (x-3)^2 + (y+2)^2 $.  \n",
    "28. Write a function to find the saddle point of $ f(x, y) = x^2 - y^2 $.  \n",
    "29. Solve constrained optimization using the method of Lagrange multipliers.  \n",
    "30. Analyze and optimize a simple cost function for linear regression.\n",
    "\n",
    "---\n",
    "\n",
    "### **Integration**\n",
    "31. Compute the definite integral of $ f(x) = x^2 $ over $ [0, 1] $.  \n",
    "32. Write a Python function to approximate the area under a curve using the trapezoidal rule.  \n",
    "33. Implement Simpson’s rule for numerical integration.  \n",
    "34. Find the integral of $ f(x) = e^{-x} $.  \n",
    "35. Compute the indefinite integral of $ f(x) = \\sin(x) $.  \n",
    "36. Solve $ \\int_{0}^{1} x^2 dx $ using `scipy.integrate`.  \n",
    "37. Integrate $ f(x, y) = x^2 + y^2 $ over $ x \\in [0, 1], y \\in [0, 1] $.  \n",
    "38. Write a function to compute the cumulative distribution function (CDF) from a probability density function (PDF).  \n",
    "39. Visualize the convergence of numerical integration methods on $ f(x) = 1/(1+x^2) $.  \n",
    "40. Use Monte Carlo integration to approximate the area of a circle.\n",
    "\n",
    "---\n",
    "\n",
    "### **Applications in Machine Learning**\n",
    "41. Implement backpropagation for a neural network using the chain rule.  \n",
    "42. Compute the gradient of the loss function for linear regression.  \n",
    "43. Write a function to calculate the softmax function and its derivative.  \n",
    "44. Perform numerical differentiation and compare it with analytical results for a cost function.  \n",
    "45. Implement the sigmoid function and compute its derivative.  \n",
    "46. Derive and compute gradients for the ReLU activation function.  \n",
    "47. Write a Python function to calculate the divergence of a vector field.  \n",
    "48. Visualize the loss surface of a simple neural network using partial derivatives.  \n",
    "49. Implement the gradient of the cross-entropy loss for logistic regression.  \n",
    "50. Combine calculus and linear algebra to solve a ridge regression problem.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -- Basic Differentiation --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General formula or find the derivative of any give function\n",
    "def finiteDifference(f, x, h=1e-5):\n",
    "    return (f(x + h) - f(x - h)) / (2 * h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partial Finite Difference Formula with Respect to X\n",
    "def partialFiniteDifferenceX(f, x, y, h=1e-5):\n",
    "    return (f(x + h, y) - f(x - h, y)) / (2 * h)\n",
    "def partialFiniteDifferenceY(f, x, y, h=1e-5):\n",
    "    return (f(x, y + h) - f(x, y - h)) / (2 * h)\n",
    "\n",
    "def partialFiniteDifferenceXY(f, x, y, h=1e-5):\n",
    "    # Partial derivative with respect to x\n",
    "    dFdx = (f(x + h, y) - f(x - h, y)) / (2 * h)\n",
    "    # Partial derivative with respect to y\n",
    "    dFdy = (f(x, y + h) - f(x, y - h)) / (2 * h)\n",
    "    return dFdx, dFdy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**General Derivative finding formulas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x= sp.symbols('x')\n",
    "y= sp.symbols('y')\n",
    "x, y= sp.symbols('x y')\n",
    "\n",
    "\n",
    "def symbolicDerivativeX(formula):\n",
    "    # Partial Derivative with respect to X\n",
    "    fDX = sp.diff(formula, x)\n",
    "    return fDX\n",
    "def symbolicDerivativeY(formula):\n",
    "    # Partial Derivative with respect to Y\n",
    "    fDY = sp.diff(formula, y)\n",
    "    return fDY\n",
    "\n",
    "def partialSymbolicDerivative(formula):\n",
    "    # Partial Derivative with respect to X\n",
    "    fDX = sp.diff(formula, x)\n",
    "    # Partial Derivative with respect to Y\n",
    "    fDY = sp.diff(formula, y) \n",
    "    return fDX, fDY\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the derivative of quadratic function\n",
    "\n",
    "## Let's quadratic Be: f(x) = ax^2 + bx + c\n",
    "## Quadratic Values: 2x^2 + 3x + 1\n",
    "\n",
    "# Numerical Implementation\n",
    "\n",
    "# Define the quadratic function\n",
    "def f(x):\n",
    "    a, b, c = 2, 3, 1  # Example coefficients\n",
    "    return a * x**2 + b * x + c\n",
    "\n",
    "# Define the derivative using finite differences\n",
    "\n",
    "finiteDifference(f, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polynomial Derivative function degree 3\n",
    "\n",
    "# Cubic Polynomial: f(x) = ax^3 + bx^2 + cx + d\n",
    "\n",
    "def cubePolynomial(x ,a, b, c, d):\n",
    "    # a,b,c,d = 2, -3, 1, 4\n",
    "    coefficients = [a, b, c, d]\n",
    "    root =np.roots(coefficients)\n",
    "    print(\"Polynomial Roots:\", root)\n",
    "    return a * x**3 + b * x**2 + c * x + d\n",
    "x=2\n",
    "a =2\n",
    "b=-3\n",
    "c=1\n",
    "d=4\n",
    "cB = cubePolynomial(x,a,b,c,d)\n",
    "print(f\"Cubic Polynomial at x = {x}: {cB}\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement the Chain Rule for Composite Function\n",
    "# Composite Function: h(x)=f(g(x))\n",
    "# Chain Rule: h′(x)=f′(g(x))⋅g ′(x)\n",
    "\n",
    "\n",
    "# Example: sin(x^2)\n",
    "\n",
    "# Define the functions\n",
    "def g(x):\n",
    "    return x**2  # Inner function\n",
    "\n",
    "def f(u):\n",
    "    return np.sin(u)  # Outer function\n",
    "\n",
    "# Define the derivatives\n",
    "def fDerivatives(u):\n",
    "    return np.cos(u)  # Derivative of f\n",
    "\n",
    "def gDerivative(x):\n",
    "    return 2 * x  # Derivative of g\n",
    "\n",
    "# Chain rule implementation\n",
    "def chainRule(x):\n",
    "    return fDerivatives(g(x)) * gDerivative(x)\n",
    "\n",
    "# Compute the derivative at a specific point\n",
    "x = 2\n",
    "cR = chainRule(x)\n",
    "print(f\"Derivative at x = {x}: {cR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the gradient of f(x) = x**2 + 3x + 5 also f'(x) = 2x + 3\n",
    "\n",
    "# f(x) = x**2 + 3x + 5 = f'(x) = 2x + 3\n",
    "\n",
    "# Symbolic\n",
    "\n",
    "def fDerivative():\n",
    "    x = sp.symbols('x')\n",
    "    f = x**2 + 3*x + 5\n",
    "    fD = sp.diff(f, x)\n",
    "    print(\"Derivative of f'(x):\", fD)\n",
    "    return fD\n",
    "\n",
    "fDerivative()\n",
    "\n",
    "# Define the function\n",
    "def f(x):\n",
    "    return x**2 + 3*x + 5\n",
    "\n",
    "# Define the gradient using finite differences\n",
    "def gradient(f, x, h=1e-5):\n",
    "    return (f(x + h) - f(x - h)) / (2 * h)\n",
    "\n",
    "# Compute the gradient at a specific point\n",
    "xValue = 2  # Point at which to compute the gradient\n",
    "gDValue = gradient(f, xValue)\n",
    "print(f\"Gradient at x = {xValue}: {gDValue}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Differentiate the f(x) = e^x and verify numerically\n",
    "\n",
    "# Formula\n",
    "def findDerivative():\n",
    "    x = sp.symbols('x')\n",
    "    f = sp.exp(x)\n",
    "    fDerivative = sp.diff(f, x)\n",
    "    print(\"Derivative of f'(x):\", fDerivative)\n",
    "    return fDerivative\n",
    "findDerivative()\n",
    "\n",
    "# Numerical\n",
    "\n",
    "def f(x):\n",
    "    return np.exp(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Compute the derivative at a specific point\n",
    "xValue = 2  # Point at which to compute the derivative\n",
    "numericalResult = finiteDifference(f, xValue)\n",
    "exactResult = np.exp(xValue)  # Exact derivative of e^x is e^x\n",
    "\n",
    "print(f\"Numerical derivative at x = {xValue}: {numericalResult}\")\n",
    "print(f\"Exact derivative at x = {xValue}: {exactResult}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find Derivative of f(x) = ln(x) for x > 0\n",
    "# Symbolically\n",
    "def findLnDerivative():\n",
    "    x  = sp.symbols('x')\n",
    "    f = sp.ln(x)\n",
    "    fDerivative = sp.diff(f, x)\n",
    "    print(\"Derivative of f'(x):\", fDerivative)\n",
    "    return fDerivative\n",
    "findLnDerivative()\n",
    "\n",
    "# Numerically\n",
    "\n",
    "def f(x):\n",
    "    return np.log(x)\n",
    "\n",
    "xValue = 2\n",
    "result = finiteDifference(f, xValue)\n",
    "\n",
    "exact_result = 1 / xValue  # Exact derivative of ln(x) is 1/x\n",
    "\n",
    "print(f\"Numerical derivative at x = {xValue}: {result}\")\n",
    "print(f\"Exact derivative at x = {xValue}: {exact_result}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the derivative of f(x) = sin(x) and f(x)= cos(x)\n",
    "\n",
    "# Symbolic\n",
    "def findSinCos():\n",
    "    x = sp.symbols('x')\n",
    "    fSin = sp.sin(x)\n",
    "    fCos = sp.cos(x)\n",
    "    fSinDerivative = sp.diff(fSin, x)\n",
    "    fCosDerivative = sp.diff(fCos, x)\n",
    "    print(\"Derivative Equations:\", fSinDerivative, fCosDerivative)\n",
    "    return fSinDerivative, fCosDerivative\n",
    "\n",
    "findSinCos()\n",
    "\n",
    "# Numerical Computation\n",
    "\n",
    "\n",
    "def fSin(x):\n",
    "    return np.sin(x)\n",
    "def fCos(x):\n",
    "    return np.cos(x)\n",
    "\n",
    "# x value at specific point \n",
    "\n",
    "xValue = np.pi / 4\n",
    "\n",
    "sinDerivative = finiteDifference(fSin, xValue)\n",
    "cosDerivative = finiteDifference(fCos, xValue)\n",
    "\n",
    "print(f\"Numerical derivative of sin(x) at x = π/4: {sinDerivative}\")\n",
    "print(f\"Exact derivative of sin(x) at x = π/4: {np.cos(xValue)}\")\n",
    "print(f\"Numerical derivative of cos(x) at x = π/4: {cosDerivative}\")\n",
    "print(f\"Exact derivative of cos(x) at x = π/4: {-np.sin(xValue)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look Above at Top to find the implementation of Finite Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find differentiation f(x) = x^3sin(x) using Product Rule\n",
    "\n",
    "# Symbolically\n",
    "def findViaProduct():\n",
    "    x = sp.symbols('x')\n",
    "    # Equation\n",
    "    f = x**3 * sp.sin(x)\n",
    "    fDerivative = sp.diff(f, x)\n",
    "    print(\"Derivative Equation:\", fDerivative)\n",
    "    return fDerivative\n",
    " \n",
    "findViaProduct()\n",
    "\n",
    "# Numerically\n",
    "def f(x):\n",
    "    return x**3 * sp.sin(x)\n",
    "\n",
    "xValue = 1\n",
    "result = finiteDifference(f, xValue)\n",
    "print(f\"Numerical derivative at x = {xValue}: {result}\")\n",
    "print(f\"Exact derivative at x = {xValue}: { 3 * (1**2) * np.sin(1) + (1**3) * np.cos(1)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Differentiate f(x) = x**2 /x+ 1 using quotient rule\n",
    "\n",
    "# symbolic\n",
    "\n",
    "def dViaQuotient():\n",
    "\n",
    "# Define the symbolic variable\n",
    "    x = sp.symbols('x')\n",
    "\n",
    "# Equation\n",
    "    f = x**2 / (x + 1)\n",
    "\n",
    "# Compute the derivative using the Quotient Rule\n",
    "    fD = sp.diff(f, x)\n",
    "    # print(\"Derivative of f(x):\", fD)\n",
    "    return fD\n",
    "print(\"DerivativeEquation:\")\n",
    "dViaQuotient()\n",
    "\n",
    "\n",
    "# Numerical\n",
    "\n",
    "def f(x):\n",
    "    return x**2 / (x + 1)\n",
    "\n",
    "xValue = 2\n",
    "\n",
    "result = finiteDifference(f, xValue)\n",
    "print(f\"Numerical derivative at x = {xValue}: {result}\")\n",
    "print(f\"Exact derivative at x = {xValue}: { (2**2 + 2 * 2) / (2 + 1)**2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -- Partial Derivatives ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute partial Derivatives of f(x, y) - x**2 + y**2 with respect to x\n",
    "\n",
    "# Symbolically\n",
    "\n",
    "def symbolicF():\n",
    "    x, y = sp.symbols('x y')\n",
    "    f = x**2 + y**2\n",
    "    # Partial Derivative with respect to X\n",
    "    fDX = sp.diff(f, x)\n",
    "    # Partial Derivative with respect to Y\n",
    "    # fDY = sp.diff(f, y) # But we don't need y so\n",
    "    return fDX\n",
    "symbolicF()\n",
    "\n",
    "# Numerical\n",
    "\n",
    "def f(x, y):\n",
    "    return x**2 + y**2\n",
    "\n",
    "xValue = 2\n",
    "yValue = 3\n",
    "result = partialFiniteDifferenceX(f, xValue, yValue)\n",
    "print(f\"Numerical partial derivative at x = {xValue}, y = {yValue}: {result}\")\n",
    "print(f\"Exact partial derivative at x = {xValue}: {2**xValue}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Gradient Vector of f(x, y) = x**2 + 3*x*y + y**2\n",
    "\n",
    "# Symbolical\n",
    "def symbolicF():\n",
    "    x, y = sp.symbols('x y')\n",
    "    f = x**2 + 3*x*y + y**2\n",
    "    # fDX = sp.diff(f, x)\n",
    "    # fDY = sp.diff(f, y)\n",
    "    # both fDX and fDY are gradients\n",
    "    gradientF = [sp.diff(f, var) for var in (x, y)]\n",
    "    return gradientF\n",
    "symbolicF()\n",
    "\n",
    "# Numerical\n",
    "\n",
    "\n",
    "# Define the function\n",
    "def f(x, y):\n",
    "    return x**2 + 3*x*y + y**2\n",
    "\n",
    "# Define the numerical gradient using finite differences\n",
    "def gradientFiniteDifference(f, x, y, h=1e-5):\n",
    "    # Partial derivative with respect to x\n",
    "    dFdX = (f(x + h, y) - f(x - h, y)) / (2 * h)\n",
    "    # Partial derivative with respect to y\n",
    "    dFdY = (f(x, y + h) - f(x, y - h)) / (2 * h)\n",
    "    return np.array([dFdX, dFdY])\n",
    "\n",
    "# Compute the gradient at a specific point\n",
    "xValue = 1  # Point at which to compute the gradient\n",
    "yValue = 2\n",
    "gradientValue = gradientFiniteDifference(f, xValue, yValue)\n",
    "\n",
    "# Exact gradient at (x, y) = (1, 2)\n",
    "exactGradient = np.array([2*xValue + 3*yValue, 3*xValue + 2*yValue])\n",
    "\n",
    "print(f\"Numerical gradient at (x, y) = ({xValue}, {yValue}): {gradientValue}\")\n",
    "print(f\"Exact gradient at (x, y) = ({xValue}, {yValue}): {exactGradient}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the partial derivative of f(x, y) = e**x+y\n",
    "\n",
    "# Symbolically\n",
    "formula = sp.exp(x + y)\n",
    "partialSymbolicDerivative(formula)\n",
    "\n",
    "# Numerical\n",
    "def f(x, y):\n",
    "    return sp.exp(x + y)\n",
    "xValue = 1\n",
    "yValue = 2\n",
    "\n",
    "resultX, resultY = partialFiniteDifferenceXY(f, xValue, yValue)\n",
    "print(f\"Numerical partial derivative with respect to x at (x, y) = ({xValue}, {yValue}): {resultX}\")\n",
    "# print(f\"Exact partial derivative with respect to x at (x, y) = ({x_value}, {y_value}): {exact_f_x}\")\n",
    "print(f\"Numerical partial derivative with respect to y at (x, y) = ({xValue}, {yValue}): {resultY}\")\n",
    "# print(f\"Exact partial derivative with respect to y at (x, y) = ({x_value}, {y_value}): {exact_f_y}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function to compute the Jacobian Matrix for f(x, y) = [x**2 + y, xy]\n",
    "\n",
    "# Symbolically\n",
    "\n",
    "def symbolicalJacobian():\n",
    "\n",
    "# Define the symbolic variables\n",
    "    x, y = sp.symbols('x y')\n",
    "\n",
    "# Define the vector-valued function\n",
    "    f1 = x**2 + y\n",
    "    f2 = x * y\n",
    "    f = sp.Matrix([f1, f2])\n",
    "\n",
    "    # Compute the Jacobian matrix\n",
    "    J = f.jacobian([x, y])\n",
    "    # print(\"Jacobian matrix of f(x, y):\")\n",
    "    # print(J)\n",
    "    return J\n",
    "\n",
    "symbolicalJacobian()\n",
    "\n",
    "# Numerical\n",
    "def f(x, y):\n",
    "    return np.array([x**2 + y, x * y])\n",
    "\n",
    "def numericalJacobian(f, x, y, h=1e-5):\n",
    "    J = np.zeros((2,2))\n",
    "    # Compute partial derivatives for f1\n",
    "    f1X = (f(x + h, y)[0] - f(x - h, y)[0]) / (2 * h)\n",
    "    f1Y = (f(x, y + h)[0] - f(x, y - h)[0]) / (2 * h)\n",
    "\n",
    "    # Compute partial derivatives for f2\n",
    "    f2X = (f(x + h, y)[1] - f(x - h, y)[1]) / (2 * h)\n",
    "    f2Y = (f(x, y + h)[1] - f(x, y - h)[1]) / (2 * h)\n",
    "        # Fill the Jacobian matrix\n",
    "    J[0, 0] = f1X\n",
    "    J[0, 1] = f1Y\n",
    "    J[1, 0] = f2X\n",
    "    J[1, 1] = f2Y\n",
    "\n",
    "    return J\n",
    "# Compute the Jacobian at a specific point\n",
    "xValue = 1  # Point at which to compute the Jacobian\n",
    "yValue = 2\n",
    "jacobian = numericalJacobian(f, xValue, yValue)\n",
    "\n",
    "# Exact Jacobian at (x, y) = (1, 2)\n",
    "testJacobian = np.array([[2 * xValue, 1],\n",
    "                    [yValue, xValue]])\n",
    "\n",
    "print(f\"Numerical Jacobian at (x, y) = ({xValue}, {yValue}):\")\n",
    "print(jacobian)\n",
    "print(f\"Exact Jacobian at (x, y) = ({xValue}, {yValue}):\")\n",
    "print(testJacobian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second Order Partial Derivative of f(x, y) = sin(xy) \n",
    "\n",
    "import sympy as sp\n",
    "\n",
    "# Define the symbolic variables\n",
    "x, y = sp.symbols('x y')\n",
    "\n",
    "# Define the function\n",
    "f = sp.sin(x * y)\n",
    "\n",
    "# Compute the first-order partial derivatives\n",
    "fX = sp.diff(f, x)\n",
    "fY = sp.diff(f, y)\n",
    "\n",
    "# Compute the second-order partial derivatives\n",
    "fXX = sp.diff(fX, x)\n",
    "fYY = sp.diff(fY, y)\n",
    "fXY = sp.diff(fX, y)\n",
    "fYX = sp.diff(fY, x)\n",
    "\n",
    "print(\"Second partial derivative with respect to x (f_xx):\", fXX)\n",
    "print(\"Second partial derivative with respect to y (f_yy):\", fYY)\n",
    "print(\"Mixed partial derivative with respect to x and y (f_xy):\", fXY)\n",
    "print(\"Mixed partial derivative with respect to y and x (f_yx):\", fYX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of Hessian Matrix for f(x, y) = x**2 + y**2 + xy\n",
    "\n",
    "# Symbolic\n",
    "\n",
    "def symbolicHessianMatrix():\n",
    "\n",
    "# Define the symbolic variables\n",
    "    x, y = sp.symbols('x y')\n",
    "\n",
    "# Define the function\n",
    "    f = x**2 + y**2 + x * y\n",
    "\n",
    "# Compute the Hessian matrix\n",
    "    H = sp.hessian(f, (x, y))\n",
    "    print(\"Hessian matrix of f(x, y):\")\n",
    "    print(H)\n",
    "    return H\n",
    "symbolicHessianMatrix()\n",
    "\n",
    "\n",
    "# Numerical\n",
    "\n",
    "# formula\n",
    "def f(x, y):\n",
    "    return x**2 + y**2 + x * y\n",
    "# Define the numerical Hessian using finite differences\n",
    "def hessianMatrix(f, x, y, h=1e-5):\n",
    "    # Initialize the Hessian matrix\n",
    "    H = np.zeros((2, 2))\n",
    "\n",
    "    # Compute second partial derivatives\n",
    "    # f_xx\n",
    "    H[0, 0] = (f(x + h, y) - 2 * f(x, y) + f(x - h, y)) / h**2\n",
    "    # f_yy\n",
    "    H[1, 1] = (f(x, y + h) - 2 * f(x, y) + f(x, y - h)) / h**2\n",
    "    # f_xy\n",
    "    H[0, 1] = (f(x + h, y + h) - f(x + h, y - h) - f(x - h, y + h) + f(x - h, y - h)) / (4 * h**2)\n",
    "    # f_yx\n",
    "    H[1, 0] = H[0, 1]  # Symmetry of mixed partial derivatives\n",
    "\n",
    "    return H\n",
    "# Compute the Hessian at a specific point\n",
    "xValue = 1  # Point at which to compute the Hessian\n",
    "yValue = 2\n",
    "hessian = hessianMatrix(f, xValue, yValue)\n",
    "\n",
    "print(f\"Numerical Hessian at (x, y) = ({xValue}, {yValue}):\")\n",
    "print(hessian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the gradient of f(x,y,z) = x**2 + y**2 + z++2\n",
    "\n",
    "\n",
    "# Symbolic\n",
    "\n",
    "def symbolicGradient():\n",
    "    # Define the symbolic variables\n",
    "    x, y, z = sp.symbols('x y z')\n",
    "\n",
    "# Define the function\n",
    "    f = x**2 + y**2 + z**2\n",
    "\n",
    "# Compute the gradient\n",
    "    gradient = sp.Matrix([sp.diff(f, var) for var in (x, y, z)])\n",
    "    print(\"Gradient of f(x, y, z):\")\n",
    "    print(gradient)\n",
    "    return gradient\n",
    "symbolicGradient()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define the function\n",
    "def f(x, y, z):\n",
    "    return x**2 + y**2 + z**2\n",
    "\n",
    "# Define the numerical gradient using finite differences\n",
    "def gradientMatrix(f, x, y, z, h=1e-5):\n",
    "    # Partial derivative with respect to x\n",
    "    dFdx = (f(x + h, y, z) - f(x - h, y, z)) / (2 * h)\n",
    "    # Partial derivative with respect to y\n",
    "    dFdy = (f(x, y + h, z) - f(x, y - h, z)) / (2 * h)\n",
    "    # Partial derivative with respect to z\n",
    "    dFdz = (f(x, y, z + h) - f(x, y, z - h)) / (2 * h)\n",
    "    return np.array([dFdx, dFdy, dFdz])\n",
    "\n",
    "# Compute the gradient at a specific point\n",
    "xValue = 1  # Point at which to compute the gradient\n",
    "yValue = 2\n",
    "zValue = 3\n",
    "gValue = gradientMatrix(f, xValue, yValue, zValue)\n",
    "\n",
    "# Exact gradient at (x, y, z) = (1, 2, 3)\n",
    "testGradient = np.array([2 * xValue, 2 * yValue, 2 * zValue])\n",
    "\n",
    "print(f\"Numerical gradient at (x, y, z) = ({xValue}, {yValue}, {zValue}): {gValue}\")\n",
    "print(f\"Exact gradient at (x, y, z) = ({xValue}, {yValue}, {zValue}): {testGradient}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve \\nabla f(x, y) = 0 $ for $ f(x, y) = x^2 + 4xy + y^2 \n",
    "\n",
    "\n",
    "# Define the symbolic variables\n",
    "x, y = sp.symbols('x y')\n",
    "\n",
    "# Define the function\n",
    "f = x**2 + 4*x*y + y**2\n",
    "\n",
    "# Compute the gradient\n",
    "gradientF = [sp.diff(f, var) for var in (x, y)]\n",
    "\n",
    "# Solve the system of equations ∇f(x, y) = 0\n",
    "criticalPoints = sp.solve(gradientF, (x, y))\n",
    "print(\"Critical point(s):\", criticalPoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Python to compute higher-order partial derivatives.\n",
    "\n",
    "# Define the symbolic variables\n",
    "# This way you can find any order of derivatives you want\n",
    "def higherOrderDerivatives():\n",
    "    x, y = sp.symbols('x y')\n",
    "\n",
    "# Define the function\n",
    "    f = x**3 * y**2 + sp.sin(x * y)\n",
    "\n",
    "# Compute the second-order partial derivatives\n",
    "    f_xx = sp.diff(f, x, x)  # Second derivative with respect to x\n",
    "    f_yy = sp.diff(f, y, y)  # Second derivative with respect to y\n",
    "\n",
    "# Compute the third-order mixed partial derivative\n",
    "    f_xxy = sp.diff(f, x, x, y)  # Third derivative: first x, then x, then y\n",
    "\n",
    "    print(\"Second-order partial derivative with respect to x (f_xx):\", f_xx)\n",
    "    print(\"Second-order partial derivative with respect to y (f_yy):\", f_yy)\n",
    "    print(\"Third-order mixed partial derivative (f_xxy):\", f_xxy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -- Optimization --\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the minimum of f(x) = x**2 + 4x + 4 using derivatives\n",
    "\n",
    "def findMinimum():\n",
    "    x = sp.symbols('x')\n",
    "    # function\n",
    "    f = x**2 + 4 * x + 4\n",
    "    # firstDerivative\n",
    "    firstDerivative = sp.diff(f, x)\n",
    "    criticalPoint = sp.solve(firstDerivative, x)\n",
    "    print(\"CriticalPoint(CP):\", criticalPoint)\n",
    "    secondDerivative = sp.diff(firstDerivative, x)\n",
    "    # Analyze the second derivative at the critical point\n",
    "    for point in criticalPoint:\n",
    "        if secondDerivative.subs(x, point) > 0:\n",
    "            print(f\"The function has a local minimum at x = {point}.\")\n",
    "        elif secondDerivative.subs(x, point) < 0:\n",
    "            print(f\"The function has a local maximum at x = {point}.\")\n",
    "        else:\n",
    "            print(f\"The second derivative test is inconclusive at x = {point}.\")\n",
    "    minValue = f.subs(x, criticalPoint[0])\n",
    "    print(f\"The minimum value of f(x) is {minValue} at x = {criticalPoint[0]}.\")\n",
    "    return criticalPoint[0]\n",
    "\n",
    "findMinimum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Descent for 2D Quadratic Function f(x, y) = x**2 + y**2\n",
    "\n",
    "def f(x, y):\n",
    "    return x**2 + y**2\n",
    "\n",
    "def gradientOfF(x, y):\n",
    "    return np.array([2 * x, 2 * y])\n",
    "\n",
    "def gradientDescent2D(learningRate= 0.1, iteration=100, tolerance=1e-5):\n",
    "    # Initial guess\n",
    "    x, y = 5.0, 5.0  # Starting point\n",
    "\n",
    "# Gradient descent algorithm\n",
    "    for i in range(iteration):\n",
    "    # Compute the gradient\n",
    "        grad = gradientOfF(x, y)\n",
    "    \n",
    "    # Update x and y\n",
    "        xNew = x - learningRate * grad[0]\n",
    "        yNew = y - learningRate * grad[1]\n",
    "    \n",
    "    # Check for convergence\n",
    "        if np.linalg.norm([xNew - x, yNew - y]) < tolerance:\n",
    "            print(f\"Converged after {i} iterations.\")\n",
    "            break\n",
    "    \n",
    "    # Update x and y for the next iteration\n",
    "        x, y = xNew, yNew\n",
    "        # Print the results\n",
    "    print(f\"Minimum point: (x, y) = ({x}, {y})\")\n",
    "    print(f\"Minimum value: f(x, y) = {f(x, y)}\")\n",
    "    return x, y\n",
    "\n",
    "gradientDescent2D()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform gradient descent to minimize f(x) = ln(x) + x**2\n",
    "\n",
    "def f(x):\n",
    "    return np.log(x) + x**2\n",
    "def derivativeOfF(x):\n",
    "    return (1 / x) + 2 ** x\n",
    "\n",
    "def minimizeGradientDecent(learningRate = 0.1, iterations=1000, tolerance =1e-6, x = 2.0):\n",
    "    for i in range(iterations):\n",
    "        gradient = derivativeOfF(x)\n",
    "        xNew = x - learningRate * gradient\n",
    "        if xNew <= 0:\n",
    "            xNew = 1e-6\n",
    "        if np.abs(xNew - x) < tolerance:\n",
    "            print(f\"Convergence after {i} iterations\")\n",
    "            break\n",
    "        x = xNew\n",
    "        # Print the results\n",
    "        print(f\"Minimum point: x = {x}\")\n",
    "        print(f\"Minimum value: f(x) = {f(x)}\")\n",
    "        return x\n",
    "    \n",
    "minimizeGradientDecent()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solve f'(x) = 0 for f(x) = 3x**3 - 4x**2 + 5 to find critical point\n",
    "\n",
    "\n",
    "def criticalPointOfDerivative():\n",
    "    x = sp.symbols('x')\n",
    "    f = 3*x**3 - 4*x**2 + 5\n",
    "    derivativeOfF = sp.diff(f, x)\n",
    "    criticalPoints = sp.solve(derivativeOfF, x)\n",
    "    print(\"CriticalPoints(CP):\", criticalPoints)\n",
    "    return criticalPoints\n",
    "\n",
    "criticalPointOfDerivative()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the hessian matrix to check the convexity of f(x, y) = x**2 + y**2\n",
    "\n",
    "def convexity():\n",
    "    x, y = sp.symbols('x y')\n",
    "    f = x**2 + y**2\n",
    "    hessian = sp.hessian(f, (x, y))\n",
    "    print(\"HessianMatrix:\\n\", hessian)\n",
    "    eigenvalues = hessian.eigenvals()\n",
    "    print(\"Eigenvalues of the Hessian matrix:\", eigenvalues)\n",
    "    return print(\"\\nHessian Matrix is definite +ve so It's Strictly Convex\")\n",
    "\n",
    "convexity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement stochastic gradient descent for f(x) = (x -3)**2 + (y + 2)**2 \n",
    "\n",
    "# Define the function\n",
    "def f(x, y):\n",
    "    return (x - 3)**2 + (y + 2)**2\n",
    "\n",
    "# Define the gradient of the function\n",
    "def gradientOfFuc(x, y):\n",
    "    return np.array([2 * (x - 3), 2 * (y + 2)])\n",
    "\n",
    "def stochasticGradientDescent(learningRate=0.1, iterations = 1000, tolerance = 1e-6, x=0.0, y=0.0):\n",
    "    for i in range(iterations):\n",
    "        gradient = gradientOfF(x, y)\n",
    "         # Add noise to the gradient (simulating stochastic)\n",
    "        noise = np.random.normal(0, 0.1, size=2)  # Mean = 0, Std Dev = 0.1\n",
    "        noisyGradient = gradient + noise\n",
    "        xNew = x - learningRate * noisyGradient[0]\n",
    "        yNew = y - learningRate * noisyGradient[1]\n",
    "        if np.linalg.norm([xNew - x, yNew - y]) < tolerance:\n",
    "            print(f\"Converged after {i} iterations.\")\n",
    "            break\n",
    "        x, y = xNew, yNew\n",
    "    print(f\"Optimal point: (x, y) = ({x}, {y})\")\n",
    "    print(f\"Minimum value: f(x, y) = {f(x, y)}\")\n",
    "\n",
    "stochasticGradientDescent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the saddle point of f(x, y) = x**2 - y**2\n",
    "# Note: A critical point is a saddle point if the Hessian matrix has both positive and negative eigenvalues.\n",
    "\n",
    "def saddlePoint():\n",
    "    x, y = sp.symbols('x y')\n",
    "    f = x**2 - y**2\n",
    "    gradientOfF =[sp.diff(f, var) for var in (x, y)]\n",
    "    print(\"Gradient of f(x, y):\", gradientOfF)\n",
    "    criticalPoint = sp.solve(gradientOfF, (x, y))\n",
    "    print(\"Critical point(s):\", criticalPoint)\n",
    "    h = sp.hessian(f, (x, y))\n",
    "    print(\"HessianMatrix:\\n\", h)\n",
    "    eigenVals = h.eigenvals()\n",
    "    print(\"Eigenvalues of the Hessian matrix:\", eigenVals)\n",
    "\n",
    "saddlePoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure! Let me explain **constrained optimization** and the **method of Lagrange multipliers** in a simple and theoretical way.\n",
    "\n",
    "---\n",
    "\n",
    "### **What is Constrained Optimization?**\n",
    "Constrained optimization is the process of finding the **maximum** or **minimum** of a function $ f(x, y, \\dots) $ subject to certain **constraints**. These constraints are additional conditions that the variables $ x, y, \\dots $ must satisfy.\n",
    "\n",
    "#### Example:\n",
    "- **Objective**: Minimize $ f(x, y) = x^2 + y^2 $.\n",
    "- **Constraint**: $ x + y = 1 $.\n",
    "\n",
    "Here, we want to minimize $ f(x, y) $, but $ x $ and $ y $ must satisfy the condition $ x + y = 1 $.\n",
    "\n",
    "---\n",
    "\n",
    "### **What is the Method of Lagrange Multipliers?**\n",
    "The **method of Lagrange multipliers** is a mathematical tool used to solve constrained optimization problems. It introduces a new variable called the **Lagrange multiplier** (denoted by $ \\lambda $) to incorporate the constraint into the optimization problem.\n",
    "\n",
    "#### Key Idea:\n",
    "Instead of directly minimizing or maximizing $ f(x, y) $ subject to $ g(x, y) = 0 $, we create a new function called the **Lagrangian**:\n",
    "$\n",
    "\\mathcal{L}(x, y, \\lambda) = f(x, y) - \\lambda \\cdot g(x, y)\n",
    "$\n",
    "Here:\n",
    "- $ f(x, y) $ is the **objective function**.\n",
    "- $ g(x, y) = 0 $ is the **constraint**.\n",
    "- $ \\lambda $ is the **Lagrange multiplier**.\n",
    "\n",
    "The method works by finding the points where the gradient of $ f(x, y) $ is proportional to the gradient of $ g(x, y) $, which ensures that the constraint is satisfied.\n",
    "\n",
    "---\n",
    "\n",
    "### **Steps to Solve Constrained Optimization Using Lagrange Multipliers**\n",
    "1. **Define the Objective Function and Constraint**:\n",
    "   - Objective: $ f(x, y) $.\n",
    "   - Constraint: $ g(x, y) = 0 $.\n",
    "\n",
    "2. **Set Up the Lagrangian**:\n",
    "   $\n",
    "   \\mathcal{L}(x, y, \\lambda) = f(x, y) - \\lambda \\cdot g(x, y)\n",
    "   $\n",
    "\n",
    "3. **Compute the Partial Derivatives**:\n",
    "   Take the partial derivatives of $ \\mathcal{L} $ with respect to $ x $, $ y $, and $ \\lambda $, and set them equal to zero:\n",
    "   $\n",
    "   \\frac{\\partial \\mathcal{L}}{\\partial x} = 0, \\quad \\frac{\\partial \\mathcal{L}}{\\partial y} = 0, \\quad \\frac{\\partial \\mathcal{L}}{\\partial \\lambda} = 0\n",
    "   $\n",
    "\n",
    "4. **Solve the System of Equations**:\n",
    "   Solve the equations obtained in Step 3 to find the values of $ x $, $ y $, and $ \\lambda $.\n",
    "\n",
    "5. **Verify the Solution**:\n",
    "   Check whether the solution is a maximum, minimum, or saddle point (optional).\n",
    "\n",
    "---\n",
    "\n",
    "### **Why Does This Work?**\n",
    "The method of Lagrange multipliers works because:\n",
    "- At the optimal point, the gradient of the objective function $ \\nabla f $ must be parallel to the gradient of the constraint $ \\nabla g $.\n",
    "- The Lagrange multiplier $ \\lambda $ ensures this proportionality:\n",
    "  $\n",
    "  \\nabla f = \\lambda \\nabla g\n",
    "  $\n",
    "- This ensures that the constraint $ g(x, y) = 0 $ is satisfied while optimizing $ f(x, y) $.\n",
    "\n",
    "---\n",
    "\n",
    "### **Example**\n",
    "Let’s revisit the example:\n",
    "\n",
    "#### Problem:\n",
    "Minimize $ f(x, y) = x^2 + y^2 $ subject to $ x + y = 1 $.\n",
    "\n",
    "#### Solution:\n",
    "1. **Define the Lagrangian**:\n",
    "   $\n",
    "   \\mathcal{L}(x, y, \\lambda) = x^2 + y^2 - \\lambda (x + y - 1)\n",
    "   $\n",
    "\n",
    "2. **Compute the Partial Derivatives**:\n",
    "   $\n",
    "   \\frac{\\partial \\mathcal{L}}{\\partial x} = 2x - \\lambda = 0 \\quad \\implies \\quad 2x = \\lambda\n",
    "   $\n",
    "   $\n",
    "   \\frac{\\partial \\mathcal{L}}{\\partial y} = 2y - \\lambda = 0 \\quad \\implies \\quad 2y = \\lambda\n",
    "   $\n",
    "   $\n",
    "   \\frac{\\partial \\mathcal{L}}{\\partial \\lambda} = -(x + y - 1) = 0 \\quad \\implies \\quad x + y = 1\n",
    "   $\n",
    "\n",
    "3. **Solve the System of Equations**:\n",
    "   From $ 2x = \\lambda $ and $ 2y = \\lambda $, we get $ x = y $.\n",
    "   Substitute $ x = y $ into $ x + y = 1 $:\n",
    "   $\n",
    "   x + x = 1 \\quad \\implies \\quad x = \\frac{1}{2}, \\quad y = \\frac{1}{2}\n",
    "   $\n",
    "   Substitute $ x = \\frac{1}{2} $ into $ 2x = \\lambda $:\n",
    "   $\n",
    "   \\lambda = 1\n",
    "   $\n",
    "\n",
    "4. **Solution**:\n",
    "   The optimal point is $ (x, y) = \\left( \\frac{1}{2}, \\frac{1}{2} \\right) $, and the Lagrange multiplier is $ \\lambda = 1 $.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Takeaways**\n",
    "- Constrained optimization involves optimizing a function subject to constraints.\n",
    "- The method of Lagrange multipliers introduces a new variable $ \\lambda $ to incorporate the constraint into the problem.\n",
    "- The solution is found by solving a system of equations derived from the Lagrangian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constrained Optimization using method of Language Multipliers\n",
    "\n",
    "def constrainedOptimization():\n",
    "    x, y, lamda = sp.symbols('x y lambda')\n",
    "    # Define the objective function and constraint\n",
    "    f = x**2 + y**2\n",
    "    g = x + y - 1\n",
    "    # Define the Lagrangian function\n",
    "    L = f - lamda * g\n",
    "    partialX,  partialY = sp.diff(L, x), sp.diff(L, y)\n",
    "    partialLambda = sp.diff(L, lamda)\n",
    "    criticalPoints = sp.solve([partialX, partialY, partialLambda], (x, y, lamda))\n",
    "    print(\"Critical point(s):\", criticalPoints)\n",
    "constrainedOptimization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To **analyze and optimize a simple cost function for linear regression**, we use the **mean squared error (MSE)** as the cost function. The goal is to minimize this cost function to find the best-fit line for the given data.\n",
    "\n",
    "---\n",
    "\n",
    "### **Linear Regression Overview**\n",
    "Linear regression models the relationship between a dependent variable $ y $ and one or more independent variables $ x $ using a linear equation:\n",
    "$\n",
    "y = mx + b\n",
    "$\n",
    "where:\n",
    "- $ m $ is the slope of the line.\n",
    "- $ b $ is the y-intercept.\n",
    "\n",
    "Given a dataset $ \\{(x_1, y_1), (x_2, y_2), \\dots, (x_n, y_n)\\} $, we want to find the values of $ m $ and $ b $ that minimize the difference between the predicted values $ \\hat{y}_i = mx_i + b $ and the actual values $ y_i $.\n",
    "\n",
    "---\n",
    "\n",
    "### **Cost Function: Mean Squared Error (MSE)**\n",
    "The cost function for linear regression is the **mean squared error (MSE)**, which measures the average squared difference between the predicted and actual values:\n",
    "$\n",
    "J(m, b) = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 = \\frac{1}{n} \\sum_{i=1}^n (y_i - (mx_i + b))^2\n",
    "$\n",
    "Here:\n",
    "- $ J(m, b) $ is the cost function.\n",
    "- $ n $ is the number of data points.\n",
    "- $ y_i $ is the actual value.\n",
    "- $ \\hat{y}_i = mx_i + b $ is the predicted value.\n",
    "\n",
    "---\n",
    "\n",
    "### **Optimizing the Cost Function**\n",
    "To minimize $ J(m, b) $, we use **gradient descent**, an iterative optimization algorithm. The steps are:\n",
    "\n",
    "1. **Initialize Parameters**:\n",
    "   Start with initial guesses for $ m $ and $ b $ (e.g., $ m = 0 $, $ b = 0 $).\n",
    "\n",
    "2. **Compute the Gradient**:\n",
    "   The gradient of $ J(m, b) $ with respect to $ m $ and $ b $ is:\n",
    "   $\n",
    "   \\frac{\\partial J}{\\partial m} = -\\frac{2}{n} \\sum_{i=1}^n x_i (y_i - \\hat{y}_i)\n",
    "   $\n",
    "   $\n",
    "   \\frac{\\partial J}{\\partial b} = -\\frac{2}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)\n",
    "   $\n",
    "\n",
    "3. **Update Parameters**:\n",
    "   Update $ m $ and $ b $ using the gradient descent update rule:\n",
    "   $\n",
    "   m \\leftarrow m - \\eta \\frac{\\partial J}{\\partial m}\n",
    "   $\n",
    "   $\n",
    "   b \\leftarrow b - \\eta \\frac{\\partial J}{\\partial b}\n",
    "   $\n",
    "   where $ \\eta $ is the **learning rate** (step size).\n",
    "\n",
    "4. **Repeat**:\n",
    "   Repeat Steps 2 and 3 until the cost function converges to a minimum.\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**\n",
    "- The cost function for linear regression is the **mean squared error (MSE)**.\n",
    "- Gradient descent is used to minimize the cost function by iteratively updating the parameters $ m $ and $ b $.\n",
    "- The optimal values of $ m $ and $ b $ give the best-fit line for the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze and Optimize a simple cost function for linear regression\n",
    "import numpy as np\n",
    "\n",
    "# Generate some sample data\n",
    "np.random.seed(42)\n",
    "X = 2 * np.random.rand(100, 1)\n",
    "y = 4 + 3 * X + np.random.randn(100, 1)\n",
    "\n",
    "# Add a bias term (x0 = 1) to X\n",
    "xB = np.c_[np.ones((100, 1)), X]\n",
    "\n",
    "# Initialize parameters\n",
    "m, b = 0, 0\n",
    "learningRate = 0.1\n",
    "iterations = 1000\n",
    "n = len(X)\n",
    "\n",
    "# Gradient descent\n",
    "for iteration in range(iterations):\n",
    "    # Predictions\n",
    "    yPrediction = m * X + b\n",
    "    \n",
    "    # Compute gradients\n",
    "    gradientM = (-2/n) * np.sum(X * (y - yPrediction))\n",
    "    gradientB = (-2/n) * np.sum(y - yPrediction)\n",
    "    \n",
    "    # Update parameters\n",
    "    m = m - learningRate * gradientM\n",
    "    b = b - learningRate * gradientB\n",
    "\n",
    "# Print the final parameters\n",
    "print(f\"Optimal slope (m): {m}\")\n",
    "print(f\"Optimal intercept (b): {b}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -- Integration --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the definite Integral of f(x) = x**2 over [0, 1]\n",
    "\n",
    "# Symbolic\n",
    "\n",
    "def integralization():\n",
    "    x = sp.symbols('x')\n",
    "    f = x**2\n",
    "    defIntegral = sp.integrate(f, x)\n",
    "    return defIntegral\n",
    "integralization()\n",
    "\n",
    "\n",
    "# Numerically\n",
    "\n",
    "def f(x):\n",
    "    return x**2\n",
    "\n",
    "def RiemannSumAndTrapezoidal():\n",
    "    intervalA, intervalB = 0, 1\n",
    "    subIntervals = 1000\n",
    "    xValues = np.linspace(intervalA, intervalB, subIntervals + 1)\n",
    "    yValues = f(xValues)\n",
    "    dx = (intervalB - intervalA) / subIntervals\n",
    "    riemannSum = np.sum(yValues[:-1]) * dx\n",
    "    print(f\"Approximate definite integral of x^2 over [0, 1] (Riemann sum): {riemannSum}\")\n",
    "    trapezoidalSum = np.trapezoid(yValues, x=xValues)\n",
    "    print(f\"Approximate definite integral of x^2 over [0, 1] (Trapezoidal rule): {trapezoidalSum}\")\n",
    "    return riemannSum, trapezoidalSum\n",
    "\n",
    "RiemannSumAndTrapezoidal()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approximation of Area Under Curve Using Trapezoidal Rule\n",
    "\n",
    "# Define the function\n",
    "def f(x):\n",
    "    return x**2\n",
    "\n",
    "# Define the interval [a, b] and the number of subintervals\n",
    "a, b = 0, 1\n",
    "n = 1000  # Number of subintervals\n",
    "\n",
    "# Generate x values\n",
    "xValues = np.linspace(a, b, n + 1)\n",
    "\n",
    "# Compute y values\n",
    "yValues = f(xValues)\n",
    "\n",
    "# Compute the width of each subinterval\n",
    "h = (b - a) / n\n",
    "\n",
    "# Apply the trapezoidal rule\n",
    "trapezoidalSum = (h / 2) * (yValues[0] + 2 * np.sum(yValues[1:-1]) + yValues[-1])\n",
    "\n",
    "# Print the result\n",
    "print(f\"Approximate area under f(x) = x^2 over [0, 1] (Trapezoidal rule): {trapezoidalSum}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Simpson's Rule** is a numerical method for approximating definite integrals. It is more accurate than the trapezoidal rule because it approximates the curve using quadratic polynomials instead of straight lines.\n",
    "\n",
    "---\n",
    "\n",
    "### **Simpson's Rule Formula**\n",
    "For a function $ f(x) $ over the interval $[a, b]$, Simpson's Rule is given by:\n",
    "\n",
    "\n",
    "$\n",
    "\\int_a^b f(x) \\, dx \\approx \\frac{h}{3} \\left[ f(x_0) + 4f(x_1) + 2f(x_2) + 4f(x_3) + \\dots + 2f(x_{n-2}) + 4f(x_{n-1}) + f(x_n) \\right]\n",
    "$\n",
    "\n",
    "\n",
    "where:\n",
    "- $ h = \\frac{b - a}{n} $ is the width of each subinterval.\n",
    "- $ n $ is the number of subintervals (must be even).\n",
    "- $ x_0 = a, x_1, x_2, \\dots, x_n = b $ are the points dividing the interval $[a, b]$ into $ n $ subintervals.\n",
    "\n",
    "---\n",
    "\n",
    "### **Steps to Implement Simpson's Rule**\n",
    "1. **Divide the interval $[a, b]$ into $ n $ subintervals** of equal width $ h = \\frac{b - a}{n} $. $ n $ must be even.\n",
    "2. **Evaluate the function** at each point $ x_0, x_1, \\dots, x_n $.\n",
    "3. **Apply Simpson's Rule formula** to compute the approximate area.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of Simpson's Rule for numerical Integration\n",
    "\n",
    "# Define the function\n",
    "def f(x):\n",
    "    return x**2\n",
    "\n",
    "def simpsonRule():\n",
    "\n",
    "# Define the interval [a, b] and the number of subintervals\n",
    "    a, b = 0, 1\n",
    "    n = 1000  # Number of subintervals (must be even)\n",
    "\n",
    "# Generate x values\n",
    "    xValues = np.linspace(a, b, n + 1)\n",
    "\n",
    "# Compute y values\n",
    "    yValues = f(xValues)\n",
    "\n",
    "# Compute the width of each subinterval\n",
    "    h = (b - a) / n\n",
    "\n",
    "# Apply Simpson's Rule\n",
    "    simpsonSum = (h / 3) * (\n",
    "        yValues[0] +  # First term\n",
    "        4 * np.sum(yValues[1:-1:2]) +  # Odd-indexed terms\n",
    "        2 * np.sum(yValues[2:-1:2]) +  # Even-indexed terms\n",
    "        yValues[-1]  # Last term\n",
    ")\n",
    "\n",
    "# Print the result\n",
    "    print(f\"Approximate area under f(x) = x^2 over [0, 1] (Simpson's Rule): {simpsonSum}\")\n",
    "    return simpsonSum\n",
    "simpsonRule()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the Integral of f(x) = e**-x\n",
    "\n",
    "x = sp.symbols('x')\n",
    "f =sp.exp(-x)\n",
    "\n",
    "inDefiniteIntegral = sp.integrate(f, x)\n",
    "print(\"InDefinite Integral:\", inDefiniteIntegral)\n",
    "\n",
    "definiteIntegral = sp.integrate(f, (x, 0, 1))\n",
    "print(\"Definite Integral over [0, 1]:\", definiteIntegral)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indefinite Integral of f(x) = sin(x)\n",
    "\n",
    "x = sp.symbols('x')\n",
    "f = sp.sin(x)\n",
    "\n",
    "inDefiniteIntegral = sp.integrate(f, (x))\n",
    "print(\"InDefinite Integral:\", inDefiniteIntegral)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the symbolic variables\n",
    "x, y = sp.symbols('x y')\n",
    "\n",
    "# Define the function\n",
    "f = x**2 + y**2\n",
    "\n",
    "# Compute the double integral over x in [0, 1] and y in [0, 1]\n",
    "doubleIntegral = sp.integrate(f, (x, 0, 1), (y, 0, 1))\n",
    "\n",
    "# Print the result\n",
    "print(f\"Double integral of x^2 + y^2 over [0, 1] x [0, 1]: {doubleIntegral}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the **Cumulative Distribution Function (CDF)** for a given **Probability Density Function (PDF)**, you need to integrate the PDF from the lower bound of its domain up to a specific point. The CDF gives the probability that a random variable $ X $ takes a value less than or equal to $ x $.\n",
    "\n",
    "---\n",
    "\n",
    "### **Mathematical Definition**\n",
    "For a continuous random variable $ X $ with PDF $ f(x) $, the CDF $ F(x) $ is defined as:\n",
    "$\n",
    "F(x) = P(X \\leq x) = \\int_{-\\infty}^x f(t) \\, dt\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "### **Steps to Compute the CDF**\n",
    "1. **Identify the PDF**:\n",
    "   - The PDF $ f(x) $ must satisfy:\n",
    "     $\n",
    "     f(x) \\geq 0 \\quad \\text{for all } x, \\quad \\text{and} \\quad \\int_{-\\infty}^\\infty f(x) \\, dx = 1\n",
    "     $\n",
    "\n",
    "2. **Integrate the PDF**:\n",
    "   - Compute the integral of $ f(x) $ from the lower bound of its domain up to $ x $.\n",
    "\n",
    "3. **Evaluate the CDF**:\n",
    "   - The result of the integral is the CDF $ F(x) $.\n",
    "\n",
    "---\n",
    "\n",
    "### **Example: Compute CDF for a Given PDF**\n",
    "Let’s compute the CDF for the PDF:\n",
    "$\n",
    "f(x) = \n",
    "\\begin{cases} \n",
    "2x & \\text{if } 0 \\leq x \\leq 1 \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "#### Step 1: Verify the PDF\n",
    "- Check that $ f(x) \\geq 0 $ for all $ x $:\n",
    "  - For $ 0 \\leq x \\leq 1 $, $ f(x) = 2x \\geq 0 $.\n",
    "  - For $ x < 0 $ or $ x > 1 $, $ f(x) = 0 $.\n",
    "- Check that the total area under the PDF is 1:\n",
    "  $\n",
    "  \\int_{-\\infty}^\\infty f(x) \\, dx = \\int_0^1 2x \\, dx = \\left[ x^2 \\right]_0^1 = 1\n",
    "  $\n",
    "\n",
    "#### Step 2: Compute the CDF\n",
    "The CDF $ F(x) $ is:\n",
    "$\n",
    "F(x) = \\int_{-\\infty}^x f(t) \\, dt\n",
    "$\n",
    "\n",
    "- For $ x < 0 $:\n",
    "  $\n",
    "  F(x) = \\int_{-\\infty}^x 0 \\, dt = 0\n",
    "  $\n",
    "\n",
    "- For $ 0 \\leq x \\leq 1 $:\n",
    "  $\n",
    "  F(x) = \\int_{-\\infty}^0 0 \\, dt + \\int_0^x 2t \\, dt = 0 + \\left[ t^2 \\right]_0^x = x^2\n",
    "  $\n",
    "\n",
    "- For $ x > 1 $:\n",
    "  $\n",
    "  F(x) = \\int_{-\\infty}^0 0 \\, dt + \\int_0^1 2t \\, dt + \\int_1^x 0 \\, dt = 0 + 1 + 0 = 1\n",
    "  $\n",
    "\n",
    "So, the CDF is:\n",
    "$\n",
    "F(x) = \n",
    "\\begin{cases} \n",
    "0 & \\text{if } x < 0 \\\\\n",
    "x^2 & \\text{if } 0 \\leq x \\leq 1 \\\\\n",
    "1 & \\text{if } x > 1\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### **Summary**\n",
    "- The CDF $ F(x) $ is computed by integrating the PDF $ f(x) $ from the lower bound of its domain up to $ x $.\n",
    "- For the example $ f(x) = 2x $ (for $ 0 \\leq x \\leq 1 $), the CDF is:\n",
    "  $\n",
    "  F(x) = \n",
    "  \\begin{cases} \n",
    "  0 & \\text{if } x < 0 \\\\\n",
    "  x^2 & \\text{if } 0 \\leq x \\leq 1 \\\\\n",
    "  1 & \\text{if } x > 1\n",
    "  \\end{cases}\n",
    "  $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Cumulative distribution Function for probability density Function\n",
    "\n",
    "# Symbolic\n",
    "\n",
    "def symbolicPdfCdf():\n",
    "    x = sp.symbols('x')\n",
    "    pdf = 2 * x\n",
    "    cdf = sp.integrate(pdf, (x, 0, x))\n",
    "    print(f\"CDF: {cdf}\")\n",
    "    return cdf\n",
    "\n",
    "symbolicPdfCdf()\n",
    "\n",
    "# Numerical\n",
    "# Define the PDF\n",
    "def pdf(x):\n",
    "    return np.where((0 <= x) & (x <= 1), 2 * x, 0)\n",
    "\n",
    "def cdf():\n",
    "    # Discretize the domain\n",
    "    xValues = np.linspace(-1, 2, 1000)  # Domain from -1 to 2 (includes the support of the PDF)\n",
    "    dx = xValues[1] - xValues[0]  # Width of each subinterval\n",
    "\n",
    "# Evaluate the PDF\n",
    "    pdfValues = pdf(xValues)\n",
    "\n",
    "# Compute the CDF using the cumulative sum (Riemann sum approximation)\n",
    "    cdfValues = np.cumsum(pdfValues) * dx\n",
    "\n",
    "# Normalize the CDF to ensure it ends at 1\n",
    "    cdfValues /= cdfValues[-1]\n",
    "\n",
    "# Compute the CDF at a specific point\n",
    "    xValue = 0.5\n",
    "    index = np.searchsorted(xValues, xValue)  # Find the index of x_value in x_values\n",
    "    cdfAtX = cdfValues[index]\n",
    "\n",
    "    print(f\"CDF at x = {xValue}: {cdfAtX}\")\n",
    "    return cdfAtX\n",
    "cdf()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Monte Carlo Integration** is a numerical method for approximating integrals using random sampling. To approximate the area of a circle using Monte Carlo Integration, follow these steps:\n",
    "\n",
    "---\n",
    "\n",
    "### **Problem Setup**\n",
    "We want to approximate the area of a circle with radius $ r = 1 $. The equation of the circle is:\n",
    "$\n",
    "x^2 + y^2 \\leq 1\n",
    "$\n",
    "The area of the circle is $ \\pi r^2 = \\pi $.\n",
    "\n",
    "---\n",
    "\n",
    "### **Monte Carlo Integration Steps**\n",
    "1. **Define the Region**:\n",
    "   - The circle is inscribed in a square with side length $ 2 $ (from $ x = -1 $ to $ 1 $ and $ y = -1 $ to $ 1 $).\n",
    "\n",
    "2. **Generate Random Points**:\n",
    "   - Generate $ N $ random points $ (x, y) $ uniformly distributed within the square.\n",
    "\n",
    "3. **Count Points Inside the Circle**:\n",
    "   - Count the number of points $ N_{\\text{inside}} $ that satisfy $ x^2 + y^2 \\leq 1 $.\n",
    "\n",
    "4. **Approximate the Area**:\n",
    "   - The area of the circle is approximately:\n",
    "     $\n",
    "     \\text{Area} \\approx \\left( \\frac{N_{\\text{inside}}}{N} \\right) \\times \\text{Area of the Square}\n",
    "     $\n",
    "     Here, the area of the square is $ 2 \\times 2 = 4 $.\n",
    "\n",
    "5. **Estimate $ \\pi $**:\n",
    "   - Since the area of the circle is $ \\pi $, we can estimate $ \\pi $ as:\n",
    "     $\n",
    "     \\pi \\approx 4 \\times \\left( \\frac{N_{\\text{inside}}}{N} \\right)\n",
    "     $\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**\n",
    "- Monte Carlo Integration approximates the area of a circle by generating random points and counting the fraction of points that lie inside the circle.\n",
    "- The area of the circle is estimated as:\n",
    "  $\n",
    "  \\text{Area} \\approx 4 \\times \\left( \\frac{N_{\\text{inside}}}{N} \\right)\n",
    "  $\n",
    "- The value of $ \\pi $ is estimated as the area of the circle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monte Carlo Integration to Approximate the Area of circle\n",
    "\n",
    "def circleMonteCarlo(points = 1000):\n",
    "    x = np.random.uniform(-1, 1, points)\n",
    "    y = np.random.uniform(-1, 1, points)\n",
    "    # Count the number of points inside the circle\n",
    "    insideCircle = (x**2 + y**2) <= 1\n",
    "    nInside = np.sum(insideCircle)\n",
    "    # Approximate the area of the circle\n",
    "    areaOfSquare = 4  # Area of the square [-1, 1] x [-1, 1]\n",
    "    areaOfCircle = (nInside / points) * areaOfSquare\n",
    "    # Estimate pi\n",
    "    piEstimate = areaOfCircle\n",
    "    print(f\"Approximate area of the circle: {areaOfCircle}\")\n",
    "    print(f\"Estimated value of pi: {piEstimate}\")\n",
    "    return piEstimate, areaOfCircle\n",
    "\n",
    "circleMonteCarlo()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -- Applications in ML --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure! Let's break this down step by step, starting with the basics and then moving on to the implementation of backpropagation using the chain rule.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **What is a Neural Network?**\n",
    "A **neural network** is a computational model inspired by the structure and function of the human brain. It consists of layers of interconnected nodes (called **neurons**), which process and transmit information. A typical neural network has:\n",
    "- **Input Layer**: Receives the input data.\n",
    "- **Hidden Layers**: Perform computations and transformations on the input data.\n",
    "- **Output Layer**: Produces the final output (e.g., a classification or prediction).\n",
    "\n",
    "Each connection between neurons has a **weight**, and each neuron applies an **activation function** to its input to produce an output.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **What is the Chain Rule?**\n",
    "The **chain rule** is a fundamental concept in calculus used to compute the derivative of a composite function. If $ y = f(g(x)) $, then the derivative of $ y $ with respect to $ x $ is:\n",
    "$\n",
    "\\frac{dy}{dx} = \\frac{dy}{dg} \\cdot \\frac{dg}{dx}\n",
    "$\n",
    "In neural networks, the chain rule is used to compute gradients of the loss function with respect to the weights, which is essential for training the network.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **What is Backpropagation?**\n",
    "**Backpropagation** (short for \"backward propagation of errors\") is an algorithm used to train neural networks. It works by:\n",
    "1. **Forward Pass**: Compute the output of the network for a given input.\n",
    "2. **Compute Loss**: Compare the output with the true value using a **loss function** (e.g., Mean Squared Error or Cross-Entropy Loss).\n",
    "3. **Backward Pass**: Use the chain rule to compute the gradients of the loss with respect to each weight in the network.\n",
    "4. **Update Weights**: Adjust the weights using gradient descent or another optimization algorithm to minimize the loss.\n",
    "\n",
    "Backpropagation is essentially the application of the chain rule to compute gradients efficiently.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Steps to Implement Backpropagation Using the Chain Rule**\n",
    "\n",
    "Let's implement a simple neural network with one hidden layer and use backpropagation to train it.\n",
    "\n",
    "---\n",
    "\n",
    "#### Neural Network Architecture\n",
    "- **Input Layer**: 2 neurons ($ x_1, x_2 $)\n",
    "- **Hidden Layer**: 2 neurons with ReLU activation\n",
    "- **Output Layer**: 1 neuron with sigmoid activation (for binary classification)\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 1: Forward Pass\n",
    "1. Compute the weighted sum of inputs for the hidden layer:\n",
    "   $\n",
    "   z_1 = w_{11}x_1 + w_{12}x_2 + b_1\n",
    "   $\n",
    "   $\n",
    "   z_2 = w_{21}x_1 + w_{22}x_2 + b_2\n",
    "   $\n",
    "2. Apply the ReLU activation function:\n",
    "   $\n",
    "   a_1 = \\text{ReLU}(z_1) = \\max(0, z_1)\n",
    "   $\n",
    "   $\n",
    "   a_2 = \\text{ReLU}(z_2) = \\max(0, z_2)\n",
    "   $\n",
    "3. Compute the weighted sum for the output layer:\n",
    "   $\n",
    "   z_3 = w_{31}a_1 + w_{32}a_2 + b_3\n",
    "   $\n",
    "4. Apply the sigmoid activation function:\n",
    "   $\n",
    "   \\hat{y} = \\sigma(z_3) = \\frac{1}{1 + e^{-z_3}}\n",
    "   $\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 2: Compute Loss\n",
    "Use the binary cross-entropy loss:\n",
    "$\n",
    "L = -\\left( y \\log(\\hat{y}) + (1 - y) \\log(1 - \\hat{y}) \\right)\n",
    "$\n",
    "where $ y $ is the true label (0 or 1) and $ \\hat{y} $ is the predicted output.\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 3: Backward Pass (Chain Rule)\n",
    "1. Compute the derivative of the loss with respect to $ \\hat{y} $:\n",
    "   $\n",
    "   \\frac{\\partial L}{\\partial \\hat{y}} = -\\left( \\frac{y}{\\hat{y}} - \\frac{1 - y}{1 - \\hat{y}} \\right)\n",
    "   $\n",
    "2. Compute the derivative of $ \\hat{y} $ with respect to $ z_3 $:\n",
    "   $\n",
    "   \\frac{\\partial \\hat{y}}{\\partial z_3} = \\hat{y} (1 - \\hat{y})\n",
    "   $\n",
    "3. Compute the derivative of $ L $ with respect to $ z_3 $:\n",
    "   $\n",
    "   \\frac{\\partial L}{\\partial z_3} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z_3}\n",
    "   $\n",
    "4. Compute the gradients for the output layer weights and bias:\n",
    "   $\n",
    "   \\frac{\\partial L}{\\partial w_{31}} = \\frac{\\partial L}{\\partial z_3} \\cdot a_1\n",
    "   $\n",
    "   $\n",
    "   \\frac{\\partial L}{\\partial w_{32}} = \\frac{\\partial L}{\\partial z_3} \\cdot a_2\n",
    "   $\n",
    "   $\n",
    "   \\frac{\\partial L}{\\partial b_3} = \\frac{\\partial L}{\\partial z_3}\n",
    "   $\n",
    "5. Compute the gradients for the hidden layer weights and biases:\n",
    "   - Use the chain rule to propagate the error back to the hidden layer.\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 4: Update Weights\n",
    "Update the weights and biases using gradient descent:\n",
    "$\n",
    "w_{ij} = w_{ij} - \\eta \\frac{\\partial L}{\\partial w_{ij}}\n",
    "$\n",
    "$\n",
    "b_i = b_i - \\eta \\frac{\\partial L}{\\partial b_i}\n",
    "$\n",
    "where $ \\eta $ is the learning rate.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. **Summary**\n",
    "- **Neural Network**: A computational model inspired by the brain.\n",
    "- **Chain Rule**: A calculus rule used to compute derivatives of composite functions.\n",
    "- **Backpropagation**: An algorithm to train neural networks by computing gradients using the chain rule.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BackPropagation for Neural Network for a neural Network using the chain rule\n",
    "\n",
    "# Define activation functions and their derivatives\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def reluDerivative(x):\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "def sigmoids(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoidDerivative(x):\n",
    "    return sigmoids(x) * (1 - sigmoids(x))\n",
    "\n",
    "def backPropagationViaChainRule(learningRate = 0.1, epochs = 1000):\n",
    "    # Initialize weights and biases\n",
    "    np.random.seed(42)\n",
    "    w1 = np.random.randn(2, 2)  # Weights for hidden layer\n",
    "    b1 = np.random.randn(2)\n",
    "    w2 = np.random.randn(2, 1)  # Weights for output layer\n",
    "    b2 = np.random.randn(1)\n",
    "    # Input and true label\n",
    "    X = np.array([[1.0, 2.0]])  # Input (1 sample, 2 features)\n",
    "    y = np.array([[1.0]])       # True label\n",
    "    # training loop\n",
    "    for epoch in range(epochs):\n",
    "        # Forward pass\n",
    "        z1 = np.dot(X, w1) + b1\n",
    "        a1 = relu(z1)\n",
    "        z2 = np.dot(a1, w2) + b2\n",
    "        yPred = sigmoids(z2)\n",
    "\n",
    "    # Compute loss (binary cross-entropy)\n",
    "        loss = - (y * np.log(yPred) + (1 - y) * np.log(1 - yPred))\n",
    "\n",
    "    # Backward pass\n",
    "        dLdyPred = - (y / yPred - (1 - y) / (1 - yPred))\n",
    "        dyPredDz2 = sigmoidDerivative(z2)\n",
    "        dLdz2 = dLdyPred * dyPredDz2\n",
    "\n",
    "    # Gradients for output layer\n",
    "        dLdw2 = np.dot(a1.T, dLdz2)\n",
    "        dLdb2 = np.sum(dLdz2, axis=0)\n",
    "\n",
    "    # Gradients for hidden layer\n",
    "        dLda1 = np.dot(dLdz2, w2.T)\n",
    "        da1Dz1 = reluDerivative(z1)\n",
    "        dLdz1 = dLda1 * da1Dz1\n",
    "\n",
    "        dLdw1 = np.dot(X.T, dLdz1)\n",
    "        dLdb1 = np.sum(dLdz1, axis=0)\n",
    "\n",
    "    # Update weights and biases\n",
    "        w2 -= learningRate * dLdw2\n",
    "        b2 -= learningRate * dLdb2\n",
    "        w1 -= learningRate * dLdw1\n",
    "        b1 -= learningRate * dLdb1\n",
    "\n",
    "    # Print loss every 100 epochs\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss[0][0]}\")\n",
    "    return f\"Final Prediction{yPred}\"\n",
    "\n",
    "backPropagationViaChainRule()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the **gradient of the loss function** for **linear regression**, we need to follow these steps:\n",
    "\n",
    "1. Define the **linear regression model**.\n",
    "2. Define the **loss function** (typically Mean Squared Error).\n",
    "3. Compute the **partial derivatives** of the loss function with respect to the model parameters (weights and bias).\n",
    "4. Use these gradients to update the model parameters using **gradient descent**.\n",
    "\n",
    "Let’s go through each step in detail.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Linear Regression Model\n",
    "The linear regression model predicts the output $ \\hat{y} $ as:\n",
    "\n",
    "\n",
    "$\n",
    "\\hat{y} = w \\cdot x + b\n",
    "$\n",
    "\n",
    "where:\n",
    "- $ x $ is the input feature,\n",
    "- $ w $ is the weight,\n",
    "- $ b $ is the bias,\n",
    "- $ \\hat{y} $ is the predicted output.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Loss Function (Mean Squared Error)\n",
    "The **Mean Squared Error (MSE)** loss function measures the difference between the predicted output $ \\hat{y} $ and the true output $ y $:\n",
    "\n",
    "\n",
    "$\n",
    "L(w, b) = \\frac{1}{2N} \\sum_{i=1}^N (\\hat{y}_i - y_i)^2\n",
    "$\n",
    "\n",
    "\n",
    "where:\n",
    "- $ N $ is the number of data points,\n",
    "- $ \\hat{y}_i = w \\cdot x_i + b $ is the predicted output for the $ i $-th data point,\n",
    "- $ y_i $ is the true output for the $ i $-th data point.\n",
    "\n",
    "The factor $ \\frac{1}{2} $ is included to simplify the derivative calculation.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Compute the Gradients\n",
    "We need to compute the partial derivatives of the loss function $ L(w, b) $ with respect to $ w $ and $ b $.\n",
    "\n",
    "#### Gradient with Respect to $ w $:\n",
    "\n",
    "\n",
    "$\n",
    "\\frac{\\partial L}{\\partial w} = \\frac{1}{N} \\sum_{i=1}^N (\\hat{y}_i - y_i) \\cdot x_i\n",
    "$\n",
    "\n",
    "\n",
    "#### Gradient with Respect to $ b $:\n",
    "\n",
    "\n",
    "$\n",
    "\\frac{\\partial L}{\\partial b} = \\frac{1}{N} \\sum_{i=1}^N (\\hat{y}_i - y_i)\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Update the Parameters Using Gradient Descent\n",
    "Using the gradients, we update the parameters $ w $ and $ b $ as follows:\n",
    "$\n",
    "w = w - \\eta \\frac{\\partial L}{\\partial w}\n",
    "$\n",
    "$\n",
    "b = b - \\eta \\frac{\\partial L}{\\partial b}\n",
    "$\n",
    "where $ \\eta $ is the **learning rate**.\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Summary\n",
    "- The gradient of the loss function for linear regression is computed using partial derivatives.\n",
    "- The gradients are used to update the model parameters $ w $ and $ b $ using gradient descent.\n",
    "- The implementation involves a forward pass (compute predictions), a backward pass (compute gradients), and parameter updates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Gradient of the loss function for linear regression\n",
    "\n",
    "def gradientLossForLinearRegression(epochs = 1000, learningRate = 0.1):\n",
    "    # Generate some sample data\n",
    "    np.random.seed(42)\n",
    "    X = 2 * np.random.rand(100, 1)  # 100 data points, 1 feature\n",
    "    y = 4 + 3 * X + np.random.randn(100, 1)  # y = 4 + 3x + noise\n",
    "\n",
    "    # Initialize parameters\n",
    "    w = np.random.randn(1)  # Weight\n",
    "    b = np.random.randn(1)  # Bias\n",
    "\n",
    "    # Training Loop\n",
    "    for epoch in range(epochs):\n",
    "    # Compute predictions\n",
    "        yPrediction = w * X + b\n",
    "\n",
    "    # Compute the loss (Mean Squared Error)\n",
    "        loss = (1 / (2 * len(X))) * np.sum((yPrediction - y) ** 2)\n",
    "\n",
    "    # Compute gradients\n",
    "        dw = (1 / len(X)) * np.sum((yPrediction - y) * X)  # Gradient for w\n",
    "        db = (1 / len(X)) * np.sum(yPrediction - y)        # Gradient for b\n",
    "\n",
    "    # Update parameters\n",
    "        w -= learningRate * dw\n",
    "        b -= learningRate * db\n",
    "\n",
    "    # Print loss every 100 epochs\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss}, w: {w}, b: {b}\")\n",
    "        # Final parameters\n",
    "        print(\"Final parameters:\")\n",
    "        print(f\"w: {w}, b: {b}\")\n",
    "        return w, b\n",
    "    \n",
    "gradientLossForLinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **softmax function** is commonly used in machine learning, particularly in classification tasks, to convert a vector of raw scores (logits) into probabilities. It ensures that the output values are positive and sum to 1, making them interpretable as probabilities.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **Softmax Function**\n",
    "For a vector $ \\mathbf{z} = [z_1, z_2, \\dots, z_n] $, the softmax function is defined as:\n",
    "$\n",
    "\\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^n e^{z_j}}\n",
    "$\n",
    "where:\n",
    "- $ z_i $ is the $ i $-th element of the input vector $ \\mathbf{z} $,\n",
    "- $ e^{z_i} $ is the exponential of $ z_i $,\n",
    "- The denominator $ \\sum_{j=1}^n e^{z_j} $ ensures that the output values sum to 1.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Derivative of the Softmax Function**\n",
    "The derivative of the softmax function is required for backpropagation in neural networks. There are two cases for the derivative:\n",
    "\n",
    "#### Case 1: Derivative of $ \\text{softmax}(z_i) $ with respect to $ z_i $:\n",
    "$\n",
    "\\frac{\\partial \\text{softmax}(z_i)}{\\partial z_i} = \\text{softmax}(z_i) \\cdot (1 - \\text{softmax}(z_i))\n",
    "$\n",
    "\n",
    "#### Case 2: Derivative of $ \\text{softmax}(z_i) $ with respect to $ z_j $ (where $ j \\neq i $):\n",
    "$\n",
    "\\frac{\\partial \\text{softmax}(z_i)}{\\partial z_j} = -\\text{softmax}(z_i) \\cdot \\text{softmax}(z_j)\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "### 6. **Summary**\n",
    "- The **softmax function** converts logits into probabilities.\n",
    "- The **derivative of the softmax function** is used in backpropagation for training neural networks.\n",
    "- The derivative is computed as a matrix, where:\n",
    "  - The diagonal elements are $ \\text{softmax}(z_i) \\cdot (1 - \\text{softmax}(z_i)) $.\n",
    "  - The off-diagonal elements are $ -\\text{softmax}(z_i) \\cdot \\text{softmax}(z_j) $.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the softmax function and it's derivatives\n",
    "\n",
    "def softmax(z):\n",
    "    # Subtract the max value for numerical stability\n",
    "    expZ = np.exp(z - np.max(z))\n",
    "    return expZ / np.sum(expZ)\n",
    "\n",
    "def softmaxDerivative(z):\n",
    "    s = softmax(z).reshape(-1, 1)  # Reshape to column vector\n",
    "    return np.diagflat(s) - np.dot(s, s.T)\n",
    "\n",
    "z = np.array([2.0, 1.0, 0.1])\n",
    "softmaxValues = softmax(z)\n",
    "print(\"Softmax values:\", softmaxValues)\n",
    "derivativeMatrix = softmaxDerivative(z)\n",
    "print(\"Derivative matrix:\")\n",
    "print(derivativeMatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Numerical differentiation** is a method to approximate the derivative of a function using finite differences. It is useful when the analytical derivative is difficult or impossible to compute. In this explanation, we will:\n",
    "\n",
    "1. Define a **cost function** (e.g., Mean Squared Error).\n",
    "2. Compute its **analytical derivative**.\n",
    "3. Approximate the derivative using **numerical differentiation**.\n",
    "4. Compare the numerical and analytical results.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Define the Cost Function\n",
    "Let’s use the **Mean Squared Error (MSE)** as the cost function for a simple linear regression problem:\n",
    "\n",
    "\n",
    "$\n",
    "J(w) = \\frac{1}{2N} \\sum_{i=1}^N (\\hat{y}_i - y_i)^2\n",
    "$\n",
    "\n",
    "\n",
    "where:\n",
    "- $ \\hat{y}_i = w \\cdot x_i $ is the predicted value,\n",
    "- $ y_i $ is the true value,\n",
    "- $ N $ is the number of data points,\n",
    "- $ w $ is the weight (parameter to optimize).\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Analytical Derivative\n",
    "The analytical derivative of $ J(w) $ with respect to $ w $ is:\n",
    "\n",
    "\n",
    "$\n",
    "\\frac{\\partial J}{\\partial w} = \\frac{1}{N} \\sum_{i=1}^N (\\hat{y}_i - y_i) \\cdot x_i\n",
    "$\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Numerical Derivative\n",
    "The numerical derivative of $ J(w) $ can be approximated using the **finite difference method**. The central difference formula is:\n",
    "\n",
    "\n",
    "$\n",
    "\\frac{\\partial J}{\\partial w} \\approx \\frac{J(w + h) - J(w - h)}{2h}\n",
    "$\n",
    "\n",
    "\n",
    "where $ h $ is a small step size (e.g., $ h = 10^{-5} $).\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### 7. Key Points\n",
    "- The **analytical derivative** is exact and derived from calculus.\n",
    "- The **numerical derivative** is an approximation and depends on the step size $ h $.\n",
    "- The difference between the two should be very small (close to 0) if the numerical derivative is accurate.\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Summary\n",
    "- Numerical differentiation is a useful tool for approximating derivatives when analytical solutions are difficult.\n",
    "- The finite difference method is simple and effective for computing numerical derivatives.\n",
    "- Comparing numerical and analytical derivatives helps verify the correctness of implementations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Numerical differentiation and compare it with analytical result of cost function\n",
    "\n",
    "\n",
    "\n",
    "# Define the cost function (MSE)\n",
    "def costFunction(w, X, y):\n",
    "    N = len(X)\n",
    "    yPrediction = w * X\n",
    "    return (1 / (2 * N)) * np.sum((yPrediction - y) ** 2)\n",
    "\n",
    "def analyticalDerivative(w, X, y):\n",
    "    N = len(X)\n",
    "    yPrediction = w * X\n",
    "    return (1 / N) * np.sum((yPrediction - y) * X)\n",
    "def numericalDerivative(w, X, y, h=1e-5):\n",
    "    return (costFunction(w + h, X, y) - costFunction(w - h, X, y)) / (2 * h)\n",
    "\n",
    "\n",
    "def compareAnalyticalNumerical()-> np.float64 | np.float64:\n",
    "    # Generate some sample data\n",
    "    np.random.seed(42)\n",
    "    X = 2 * np.random.rand(100, 1)  # 100 data points, 1 feature\n",
    "    y = 4 + 3 * X + np.random.randn(100, 1)  # y = 4 + 3x + noise\n",
    "\n",
    "# Initialize weight\n",
    "    w = 2.0  # Initial guess for the weight\n",
    "\n",
    "# Compute analytical and numerical derivatives\n",
    "    analytical = analyticalDerivative(w, X, y)\n",
    "    numerical = numericalDerivative(w, X, y)\n",
    "\n",
    "# Print results\n",
    "    print(\"Analytical derivative:\", analytical)\n",
    "    print(\"Numerical derivative:\", numerical)\n",
    "    print(\"Difference:\", abs(analytical - numerical))\n",
    "    return analytical, numerical\n",
    "\n",
    "compareAnalyticalNumerical()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### sigmoid and It's Derivative and  Gradient ReLu implemented above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the divergence of vector field\n",
    "\n",
    "def symbolicDivergence():\n",
    "\n",
    "# Define the symbolic variables\n",
    "    x, y, z = sp.symbols('x y z')\n",
    "\n",
    "# Define the vector field in 3D\n",
    "    fieldX = x**2\n",
    "    fieldY = y**2\n",
    "    fieldZ = z**2\n",
    "\n",
    "# Compute the divergence\n",
    "    divergenceF = sp.diff(fieldX, x) + sp.diff(fieldY, y) + sp.diff(fieldZ, z)\n",
    "    print(\"Divergence of F in 3D:\", divergenceF)\n",
    "\n",
    "# Define the vector field in 2D\n",
    "    fieldX2d = x**2\n",
    "    fieldY2d = y**2\n",
    "\n",
    "# Compute the divergence\n",
    "    divergenceField2d = sp.diff(fieldX2d, x) + sp.diff(fieldY2d, y)\n",
    "    print(\"Divergence of F in 2D:\", divergenceField2d)\n",
    "    return divergenceF, divergenceField2d,\n",
    "\n",
    "symbolicDivergence()\n",
    "\n",
    "# Numerical\n",
    "# Define the vector field in 2D\n",
    "def fieldX(x, y):\n",
    "    return x**2\n",
    "\n",
    "def fieldY(x, y):\n",
    "    return y**2\n",
    "def numericalDivergence(F_x, F_y, x, y, h=1e-5):\n",
    "    # Compute partial derivatives\n",
    "    dFXdx = (fieldX(x + h, y) - fieldX(x - h, y)) / (2 * h)\n",
    "    dFYdy = (fieldY(x, y + h) - fieldY(x, y - h)) / (2 * h)\n",
    "    return dFXdx + dFYdy\n",
    "\n",
    "# Compute the divergence at a specific point\n",
    "xValue = 1.0\n",
    "yValue = 2.0\n",
    "numericalDivergence = numericalDivergence(fieldX, fieldY, xValue, yValue)\n",
    "\n",
    "# Exact divergence at (x, y) = (1, 2)\n",
    "exactDivergence = 2 * xValue + 2 * yValue\n",
    "\n",
    "print(f\"Numerical divergence at (x, y) = ({xValue}, {yValue}): {numericalDivergence}\")\n",
    "print(f\"Exact divergence at (x, y) = ({xValue}, {yValue}): {exactDivergence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement the **gradient of the cross-entropy loss** for **logistic regression**, we need to:\n",
    "\n",
    "1. Define the **logistic regression model**.\n",
    "2. Define the **cross-entropy loss function**.\n",
    "3. Compute the **gradient of the loss** with respect to the model parameters (weights and bias).\n",
    "4. Use the gradient to update the parameters using **gradient descent**.\n",
    "\n",
    "Let’s go through each step in detail.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Logistic Regression Model\n",
    "The logistic regression model predicts the probability $ \\hat{y} $ of a binary outcome (0 or 1) as:\n",
    "$\n",
    "\\hat{y} = \\sigma(w \\cdot x + b)\n",
    "$\n",
    "where:\n",
    "- $ x $ is the input feature,\n",
    "- $ w $ is the weight,\n",
    "- $ b $ is the bias,\n",
    "- $ \\sigma(z) = \\frac{1}{1 + e^{-z}} $ is the **sigmoid function**.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Cross-Entropy Loss Function\n",
    "The **cross-entropy loss** for a single data point is:\n",
    "$\n",
    "L(w, b) = - \\left( y \\log(\\hat{y}) + (1 - y) \\log(1 - \\hat{y}) \\right)\n",
    "$\n",
    "where:\n",
    "- $ y $ is the true label (0 or 1),\n",
    "- $ \\hat{y} $ is the predicted probability.\n",
    "\n",
    "For $ N $ data points, the average loss is:\n",
    "$\n",
    "L(w, b) = -\\frac{1}{N} \\sum_{i=1}^N \\left( y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right)\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Gradient of the Loss\n",
    "The gradient of the loss with respect to $ w $ and $ b $ is computed as follows:\n",
    "\n",
    "#### Gradient with Respect to $ w $:\n",
    "$\n",
    "\\frac{\\partial L}{\\partial w} = \\frac{1}{N} \\sum_{i=1}^N (\\hat{y}_i - y_i) \\cdot x_i\n",
    "$\n",
    "\n",
    "#### Gradient with Respect to $ b $:\n",
    "$\n",
    "\\frac{\\partial L}{\\partial b} = \\frac{1}{N} \\sum_{i=1}^N (\\hat{y}_i - y_i)\n",
    "$\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Summary\n",
    "- The **gradient of the cross-entropy loss** for logistic regression is computed using the formulas:\n",
    "  $\n",
    "  \\frac{\\partial L}{\\partial w} = \\frac{1}{N} \\sum_{i=1}^N (\\hat{y}_i - y_i) \\cdot x_i\n",
    "  $\n",
    "  $\n",
    "  \\frac{\\partial L}{\\partial b} = \\frac{1}{N} \\sum_{i=1}^N (\\hat{y}_i - y_i)\n",
    "  $\n",
    "- The gradients are used to update the parameters $ w $ and $ b $ using gradient descent.\n",
    "- The implementation involves a forward pass (compute predictions), a backward pass (compute gradients), and parameter updates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the sigmoid function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Define the cross-entropy loss function\n",
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    N = len(y_true)\n",
    "    return - (1 / N) * np.sum(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "# Define the gradient of the loss function\n",
    "def gradient(X, y_true, y_pred):\n",
    "    N = len(y_true)\n",
    "    dw = (1 / N) * np.sum((y_pred - y_true) * X, axis=0)  # Gradient for weights\n",
    "    db = (1 / N) * np.sum(y_pred - y_true)                # Gradient for bias\n",
    "    return dw, db\n",
    "\n",
    "# Generate some sample data\n",
    "np.random.seed(42)\n",
    "X = 2 * np.random.rand(100, 1)  # 100 data points, 1 feature\n",
    "y = (4 + 3 * X + np.random.randn(100, 1) > 5).astype(int)  # Binary labels (0 or 1)\n",
    "\n",
    "# Initialize parameters\n",
    "w = np.random.randn(1)  # Weight (shape: (1,))\n",
    "b = np.random.randn(1)  # Bias (shape: (1,))\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.1\n",
    "epochs = 1000\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    # Compute predictions\n",
    "    z = np.dot(X, w) + b\n",
    "    y_pred = sigmoids(z)\n",
    "\n",
    "    # Compute the loss\n",
    "    loss = cross_entropy_loss(y, y_pred)\n",
    "\n",
    "    # Compute gradients\n",
    "    dw, db = gradient(X, y, y_pred)\n",
    "\n",
    "    # Update parameters\n",
    "    w -= learning_rate * dw\n",
    "    b -= learning_rate * db\n",
    "\n",
    "    # Print loss every 100 epochs\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "\n",
    "# Final parameters\n",
    "print(\"Final parameters:\")\n",
    "print(f\"w: {w}, b: {b}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
