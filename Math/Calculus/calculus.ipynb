{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculus\n",
    "\n",
    "I will skip the basics because all the basic material about calculus are easily available in the internet and i you want to learn the basic i would recommend this video that helped me to understand the basic of calculus theoretically.\n",
    "\n",
    "Here: I would do only challenges and implementations that are important in machine learning and I will Implement Numerically but for finding the differentiation Equation i did symbolically.\n",
    "\n",
    " [Calculus](https://www.youtube.com/watch?v=MO-AExWdl4Q)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Calculus challenges**\n",
    "\n",
    "---\n",
    "\n",
    "### **Basic Differentiation**\n",
    "1. Write a Python function to compute the derivative of a quadratic function.  \n",
    "2. Calculate the derivative of a polynomial function of degree 3.  \n",
    "3. Implement the chain rule for composite functions.  \n",
    "4. Compute the gradient of $ f(x) = x^2 + 3x + 5 $.  \n",
    "5. Differentiate $ f(x) = e^x $ and verify your result numerically.  \n",
    "6. Find the derivative of $ f(x) = \\ln(x) $ for $ x > 0 $.  \n",
    "7. Compute the derivative of $ f(x) = \\sin(x) $ and $ f(x) = \\cos(x) $.  \n",
    "8. Implement numerical differentiation using the finite difference method.  \n",
    "9. Differentiate $ f(x) = x^3 \\sin(x) $ using the product rule.  \n",
    "10. Differentiate $ f(x) = \\frac{x^2}{x+1} $ using the quotient rule.\n",
    "\n",
    "---\n",
    "\n",
    "### **Partial Derivatives**\n",
    "11. Compute the partial derivative of $ f(x, y) = x^2 + y^2 $ with respect to $ x $.  \n",
    "12. Calculate the gradient vector of $ f(x, y) = x^2 + 3xy + y^2 $.  \n",
    "13. Find the partial derivatives of $ f(x, y) = e^{x+y} $.  \n",
    "14. Write a function to compute the Jacobian matrix for $ f(x, y) = [x^2 + y, xy] $.  \n",
    "15. Compute second-order partial derivatives of $ f(x, y) = \\sin(xy) $.  \n",
    "16. Implement a function to calculate the Hessian matrix for $ f(x, y) = x^2 + y^2 + xy $.  \n",
    "17. Visualize the gradient vector field of $ f(x, y) = x^2 - y^2 $ using Python.  \n",
    "18. Find the gradient of $ f(x, y, z) = x^2 + y^2 + z^2 $.  \n",
    "19. Solve $ \\nabla f(x, y) = 0 $ for $ f(x, y) = x^2 + 4xy + y^2 $.  \n",
    "20. Use Python to compute higher-order partial derivatives.\n",
    "\n",
    "---\n",
    "\n",
    "### **Optimization**\n",
    "21. Write a Python function to find the minimum of $ f(x) = x^2 + 4x + 4 $ using derivatives.  \n",
    "22. Implement gradient descent for a 2D quadratic function $ f(x, y) = x^2 + y^2 $.  \n",
    "23. Perform gradient descent to minimize $ f(x) = \\ln(x) + x^2 $.  \n",
    "24. Solve $ f'(x) = 0 $ for $ f(x) = 3x^3 - 4x^2 + 5 $ to find critical points.  \n",
    "25. Visualize the optimization path of gradient descent on $ f(x, y) = x^2 + y^2 $.  \n",
    "26. Use the Hessian matrix to check the convexity of $ f(x, y) = x^2 + y^2 $.  \n",
    "27. Implement stochastic gradient descent for $ f(x) = (x-3)^2 + (y+2)^2 $.  \n",
    "28. Write a function to find the saddle point of $ f(x, y) = x^2 - y^2 $.  \n",
    "29. Solve constrained optimization using the method of Lagrange multipliers.  \n",
    "30. Analyze and optimize a simple cost function for linear regression.\n",
    "\n",
    "---\n",
    "\n",
    "### **Integration**\n",
    "31. Compute the definite integral of $ f(x) = x^2 $ over $ [0, 1] $.  \n",
    "32. Write a Python function to approximate the area under a curve using the trapezoidal rule.  \n",
    "33. Implement Simpson’s rule for numerical integration.  \n",
    "34. Find the integral of $ f(x) = e^{-x} $.  \n",
    "35. Compute the indefinite integral of $ f(x) = \\sin(x) $.  \n",
    "36. Solve $ \\int_{0}^{1} x^2 dx $ using `scipy.integrate`.  \n",
    "37. Integrate $ f(x, y) = x^2 + y^2 $ over $ x \\in [0, 1], y \\in [0, 1] $.  \n",
    "38. Write a function to compute the cumulative distribution function (CDF) from a probability density function (PDF).  \n",
    "39. Visualize the convergence of numerical integration methods on $ f(x) = 1/(1+x^2) $.  \n",
    "40. Use Monte Carlo integration to approximate the area of a circle.\n",
    "\n",
    "---\n",
    "\n",
    "### **Applications in Machine Learning**\n",
    "41. Implement backpropagation for a neural network using the chain rule.  \n",
    "42. Compute the gradient of the loss function for linear regression.  \n",
    "43. Write a function to calculate the softmax function and its derivative.  \n",
    "44. Perform numerical differentiation and compare it with analytical results for a cost function.  \n",
    "45. Implement the sigmoid function and compute its derivative.  \n",
    "46. Derive and compute gradients for the ReLU activation function.  \n",
    "47. Write a Python function to calculate the divergence of a vector field.  \n",
    "48. Visualize the loss surface of a simple neural network using partial derivatives.  \n",
    "49. Implement the gradient of the cross-entropy loss for logistic regression.  \n",
    "50. Combine calculus and linear algebra to solve a ridge regression problem.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -- Basic Differentiation --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General formula or find the derivative of any give function\n",
    "def finiteDifference(f, x, h=1e-5):\n",
    "    return (f(x + h) - f(x - h)) / (2 * h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partial Finite Difference Formula with Respect to X\n",
    "def partialFiniteDifferenceX(f, x, y, h=1e-5):\n",
    "    return (f(x + h, y) - f(x - h, y)) / (2 * h)\n",
    "def partialFiniteDifferenceY(f, x, y, h=1e-5):\n",
    "    return (f(x, y + h) - f(x, y - h)) / (2 * h)\n",
    "\n",
    "def partialFiniteDifferenceXY(f, x, y, h=1e-5):\n",
    "    # Partial derivative with respect to x\n",
    "    dFdx = (f(x + h, y) - f(x - h, y)) / (2 * h)\n",
    "    # Partial derivative with respect to y\n",
    "    dFdy = (f(x, y + h) - f(x, y - h)) / (2 * h)\n",
    "    return dFdx, dFdy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**General Derivative finding formulas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x= sp.symbols('x')\n",
    "y= sp.symbols('y')\n",
    "x, y= sp.symbols('x y')\n",
    "\n",
    "\n",
    "def symbolicDerivativeX(formula):\n",
    "    # Partial Derivative with respect to X\n",
    "    fDX = sp.diff(formula, x)\n",
    "    return fDX\n",
    "def symbolicDerivativeY(formula):\n",
    "    # Partial Derivative with respect to Y\n",
    "    fDY = sp.diff(formula, y)\n",
    "    return fDY\n",
    "\n",
    "def partialSymbolicDerivative(formula):\n",
    "    # Partial Derivative with respect to X\n",
    "    fDX = sp.diff(formula, x)\n",
    "    # Partial Derivative with respect to Y\n",
    "    fDY = sp.diff(formula, y) \n",
    "    return fDX, fDY\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the derivative of quadratic function\n",
    "\n",
    "## Let's quadratic Be: f(x) = ax^2 + bx + c\n",
    "## Quadratic Values: 2x^2 + 3x + 1\n",
    "\n",
    "# Numerical Implementation\n",
    "\n",
    "# Define the quadratic function\n",
    "def f(x):\n",
    "    a, b, c = 2, 3, 1  # Example coefficients\n",
    "    return a * x**2 + b * x + c\n",
    "\n",
    "# Define the derivative using finite differences\n",
    "\n",
    "finiteDifference(f, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polynomial Derivative function degree 3\n",
    "\n",
    "# Cubic Polynomial: f(x) = ax^3 + bx^2 + cx + d\n",
    "\n",
    "def cubePolynomial(x ,a, b, c, d):\n",
    "    # a,b,c,d = 2, -3, 1, 4\n",
    "    coefficients = [a, b, c, d]\n",
    "    root =np.roots(coefficients)\n",
    "    print(\"Polynomial Roots:\", root)\n",
    "    return a * x**3 + b * x**2 + c * x + d\n",
    "x=2\n",
    "a =2\n",
    "b=-3\n",
    "c=1\n",
    "d=4\n",
    "cB = cubePolynomial(x,a,b,c,d)\n",
    "print(f\"Cubic Polynomial at x = {x}: {cB}\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement the Chain Rule for Composite Function\n",
    "# Composite Function: h(x)=f(g(x))\n",
    "# Chain Rule: h′(x)=f′(g(x))⋅g ′(x)\n",
    "\n",
    "\n",
    "# Example: sin(x^2)\n",
    "\n",
    "# Define the functions\n",
    "def g(x):\n",
    "    return x**2  # Inner function\n",
    "\n",
    "def f(u):\n",
    "    return np.sin(u)  # Outer function\n",
    "\n",
    "# Define the derivatives\n",
    "def fDerivatives(u):\n",
    "    return np.cos(u)  # Derivative of f\n",
    "\n",
    "def gDerivative(x):\n",
    "    return 2 * x  # Derivative of g\n",
    "\n",
    "# Chain rule implementation\n",
    "def chainRule(x):\n",
    "    return fDerivatives(g(x)) * gDerivative(x)\n",
    "\n",
    "# Compute the derivative at a specific point\n",
    "x = 2\n",
    "cR = chainRule(x)\n",
    "print(f\"Derivative at x = {x}: {cR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the gradient of f(x) = x**2 + 3x + 5 also f'(x) = 2x + 3\n",
    "\n",
    "# f(x) = x**2 + 3x + 5 = f'(x) = 2x + 3\n",
    "\n",
    "# Symbolic\n",
    "\n",
    "def fDerivative():\n",
    "    x = sp.symbols('x')\n",
    "    f = x**2 + 3*x + 5\n",
    "    fD = sp.diff(f, x)\n",
    "    print(\"Derivative of f'(x):\", fD)\n",
    "    return fD\n",
    "\n",
    "fDerivative()\n",
    "\n",
    "# Define the function\n",
    "def f(x):\n",
    "    return x**2 + 3*x + 5\n",
    "\n",
    "# Define the gradient using finite differences\n",
    "def gradient(f, x, h=1e-5):\n",
    "    return (f(x + h) - f(x - h)) / (2 * h)\n",
    "\n",
    "# Compute the gradient at a specific point\n",
    "xValue = 2  # Point at which to compute the gradient\n",
    "gDValue = gradient(f, xValue)\n",
    "print(f\"Gradient at x = {xValue}: {gDValue}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Differentiate the f(x) = e^x and verify numerically\n",
    "\n",
    "# Formula\n",
    "def findDerivative():\n",
    "    x = sp.symbols('x')\n",
    "    f = sp.exp(x)\n",
    "    fDerivative = sp.diff(f, x)\n",
    "    print(\"Derivative of f'(x):\", fDerivative)\n",
    "    return fDerivative\n",
    "findDerivative()\n",
    "\n",
    "# Numerical\n",
    "\n",
    "def f(x):\n",
    "    return np.exp(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Compute the derivative at a specific point\n",
    "xValue = 2  # Point at which to compute the derivative\n",
    "numericalResult = finiteDifference(f, xValue)\n",
    "exactResult = np.exp(xValue)  # Exact derivative of e^x is e^x\n",
    "\n",
    "print(f\"Numerical derivative at x = {xValue}: {numericalResult}\")\n",
    "print(f\"Exact derivative at x = {xValue}: {exactResult}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find Derivative of f(x) = ln(x) for x > 0\n",
    "# Symbolically\n",
    "def findLnDerivative():\n",
    "    x  = sp.symbols('x')\n",
    "    f = sp.ln(x)\n",
    "    fDerivative = sp.diff(f, x)\n",
    "    print(\"Derivative of f'(x):\", fDerivative)\n",
    "    return fDerivative\n",
    "findLnDerivative()\n",
    "\n",
    "# Numerically\n",
    "\n",
    "def f(x):\n",
    "    return np.log(x)\n",
    "\n",
    "xValue = 2\n",
    "result = finiteDifference(f, xValue)\n",
    "\n",
    "exact_result = 1 / xValue  # Exact derivative of ln(x) is 1/x\n",
    "\n",
    "print(f\"Numerical derivative at x = {xValue}: {result}\")\n",
    "print(f\"Exact derivative at x = {xValue}: {exact_result}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the derivative of f(x) = sin(x) and f(x)= cos(x)\n",
    "\n",
    "# Symbolic\n",
    "def findSinCos():\n",
    "    x = sp.symbols('x')\n",
    "    fSin = sp.sin(x)\n",
    "    fCos = sp.cos(x)\n",
    "    fSinDerivative = sp.diff(fSin, x)\n",
    "    fCosDerivative = sp.diff(fCos, x)\n",
    "    print(\"Derivative Equations:\", fSinDerivative, fCosDerivative)\n",
    "    return fSinDerivative, fCosDerivative\n",
    "\n",
    "findSinCos()\n",
    "\n",
    "# Numerical Computation\n",
    "\n",
    "\n",
    "def fSin(x):\n",
    "    return np.sin(x)\n",
    "def fCos(x):\n",
    "    return np.cos(x)\n",
    "\n",
    "# x value at specific point \n",
    "\n",
    "xValue = np.pi / 4\n",
    "\n",
    "sinDerivative = finiteDifference(fSin, xValue)\n",
    "cosDerivative = finiteDifference(fCos, xValue)\n",
    "\n",
    "print(f\"Numerical derivative of sin(x) at x = π/4: {sinDerivative}\")\n",
    "print(f\"Exact derivative of sin(x) at x = π/4: {np.cos(xValue)}\")\n",
    "print(f\"Numerical derivative of cos(x) at x = π/4: {cosDerivative}\")\n",
    "print(f\"Exact derivative of cos(x) at x = π/4: {-np.sin(xValue)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look Above at Top to find the implementation of Finite Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find differentiation f(x) = x^3sin(x) using Product Rule\n",
    "\n",
    "# Symbolically\n",
    "def findViaProduct():\n",
    "    x = sp.symbols('x')\n",
    "    # Equation\n",
    "    f = x**3 * sp.sin(x)\n",
    "    fDerivative = sp.diff(f, x)\n",
    "    print(\"Derivative Equation:\", fDerivative)\n",
    "    return fDerivative\n",
    " \n",
    "findViaProduct()\n",
    "\n",
    "# Numerically\n",
    "def f(x):\n",
    "    return x**3 * sp.sin(x)\n",
    "\n",
    "xValue = 1\n",
    "result = finiteDifference(f, xValue)\n",
    "print(f\"Numerical derivative at x = {xValue}: {result}\")\n",
    "print(f\"Exact derivative at x = {xValue}: { 3 * (1**2) * np.sin(1) + (1**3) * np.cos(1)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Differentiate f(x) = x**2 /x+ 1 using quotient rule\n",
    "\n",
    "# symbolic\n",
    "\n",
    "def dViaQuotient():\n",
    "\n",
    "# Define the symbolic variable\n",
    "    x = sp.symbols('x')\n",
    "\n",
    "# Equation\n",
    "    f = x**2 / (x + 1)\n",
    "\n",
    "# Compute the derivative using the Quotient Rule\n",
    "    fD = sp.diff(f, x)\n",
    "    # print(\"Derivative of f(x):\", fD)\n",
    "    return fD\n",
    "print(\"DerivativeEquation:\")\n",
    "dViaQuotient()\n",
    "\n",
    "\n",
    "# Numerical\n",
    "\n",
    "def f(x):\n",
    "    return x**2 / (x + 1)\n",
    "\n",
    "xValue = 2\n",
    "\n",
    "result = finiteDifference(f, xValue)\n",
    "print(f\"Numerical derivative at x = {xValue}: {result}\")\n",
    "print(f\"Exact derivative at x = {xValue}: { (2**2 + 2 * 2) / (2 + 1)**2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -- Partial Derivatives ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute partial Derivatives of f(x, y) - x**2 + y**2 with respect to x\n",
    "\n",
    "# Symbolically\n",
    "\n",
    "def symbolicF():\n",
    "    x, y = sp.symbols('x y')\n",
    "    f = x**2 + y**2\n",
    "    # Partial Derivative with respect to X\n",
    "    fDX = sp.diff(f, x)\n",
    "    # Partial Derivative with respect to Y\n",
    "    # fDY = sp.diff(f, y) # But we don't need y so\n",
    "    return fDX\n",
    "symbolicF()\n",
    "\n",
    "# Numerical\n",
    "\n",
    "def f(x, y):\n",
    "    return x**2 + y**2\n",
    "\n",
    "xValue = 2\n",
    "yValue = 3\n",
    "result = partialFiniteDifferenceX(f, xValue, yValue)\n",
    "print(f\"Numerical partial derivative at x = {xValue}, y = {yValue}: {result}\")\n",
    "print(f\"Exact partial derivative at x = {xValue}: {2**xValue}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Gradient Vector of f(x, y) = x**2 + 3*x*y + y**2\n",
    "\n",
    "# Symbolical\n",
    "def symbolicF():\n",
    "    x, y = sp.symbols('x y')\n",
    "    f = x**2 + 3*x*y + y**2\n",
    "    # fDX = sp.diff(f, x)\n",
    "    # fDY = sp.diff(f, y)\n",
    "    # both fDX and fDY are gradients\n",
    "    gradientF = [sp.diff(f, var) for var in (x, y)]\n",
    "    return gradientF\n",
    "symbolicF()\n",
    "\n",
    "# Numerical\n",
    "\n",
    "\n",
    "# Define the function\n",
    "def f(x, y):\n",
    "    return x**2 + 3*x*y + y**2\n",
    "\n",
    "# Define the numerical gradient using finite differences\n",
    "def gradientFiniteDifference(f, x, y, h=1e-5):\n",
    "    # Partial derivative with respect to x\n",
    "    dFdX = (f(x + h, y) - f(x - h, y)) / (2 * h)\n",
    "    # Partial derivative with respect to y\n",
    "    dFdY = (f(x, y + h) - f(x, y - h)) / (2 * h)\n",
    "    return np.array([dFdX, dFdY])\n",
    "\n",
    "# Compute the gradient at a specific point\n",
    "xValue = 1  # Point at which to compute the gradient\n",
    "yValue = 2\n",
    "gradientValue = gradientFiniteDifference(f, xValue, yValue)\n",
    "\n",
    "# Exact gradient at (x, y) = (1, 2)\n",
    "exactGradient = np.array([2*xValue + 3*yValue, 3*xValue + 2*yValue])\n",
    "\n",
    "print(f\"Numerical gradient at (x, y) = ({xValue}, {yValue}): {gradientValue}\")\n",
    "print(f\"Exact gradient at (x, y) = ({xValue}, {yValue}): {exactGradient}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the partial derivative of f(x, y) = e**x+y\n",
    "\n",
    "# Symbolically\n",
    "formula = sp.exp(x + y)\n",
    "partialSymbolicDerivative(formula)\n",
    "\n",
    "# Numerical\n",
    "def f(x, y):\n",
    "    return sp.exp(x + y)\n",
    "xValue = 1\n",
    "yValue = 2\n",
    "\n",
    "resultX, resultY = partialFiniteDifferenceXY(f, xValue, yValue)\n",
    "print(f\"Numerical partial derivative with respect to x at (x, y) = ({xValue}, {yValue}): {resultX}\")\n",
    "# print(f\"Exact partial derivative with respect to x at (x, y) = ({x_value}, {y_value}): {exact_f_x}\")\n",
    "print(f\"Numerical partial derivative with respect to y at (x, y) = ({xValue}, {yValue}): {resultY}\")\n",
    "# print(f\"Exact partial derivative with respect to y at (x, y) = ({x_value}, {y_value}): {exact_f_y}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function to compute the Jacobian Matrix for f(x, y) = [x**2 + y, xy]\n",
    "\n",
    "# Symbolically\n",
    "\n",
    "def symbolicalJacobian():\n",
    "\n",
    "# Define the symbolic variables\n",
    "    x, y = sp.symbols('x y')\n",
    "\n",
    "# Define the vector-valued function\n",
    "    f1 = x**2 + y\n",
    "    f2 = x * y\n",
    "    f = sp.Matrix([f1, f2])\n",
    "\n",
    "    # Compute the Jacobian matrix\n",
    "    J = f.jacobian([x, y])\n",
    "    # print(\"Jacobian matrix of f(x, y):\")\n",
    "    # print(J)\n",
    "    return J\n",
    "\n",
    "symbolicalJacobian()\n",
    "\n",
    "# Numerical\n",
    "def f(x, y):\n",
    "    return np.array([x**2 + y, x * y])\n",
    "\n",
    "def numericalJacobian(f, x, y, h=1e-5):\n",
    "    J = np.zeros((2,2))\n",
    "    # Compute partial derivatives for f1\n",
    "    f1X = (f(x + h, y)[0] - f(x - h, y)[0]) / (2 * h)\n",
    "    f1Y = (f(x, y + h)[0] - f(x, y - h)[0]) / (2 * h)\n",
    "\n",
    "    # Compute partial derivatives for f2\n",
    "    f2X = (f(x + h, y)[1] - f(x - h, y)[1]) / (2 * h)\n",
    "    f2Y = (f(x, y + h)[1] - f(x, y - h)[1]) / (2 * h)\n",
    "        # Fill the Jacobian matrix\n",
    "    J[0, 0] = f1X\n",
    "    J[0, 1] = f1Y\n",
    "    J[1, 0] = f2X\n",
    "    J[1, 1] = f2Y\n",
    "\n",
    "    return J\n",
    "# Compute the Jacobian at a specific point\n",
    "xValue = 1  # Point at which to compute the Jacobian\n",
    "yValue = 2\n",
    "jacobian = numericalJacobian(f, xValue, yValue)\n",
    "\n",
    "# Exact Jacobian at (x, y) = (1, 2)\n",
    "testJacobian = np.array([[2 * xValue, 1],\n",
    "                    [yValue, xValue]])\n",
    "\n",
    "print(f\"Numerical Jacobian at (x, y) = ({xValue}, {yValue}):\")\n",
    "print(jacobian)\n",
    "print(f\"Exact Jacobian at (x, y) = ({xValue}, {yValue}):\")\n",
    "print(testJacobian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second Order Partial Derivative of f(x, y) = sin(xy) \n",
    "\n",
    "import sympy as sp\n",
    "\n",
    "# Define the symbolic variables\n",
    "x, y = sp.symbols('x y')\n",
    "\n",
    "# Define the function\n",
    "f = sp.sin(x * y)\n",
    "\n",
    "# Compute the first-order partial derivatives\n",
    "fX = sp.diff(f, x)\n",
    "fY = sp.diff(f, y)\n",
    "\n",
    "# Compute the second-order partial derivatives\n",
    "fXX = sp.diff(fX, x)\n",
    "fYY = sp.diff(fY, y)\n",
    "fXY = sp.diff(fX, y)\n",
    "fYX = sp.diff(fY, x)\n",
    "\n",
    "print(\"Second partial derivative with respect to x (f_xx):\", fXX)\n",
    "print(\"Second partial derivative with respect to y (f_yy):\", fYY)\n",
    "print(\"Mixed partial derivative with respect to x and y (f_xy):\", fXY)\n",
    "print(\"Mixed partial derivative with respect to y and x (f_yx):\", fYX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of Hessian Matrix for f(x, y) = x**2 + y**2 + xy\n",
    "\n",
    "# Symbolic\n",
    "\n",
    "def symbolicHessianMatrix():\n",
    "\n",
    "# Define the symbolic variables\n",
    "    x, y = sp.symbols('x y')\n",
    "\n",
    "# Define the function\n",
    "    f = x**2 + y**2 + x * y\n",
    "\n",
    "# Compute the Hessian matrix\n",
    "    H = sp.hessian(f, (x, y))\n",
    "    print(\"Hessian matrix of f(x, y):\")\n",
    "    print(H)\n",
    "    return H\n",
    "symbolicHessianMatrix()\n",
    "\n",
    "\n",
    "# Numerical\n",
    "\n",
    "# formula\n",
    "def f(x, y):\n",
    "    return x**2 + y**2 + x * y\n",
    "# Define the numerical Hessian using finite differences\n",
    "def hessianMatrix(f, x, y, h=1e-5):\n",
    "    # Initialize the Hessian matrix\n",
    "    H = np.zeros((2, 2))\n",
    "\n",
    "    # Compute second partial derivatives\n",
    "    # f_xx\n",
    "    H[0, 0] = (f(x + h, y) - 2 * f(x, y) + f(x - h, y)) / h**2\n",
    "    # f_yy\n",
    "    H[1, 1] = (f(x, y + h) - 2 * f(x, y) + f(x, y - h)) / h**2\n",
    "    # f_xy\n",
    "    H[0, 1] = (f(x + h, y + h) - f(x + h, y - h) - f(x - h, y + h) + f(x - h, y - h)) / (4 * h**2)\n",
    "    # f_yx\n",
    "    H[1, 0] = H[0, 1]  # Symmetry of mixed partial derivatives\n",
    "\n",
    "    return H\n",
    "# Compute the Hessian at a specific point\n",
    "xValue = 1  # Point at which to compute the Hessian\n",
    "yValue = 2\n",
    "hessian = hessianMatrix(f, xValue, yValue)\n",
    "\n",
    "print(f\"Numerical Hessian at (x, y) = ({xValue}, {yValue}):\")\n",
    "print(hessian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the gradient of f(x,y,z) = x**2 + y**2 + z++2\n",
    "\n",
    "\n",
    "# Symbolic\n",
    "\n",
    "def symbolicGradient():\n",
    "    # Define the symbolic variables\n",
    "    x, y, z = sp.symbols('x y z')\n",
    "\n",
    "# Define the function\n",
    "    f = x**2 + y**2 + z**2\n",
    "\n",
    "# Compute the gradient\n",
    "    gradient = sp.Matrix([sp.diff(f, var) for var in (x, y, z)])\n",
    "    print(\"Gradient of f(x, y, z):\")\n",
    "    print(gradient)\n",
    "    return gradient\n",
    "symbolicGradient()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define the function\n",
    "def f(x, y, z):\n",
    "    return x**2 + y**2 + z**2\n",
    "\n",
    "# Define the numerical gradient using finite differences\n",
    "def gradientMatrix(f, x, y, z, h=1e-5):\n",
    "    # Partial derivative with respect to x\n",
    "    dFdx = (f(x + h, y, z) - f(x - h, y, z)) / (2 * h)\n",
    "    # Partial derivative with respect to y\n",
    "    dFdy = (f(x, y + h, z) - f(x, y - h, z)) / (2 * h)\n",
    "    # Partial derivative with respect to z\n",
    "    dFdz = (f(x, y, z + h) - f(x, y, z - h)) / (2 * h)\n",
    "    return np.array([dFdx, dFdy, dFdz])\n",
    "\n",
    "# Compute the gradient at a specific point\n",
    "xValue = 1  # Point at which to compute the gradient\n",
    "yValue = 2\n",
    "zValue = 3\n",
    "gValue = gradientMatrix(f, xValue, yValue, zValue)\n",
    "\n",
    "# Exact gradient at (x, y, z) = (1, 2, 3)\n",
    "testGradient = np.array([2 * xValue, 2 * yValue, 2 * zValue])\n",
    "\n",
    "print(f\"Numerical gradient at (x, y, z) = ({xValue}, {yValue}, {zValue}): {gValue}\")\n",
    "print(f\"Exact gradient at (x, y, z) = ({xValue}, {yValue}, {zValue}): {testGradient}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve \\nabla f(x, y) = 0 $ for $ f(x, y) = x^2 + 4xy + y^2 \n",
    "\n",
    "\n",
    "# Define the symbolic variables\n",
    "x, y = sp.symbols('x y')\n",
    "\n",
    "# Define the function\n",
    "f = x**2 + 4*x*y + y**2\n",
    "\n",
    "# Compute the gradient\n",
    "gradientF = [sp.diff(f, var) for var in (x, y)]\n",
    "\n",
    "# Solve the system of equations ∇f(x, y) = 0\n",
    "criticalPoints = sp.solve(gradientF, (x, y))\n",
    "print(\"Critical point(s):\", criticalPoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Python to compute higher-order partial derivatives.\n",
    "\n",
    "# Define the symbolic variables\n",
    "# This way you can find any order of derivatives you want\n",
    "def higherOrderDerivatives():\n",
    "    x, y = sp.symbols('x y')\n",
    "\n",
    "# Define the function\n",
    "    f = x**3 * y**2 + sp.sin(x * y)\n",
    "\n",
    "# Compute the second-order partial derivatives\n",
    "    f_xx = sp.diff(f, x, x)  # Second derivative with respect to x\n",
    "    f_yy = sp.diff(f, y, y)  # Second derivative with respect to y\n",
    "\n",
    "# Compute the third-order mixed partial derivative\n",
    "    f_xxy = sp.diff(f, x, x, y)  # Third derivative: first x, then x, then y\n",
    "\n",
    "    print(\"Second-order partial derivative with respect to x (f_xx):\", f_xx)\n",
    "    print(\"Second-order partial derivative with respect to y (f_yy):\", f_yy)\n",
    "    print(\"Third-order mixed partial derivative (f_xxy):\", f_xxy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -- Optimization --\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the minimum of f(x) = x**2 + 4x + 4 using derivatives\n",
    "\n",
    "def findMinimum():\n",
    "    x = sp.symbols('x')\n",
    "    # function\n",
    "    f = x**2 + 4 * x + 4\n",
    "    # firstDerivative\n",
    "    firstDerivative = sp.diff(f, x)\n",
    "    criticalPoint = sp.solve(firstDerivative, x)\n",
    "    print(\"CriticalPoint(CP):\", criticalPoint)\n",
    "    secondDerivative = sp.diff(firstDerivative, x)\n",
    "    # Analyze the second derivative at the critical point\n",
    "    for point in criticalPoint:\n",
    "        if secondDerivative.subs(x, point) > 0:\n",
    "            print(f\"The function has a local minimum at x = {point}.\")\n",
    "        elif secondDerivative.subs(x, point) < 0:\n",
    "            print(f\"The function has a local maximum at x = {point}.\")\n",
    "        else:\n",
    "            print(f\"The second derivative test is inconclusive at x = {point}.\")\n",
    "    minValue = f.subs(x, criticalPoint[0])\n",
    "    print(f\"The minimum value of f(x) is {minValue} at x = {criticalPoint[0]}.\")\n",
    "    return criticalPoint[0]\n",
    "\n",
    "findMinimum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Descent for 2D Quadratic Function f(x, y) = x**2 + y**2\n",
    "\n",
    "def f(x, y):\n",
    "    return x**2 + y**2\n",
    "\n",
    "def gradientOfF(x, y):\n",
    "    return np.array([2 * x, 2 * y])\n",
    "\n",
    "def gradientDescent2D(learningRate= 0.1, iteration=100, tolerance=1e-5):\n",
    "    # Initial guess\n",
    "    x, y = 5.0, 5.0  # Starting point\n",
    "\n",
    "# Gradient descent algorithm\n",
    "    for i in range(iteration):\n",
    "    # Compute the gradient\n",
    "        grad = gradientOfF(x, y)\n",
    "    \n",
    "    # Update x and y\n",
    "        xNew = x - learningRate * grad[0]\n",
    "        yNew = y - learningRate * grad[1]\n",
    "    \n",
    "    # Check for convergence\n",
    "        if np.linalg.norm([xNew - x, yNew - y]) < tolerance:\n",
    "            print(f\"Converged after {i} iterations.\")\n",
    "            break\n",
    "    \n",
    "    # Update x and y for the next iteration\n",
    "        x, y = xNew, yNew\n",
    "        # Print the results\n",
    "    print(f\"Minimum point: (x, y) = ({x}, {y})\")\n",
    "    print(f\"Minimum value: f(x, y) = {f(x, y)}\")\n",
    "    return x, y\n",
    "\n",
    "gradientDescent2D()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform gradient descent to minimize f(x) = ln(x) + x**2\n",
    "\n",
    "def f(x):\n",
    "    return np.log(x) + x**2\n",
    "def derivativeOfF(x):\n",
    "    return (1 / x) + 2 ** x\n",
    "\n",
    "def minimizeGradientDecent(learningRate = 0.1, iterations=1000, tolerance =1e-6, x = 2.0):\n",
    "    for i in range(iterations):\n",
    "        gradient = derivativeOfF(x)\n",
    "        xNew = x - learningRate * gradient\n",
    "        if xNew <= 0:\n",
    "            xNew = 1e-6\n",
    "        if np.abs(xNew - x) < tolerance:\n",
    "            print(f\"Convergence after {i} iterations\")\n",
    "            break\n",
    "        x = xNew\n",
    "        # Print the results\n",
    "        print(f\"Minimum point: x = {x}\")\n",
    "        print(f\"Minimum value: f(x) = {f(x)}\")\n",
    "        return x\n",
    "    \n",
    "minimizeGradientDecent()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solve f'(x) = 0 for f(x) = 3x**3 - 4x**2 + 5 to find critical point\n",
    "\n",
    "\n",
    "def criticalPointOfDerivative():\n",
    "    x = sp.symbols('x')\n",
    "    f = 3*x**3 - 4*x**2 + 5\n",
    "    derivativeOfF = sp.diff(f, x)\n",
    "    criticalPoints = sp.solve(derivativeOfF, x)\n",
    "    print(\"CriticalPoints(CP):\", criticalPoints)\n",
    "    return criticalPoints\n",
    "\n",
    "criticalPointOfDerivative()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the hessian matrix to check the convexity of f(x, y) = x**2 + y**2\n",
    "\n",
    "def convexity():\n",
    "    x, y = sp.symbols('x y')\n",
    "    f = x**2 + y**2\n",
    "    hessian = sp.hessian(f, (x, y))\n",
    "    print(\"HessianMatrix:\\n\", hessian)\n",
    "    eigenvalues = hessian.eigenvals()\n",
    "    print(\"Eigenvalues of the Hessian matrix:\", eigenvalues)\n",
    "    return print(\"\\nHessian Matrix is definite +ve so It's Strictly Convex\")\n",
    "\n",
    "convexity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement stochastic gradient descent for f(x) = (x -3)**2 + (y + 2)**2 \n",
    "\n",
    "# Define the function\n",
    "def f(x, y):\n",
    "    return (x - 3)**2 + (y + 2)**2\n",
    "\n",
    "# Define the gradient of the function\n",
    "def gradientOfFuc(x, y):\n",
    "    return np.array([2 * (x - 3), 2 * (y + 2)])\n",
    "\n",
    "def stochasticGradientDescent(learningRate=0.1, iterations = 1000, tolerance = 1e-6, x=0.0, y=0.0):\n",
    "    for i in range(iterations):\n",
    "        gradient = gradientOfF(x, y)\n",
    "         # Add noise to the gradient (simulating stochastic)\n",
    "        noise = np.random.normal(0, 0.1, size=2)  # Mean = 0, Std Dev = 0.1\n",
    "        noisyGradient = gradient + noise\n",
    "        xNew = x - learningRate * noisyGradient[0]\n",
    "        yNew = y - learningRate * noisyGradient[1]\n",
    "        if np.linalg.norm([xNew - x, yNew - y]) < tolerance:\n",
    "            print(f\"Converged after {i} iterations.\")\n",
    "            break\n",
    "        x, y = xNew, yNew\n",
    "    print(f\"Optimal point: (x, y) = ({x}, {y})\")\n",
    "    print(f\"Minimum value: f(x, y) = {f(x, y)}\")\n",
    "\n",
    "stochasticGradientDescent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the saddle point of f(x, y) = x**2 - y**2\n",
    "# Note: A critical point is a saddle point if the Hessian matrix has both positive and negative eigenvalues.\n",
    "\n",
    "def saddlePoint():\n",
    "    x, y = sp.symbols('x y')\n",
    "    f = x**2 - y**2\n",
    "    gradientOfF =[sp.diff(f, var) for var in (x, y)]\n",
    "    print(\"Gradient of f(x, y):\", gradientOfF)\n",
    "    criticalPoint = sp.solve(gradientOfF, (x, y))\n",
    "    print(\"Critical point(s):\", criticalPoint)\n",
    "    h = sp.hessian(f, (x, y))\n",
    "    print(\"HessianMatrix:\\n\", h)\n",
    "    eigenVals = h.eigenvals()\n",
    "    print(\"Eigenvalues of the Hessian matrix:\", eigenVals)\n",
    "\n",
    "saddlePoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure! Let me explain **constrained optimization** and the **method of Lagrange multipliers** in a simple and theoretical way.\n",
    "\n",
    "---\n",
    "\n",
    "### **What is Constrained Optimization?**\n",
    "Constrained optimization is the process of finding the **maximum** or **minimum** of a function $ f(x, y, \\dots) $ subject to certain **constraints**. These constraints are additional conditions that the variables $ x, y, \\dots $ must satisfy.\n",
    "\n",
    "#### Example:\n",
    "- **Objective**: Minimize $ f(x, y) = x^2 + y^2 $.\n",
    "- **Constraint**: $ x + y = 1 $.\n",
    "\n",
    "Here, we want to minimize $ f(x, y) $, but $ x $ and $ y $ must satisfy the condition $ x + y = 1 $.\n",
    "\n",
    "---\n",
    "\n",
    "### **What is the Method of Lagrange Multipliers?**\n",
    "The **method of Lagrange multipliers** is a mathematical tool used to solve constrained optimization problems. It introduces a new variable called the **Lagrange multiplier** (denoted by $ \\lambda $) to incorporate the constraint into the optimization problem.\n",
    "\n",
    "#### Key Idea:\n",
    "Instead of directly minimizing or maximizing $ f(x, y) $ subject to $ g(x, y) = 0 $, we create a new function called the **Lagrangian**:\n",
    "$\n",
    "\\mathcal{L}(x, y, \\lambda) = f(x, y) - \\lambda \\cdot g(x, y)\n",
    "$\n",
    "Here:\n",
    "- $ f(x, y) $ is the **objective function**.\n",
    "- $ g(x, y) = 0 $ is the **constraint**.\n",
    "- $ \\lambda $ is the **Lagrange multiplier**.\n",
    "\n",
    "The method works by finding the points where the gradient of $ f(x, y) $ is proportional to the gradient of $ g(x, y) $, which ensures that the constraint is satisfied.\n",
    "\n",
    "---\n",
    "\n",
    "### **Steps to Solve Constrained Optimization Using Lagrange Multipliers**\n",
    "1. **Define the Objective Function and Constraint**:\n",
    "   - Objective: $ f(x, y) $.\n",
    "   - Constraint: $ g(x, y) = 0 $.\n",
    "\n",
    "2. **Set Up the Lagrangian**:\n",
    "   $\n",
    "   \\mathcal{L}(x, y, \\lambda) = f(x, y) - \\lambda \\cdot g(x, y)\n",
    "   $\n",
    "\n",
    "3. **Compute the Partial Derivatives**:\n",
    "   Take the partial derivatives of $ \\mathcal{L} $ with respect to $ x $, $ y $, and $ \\lambda $, and set them equal to zero:\n",
    "   $\n",
    "   \\frac{\\partial \\mathcal{L}}{\\partial x} = 0, \\quad \\frac{\\partial \\mathcal{L}}{\\partial y} = 0, \\quad \\frac{\\partial \\mathcal{L}}{\\partial \\lambda} = 0\n",
    "   $\n",
    "\n",
    "4. **Solve the System of Equations**:\n",
    "   Solve the equations obtained in Step 3 to find the values of $ x $, $ y $, and $ \\lambda $.\n",
    "\n",
    "5. **Verify the Solution**:\n",
    "   Check whether the solution is a maximum, minimum, or saddle point (optional).\n",
    "\n",
    "---\n",
    "\n",
    "### **Why Does This Work?**\n",
    "The method of Lagrange multipliers works because:\n",
    "- At the optimal point, the gradient of the objective function $ \\nabla f $ must be parallel to the gradient of the constraint $ \\nabla g $.\n",
    "- The Lagrange multiplier $ \\lambda $ ensures this proportionality:\n",
    "  $\n",
    "  \\nabla f = \\lambda \\nabla g\n",
    "  $\n",
    "- This ensures that the constraint $ g(x, y) = 0 $ is satisfied while optimizing $ f(x, y) $.\n",
    "\n",
    "---\n",
    "\n",
    "### **Example**\n",
    "Let’s revisit the example:\n",
    "\n",
    "#### Problem:\n",
    "Minimize $ f(x, y) = x^2 + y^2 $ subject to $ x + y = 1 $.\n",
    "\n",
    "#### Solution:\n",
    "1. **Define the Lagrangian**:\n",
    "   $\n",
    "   \\mathcal{L}(x, y, \\lambda) = x^2 + y^2 - \\lambda (x + y - 1)\n",
    "   $\n",
    "\n",
    "2. **Compute the Partial Derivatives**:\n",
    "   $\n",
    "   \\frac{\\partial \\mathcal{L}}{\\partial x} = 2x - \\lambda = 0 \\quad \\implies \\quad 2x = \\lambda\n",
    "   $\n",
    "   $\n",
    "   \\frac{\\partial \\mathcal{L}}{\\partial y} = 2y - \\lambda = 0 \\quad \\implies \\quad 2y = \\lambda\n",
    "   $\n",
    "   $\n",
    "   \\frac{\\partial \\mathcal{L}}{\\partial \\lambda} = -(x + y - 1) = 0 \\quad \\implies \\quad x + y = 1\n",
    "   $\n",
    "\n",
    "3. **Solve the System of Equations**:\n",
    "   From $ 2x = \\lambda $ and $ 2y = \\lambda $, we get $ x = y $.\n",
    "   Substitute $ x = y $ into $ x + y = 1 $:\n",
    "   $\n",
    "   x + x = 1 \\quad \\implies \\quad x = \\frac{1}{2}, \\quad y = \\frac{1}{2}\n",
    "   $\n",
    "   Substitute $ x = \\frac{1}{2} $ into $ 2x = \\lambda $:\n",
    "   $\n",
    "   \\lambda = 1\n",
    "   $\n",
    "\n",
    "4. **Solution**:\n",
    "   The optimal point is $ (x, y) = \\left( \\frac{1}{2}, \\frac{1}{2} \\right) $, and the Lagrange multiplier is $ \\lambda = 1 $.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Takeaways**\n",
    "- Constrained optimization involves optimizing a function subject to constraints.\n",
    "- The method of Lagrange multipliers introduces a new variable $ \\lambda $ to incorporate the constraint into the problem.\n",
    "- The solution is found by solving a system of equations derived from the Lagrangian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constrained Optimization using method of Language Multipliers\n",
    "\n",
    "def constrainedOptimization():\n",
    "    x, y, lamda = sp.symbols('x y lambda')\n",
    "    # Define the objective function and constraint\n",
    "    f = x**2 + y**2\n",
    "    g = x + y - 1\n",
    "    # Define the Lagrangian function\n",
    "    L = f - lamda * g\n",
    "    partialX,  partialY = sp.diff(L, x), sp.diff(L, y)\n",
    "    partialLambda = sp.diff(L, lamda)\n",
    "    criticalPoints = sp.solve([partialX, partialY, partialLambda], (x, y, lamda))\n",
    "    print(\"Critical point(s):\", criticalPoints)\n",
    "constrainedOptimization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To **analyze and optimize a simple cost function for linear regression**, we use the **mean squared error (MSE)** as the cost function. The goal is to minimize this cost function to find the best-fit line for the given data.\n",
    "\n",
    "---\n",
    "\n",
    "### **Linear Regression Overview**\n",
    "Linear regression models the relationship between a dependent variable $ y $ and one or more independent variables $ x $ using a linear equation:\n",
    "$\n",
    "y = mx + b\n",
    "$\n",
    "where:\n",
    "- $ m $ is the slope of the line.\n",
    "- $ b $ is the y-intercept.\n",
    "\n",
    "Given a dataset $ \\{(x_1, y_1), (x_2, y_2), \\dots, (x_n, y_n)\\} $, we want to find the values of $ m $ and $ b $ that minimize the difference between the predicted values $ \\hat{y}_i = mx_i + b $ and the actual values $ y_i $.\n",
    "\n",
    "---\n",
    "\n",
    "### **Cost Function: Mean Squared Error (MSE)**\n",
    "The cost function for linear regression is the **mean squared error (MSE)**, which measures the average squared difference between the predicted and actual values:\n",
    "$\n",
    "J(m, b) = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 = \\frac{1}{n} \\sum_{i=1}^n (y_i - (mx_i + b))^2\n",
    "$\n",
    "Here:\n",
    "- $ J(m, b) $ is the cost function.\n",
    "- $ n $ is the number of data points.\n",
    "- $ y_i $ is the actual value.\n",
    "- $ \\hat{y}_i = mx_i + b $ is the predicted value.\n",
    "\n",
    "---\n",
    "\n",
    "### **Optimizing the Cost Function**\n",
    "To minimize $ J(m, b) $, we use **gradient descent**, an iterative optimization algorithm. The steps are:\n",
    "\n",
    "1. **Initialize Parameters**:\n",
    "   Start with initial guesses for $ m $ and $ b $ (e.g., $ m = 0 $, $ b = 0 $).\n",
    "\n",
    "2. **Compute the Gradient**:\n",
    "   The gradient of $ J(m, b) $ with respect to $ m $ and $ b $ is:\n",
    "   $\n",
    "   \\frac{\\partial J}{\\partial m} = -\\frac{2}{n} \\sum_{i=1}^n x_i (y_i - \\hat{y}_i)\n",
    "   $\n",
    "   $\n",
    "   \\frac{\\partial J}{\\partial b} = -\\frac{2}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)\n",
    "   $\n",
    "\n",
    "3. **Update Parameters**:\n",
    "   Update $ m $ and $ b $ using the gradient descent update rule:\n",
    "   $\n",
    "   m \\leftarrow m - \\eta \\frac{\\partial J}{\\partial m}\n",
    "   $\n",
    "   $\n",
    "   b \\leftarrow b - \\eta \\frac{\\partial J}{\\partial b}\n",
    "   $\n",
    "   where $ \\eta $ is the **learning rate** (step size).\n",
    "\n",
    "4. **Repeat**:\n",
    "   Repeat Steps 2 and 3 until the cost function converges to a minimum.\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**\n",
    "- The cost function for linear regression is the **mean squared error (MSE)**.\n",
    "- Gradient descent is used to minimize the cost function by iteratively updating the parameters $ m $ and $ b $.\n",
    "- The optimal values of $ m $ and $ b $ give the best-fit line for the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze and Optimize a simple cost function for linear regression\n",
    "import numpy as np\n",
    "\n",
    "# Generate some sample data\n",
    "np.random.seed(42)\n",
    "X = 2 * np.random.rand(100, 1)\n",
    "y = 4 + 3 * X + np.random.randn(100, 1)\n",
    "\n",
    "# Add a bias term (x0 = 1) to X\n",
    "xB = np.c_[np.ones((100, 1)), X]\n",
    "\n",
    "# Initialize parameters\n",
    "m, b = 0, 0\n",
    "learningRate = 0.1\n",
    "iterations = 1000\n",
    "n = len(X)\n",
    "\n",
    "# Gradient descent\n",
    "for iteration in range(iterations):\n",
    "    # Predictions\n",
    "    yPrediction = m * X + b\n",
    "    \n",
    "    # Compute gradients\n",
    "    gradientM = (-2/n) * np.sum(X * (y - yPrediction))\n",
    "    gradientB = (-2/n) * np.sum(y - yPrediction)\n",
    "    \n",
    "    # Update parameters\n",
    "    m = m - learningRate * gradientM\n",
    "    b = b - learningRate * gradientB\n",
    "\n",
    "# Print the final parameters\n",
    "print(f\"Optimal slope (m): {m}\")\n",
    "print(f\"Optimal intercept (b): {b}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
