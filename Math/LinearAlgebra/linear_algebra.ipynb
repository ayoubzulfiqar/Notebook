{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Algebra with Python\n",
    "Because all the theory are available on the internet so we will basically dive into the challenges \n",
    "\n",
    "[Source](https://www.w3resource.com/python-exercises/numpy/linear-algebra/index.php)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **Basic Operations**\n",
    "1. Implement scalar multiplication of a vector.  \n",
    "2. Write a function to add two vectors.  \n",
    "3. Compute the dot product of two vectors.  \n",
    "4. Implement vector normalization (L2 norm).  \n",
    "5. Write a function to compute the cross product of two 3D vectors.  \n",
    "6. Multiply two matrices using nested loops.  \n",
    "7. Write a function to calculate the trace of a matrix.  \n",
    "8. Implement matrix transposition.  \n",
    "9. Compute the determinant of a 2x2 matrix.  \n",
    "10. Check if a matrix is symmetric.\n",
    "\n",
    "---\n",
    "\n",
    "### **Intermediate Operations**\n",
    "11. Implement matrix-vector multiplication.  \n",
    "12. Write a function to perform matrix-matrix multiplication.  \n",
    "13. Check if a matrix is orthogonal.  \n",
    "14. Compute the inverse of a 2x2 matrix.  \n",
    "15. Implement Gram-Schmidt orthogonalization for a set of vectors.  \n",
    "16. Calculate the rank of a matrix using row-reduction.  \n",
    "17. Write a function to find the null space of a matrix.  \n",
    "18. Compute the column space of a matrix.  \n",
    "19. Solve a system of linear equations using Gaussian elimination.  \n",
    "20. Implement LU decomposition of a matrix.\n",
    "\n",
    "---\n",
    "\n",
    "### **Eigenvalues and Eigenvectors**\n",
    "21. Write a function to compute eigenvalues of a 2x2 matrix.  \n",
    "22. Find eigenvectors corresponding to eigenvalues.  \n",
    "23. Implement power iteration to approximate the largest eigenvalue.  \n",
    "24. Compute the spectral radius of a matrix.  \n",
    "25. Verify the diagonalization of a matrix using its eigenvalues and eigenvectors.\n",
    "\n",
    "---\n",
    "\n",
    "### **Singular Value Decomposition (SVD)**\n",
    "26. Implement SVD manually for a 2x2 matrix.  \n",
    "27. Write a function to compute the rank of a matrix using SVD.  \n",
    "28. Perform dimensionality reduction using SVD on a dataset.  \n",
    "29. Reconstruct a matrix using its singular values and vectors.  \n",
    "30. Use SVD to approximate the largest singular value of a matrix.\n",
    "\n",
    "---\n",
    "\n",
    "### **Projections and Orthogonality**\n",
    "31. Write a function to project a vector onto another vector.  \n",
    "32. Implement projection of a vector onto a subspace.  \n",
    "33. Verify if two vectors are orthogonal.  \n",
    "34. Compute the orthogonal complement of a vector in 3D space.  \n",
    "35. Find the closest point in a subspace to a given vector.\n",
    "\n",
    "---\n",
    "\n",
    "### **Linear Transformations**\n",
    "36. Apply a rotation transformation to a 2D vector.  \n",
    "37. Implement a scaling transformation for 2D and 3D vectors.  \n",
    "38. Write a function to perform shearing transformation in 2D.  \n",
    "39. Combine rotation, scaling, and translation into an affine transformation.  \n",
    "40. Verify if a given transformation is linear.\n",
    "\n",
    "---\n",
    "\n",
    "### **Applications in ML**\n",
    "41. Implement Principal Component Analysis (PCA) on a dataset.  \n",
    "42. Perform dimensionality reduction using PCA and visualize results.  \n",
    "43. Write a function to compute cosine similarity between two vectors.  \n",
    "44. Implement a recommendation system using matrix factorization.  \n",
    "45. Solve a least squares problem for linear regression using matrix operations.\n",
    "\n",
    "---\n",
    "\n",
    "### **Advanced Challenges**\n",
    "46. Implement the Cholesky decomposition for a symmetric positive-definite matrix.  \n",
    "47. Compute the Moore-Penrose pseudoinverse of a matrix.  \n",
    "48. Solve a linear system using the conjugate gradient method.  \n",
    "49. Verify the stability of a system by checking eigenvalues of its matrix.  \n",
    "50. Simulate a Markov process using transition matrices.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scalar Multiplication of vector\n",
    "# For creativity I created random values \n",
    "vector = np.random.rand(9)\n",
    "scalar = np.random.randint(99)\n",
    "print(vector)\n",
    "print(scalar)\n",
    "\n",
    "print(np.multiply(scalar, vector))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding Two Vector\n",
    "\n",
    "vectorOne = np.random.rand(9)\n",
    "vectorTwo = np.random.rand(9)\n",
    "print(vectorOne)\n",
    "print(vectorTwo)\n",
    "\n",
    "\n",
    "print(np.add(vectorOne,vectorTwo))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dot Product of Two Vectors\n",
    "\n",
    "vectorOne = np.random.rand(9)\n",
    "vectorTwo = np.random.rand(9)\n",
    "print(vectorOne)\n",
    "print(vectorTwo)\n",
    "\n",
    "print(np.dot(vectorOne,vectorTwo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector Normalization (L2 Norm)\n",
    "## Explanation\n",
    "    # The L2 norm is for the shortest distance indicated by a vector. It is called a Euclidean norm too. As in Definition 1.2, substituting 2 for p, the l 2 norm is the square root of the summation of vector/distance squared element magnitudes:\n",
    "\n",
    "    # The L1 norm is a measure of distance or magnitude in vector spaces. For a matrix, the L1 norm is calculated as the sum of the absolute values of its elements.\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "\n",
    "def euclideanNorm(vec)-> float:\n",
    "    norm = 0.0\n",
    "    for _,v in enumerate(vec):\n",
    "        norm += v*v\n",
    "    return sqrt(norm)\n",
    "\n",
    "\n",
    "\n",
    "vector = np.array([-3,2,1,-1,1])\n",
    "# vec = [-3,2,1,-1,1]\n",
    "print(euclideanNorm(vector))\n",
    "print(np.linalg.norm(vector))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Product of Two 3D Vector\n",
    "\n",
    "v13D = np.array([3,2,1])\n",
    "v23D = np.array([1,2,3])\n",
    "\n",
    "print(np.cross(v13D,v23D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiply Two Matrices using nested loops\n",
    "\n",
    "\n",
    "\n",
    "mOne = [[1,2],[3,4]]\n",
    "mTwo = [[5,6],[7,8]]\n",
    "print(mOne)\n",
    "print(mTwo)\n",
    "print(np.multiply(mOne,mTwo))\n",
    "\n",
    "def matrixMultiplication(matrixOne,matrixTwo)-> list[list[int]]:\n",
    "    rows_A:int = len(matrixOne)\n",
    "    cols_A:int = len(matrixOne[0])\n",
    "    rows_B:int = len(matrixTwo)\n",
    "    cols_B:int = len(matrixTwo[0])\n",
    "\n",
    "    if cols_A != rows_B:\n",
    "        raise ValueError(\"Matrices cannot be multiplied. Number of columns in A must be equal to the number of rows in B.\")\n",
    "\n",
    "    result: list[list[int]] = [[0 for _ in range(cols_B)] for _ in range(rows_A)]\n",
    "\n",
    "    for i in range(rows_A):\n",
    "        for j in range(cols_B):\n",
    "            for k in range(cols_A):\n",
    "                result[i][j] += matrixOne[i][k] * matrixTwo[k][j]\n",
    "    print(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the trace of Matrix\n",
    "    # The trace of a matrix is the sum of the diagonal elements of the matrix\n",
    "            # Formula\n",
    "                # tr(A) = \\sum_{i=1}^n a_{ii}\n",
    "m = np.identity(4, dtype=int)\n",
    "print(m)\n",
    "print(np.trace(m))\n",
    "\n",
    "def tracing(matrix)-> int:\n",
    "    diagonalValues =0\n",
    "    for i in range(len(matrix)):\n",
    "            diagonalValues += matrix[i][i]\n",
    "    print(diagonalValues)\n",
    "\n",
    "tracing(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix Transpositions Implementation\n",
    "    # Matrix transposition is the process of switching the rows and columns of a matrix.\n",
    "        # AT)ij = aji\n",
    "\n",
    "m = np.arange(1, 28).reshape(3,3,3)\n",
    "\n",
    "print(m)\n",
    "\n",
    "print(\"Transposed: \\n\",np.transpose(m))\n",
    "\n",
    "# def transposition(matrix):\n",
    "\n",
    "    # return transposed\n",
    "\n",
    "# print(\"FunctionalTransposition:\\n\",transposition(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Compute the determent of 2x2 Matrix\n",
    "    # The determinant of a matrix is the scalar value or number calculated using a square matrix.\n",
    "        # The determinant is only defined for square matrices. A matrix is said to be singular if its determinant is zero. The general formula for the determinant of matrices of any size is very complicated.  \n",
    "\n",
    "print(np.linalg.det(np.arange(1,10).reshape(3,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Matrix Symmetrical\n",
    "    # A symmetric matrix is a square matrix that remains unchanged when its rows and columns are interchanged. In other words, it is equal to its transpose.\n",
    "\n",
    "def isSymmetrical(matrixOne,matrixTwo):\n",
    "    print(np.array_equal(matrixOne,matrixTwo.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --- intermediate ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix Vector Multiplication\n",
    "\n",
    "vec = np.random.rand(3)\n",
    "print(\"Vector:\\n\",vec)\n",
    "mat = np.arange(1,10).reshape(3,3)\n",
    "print(\"3 by 3 Matrix\\n\",mat)\n",
    "\n",
    "# Multiplication shoulD be based on SAME SIZE\n",
    "print(\"\\n Matrix Vector Multiplication: \\n\",mat * vec)\n",
    "print(\"\\n\\n Numpy Multiplication:\",np.multiply(vec, mat))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix Matrix Multiplication\n",
    "matrixOne = np.arange(1,10).reshape(3,3)\n",
    "matrixTwo = np.arange(1,10).reshape(3,3)\n",
    "print(matrixOne * matrixTwo)\n",
    "print(np.multiply(matrixOne, matrixTwo))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Orthogonal Matrix\n",
    "    # A square matrix with real numbers or values is termed as an orthogonal matrix if its transpose is equal to the inverse matrix of it.\n",
    "\n",
    "def orthogonality(m)-> bool:\n",
    "    inverse = np.linalg.inv(m)\n",
    "    transpose = np.transpose(m)\n",
    "    print(transpose == inverse)\n",
    "    return transpose == inverse\n",
    "\n",
    "orthogonality(np.arange(1,5).reshape(2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inverse of Inverse matrix\n",
    "\n",
    "print(np.linalg.inv(np.arange(1,5).reshape(2,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gram-Schmidt Orthogonalization\n",
    "The Gram-Schmidt process is an algorithm in linear algebra that takes a set of linearly independent vectors and produces an orthonormal set of vectors that span the same subspace.\n",
    "\n",
    "- Key Concepts:\n",
    "\n",
    "  - Orthonormal Vectors:\n",
    "    -   Orthogonal: Vectors are orthogonal if their dot product is zero (they are perpendicular).\n",
    "    -   Normalized: A vector is normalized if its length (magnitude) is one.\n",
    "    - Linear Independence: A set of vectors is linearly independent if no vector in the set can be written as a linear combination of the other vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gram-Schmidt Orthogonalization for set of Vectors\n",
    "\n",
    "\n",
    "\n",
    "def gramSchmidt(vectors)-> np.float64:\n",
    "    orthogonalVectors: np.ndarray = []\n",
    "    for v in range(len(vectors)):\n",
    "        u = np.array(v)\n",
    "        for w in range(len(orthogonalVectors)):\n",
    "            u -= np.dot(v, w) * w\n",
    "        if np.linalg.norm(u) > 1e-10:\n",
    "            orthogonalVectors.append(u / np.linalg.norm(u))\n",
    "    return orthogonalVectors\n",
    "\n",
    "vectors = [np.array([1, 1]), np.array([2, 0])] \n",
    "orthonormalBasis = gramSchmidt([[2,2],[2,0]])\n",
    "\n",
    "print(\"Orthonormal Basis:\", np.float64(orthonormalBasis)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrix Rank\n",
    "\n",
    "**1. Understanding Matrix Rank**\n",
    "\n",
    "* **Definition:** The rank of a matrix is the maximum number of linearly independent rows or columns in the matrix. \n",
    "\n",
    "* **Key Concepts:**\n",
    "    * **Linear Independence:** A set of vectors is linearly independent if no vector in the set can be expressed as a linear combination of the other vectors.\n",
    "    * **Row Equivalence:** Two matrices are row equivalent if one can be obtained from the other by a sequence of elementary row operations (swapping rows, multiplying a row by a non-zero scalar, adding a multiple of one row to another).\n",
    "\n",
    "**2. Finding Rank Using Row Reduction**\n",
    "\n",
    "* **Row Reduction:** The process of transforming a matrix into its row-echelon form or reduced row-echelon form (RREF) using elementary row operations.\n",
    "\n",
    "* **Steps:**\n",
    "    1. **Start with the given matrix.**\n",
    "    2. **Perform row operations:**\n",
    "        * **Swap rows:** Interchange the positions of two rows.\n",
    "        * **Multiply a row by a scalar:** Multiply all elements of a row by a non-zero constant.\n",
    "        * **Add a multiple of one row to another:** Add a multiple of one row to another row.\n",
    "    3. **Continue row operations until the matrix is in row-echelon form or RREF:**\n",
    "        * **Row-echelon form:**\n",
    "            * All rows consisting entirely of zeros are located at the bottom of the matrix.\n",
    "            * The first non-zero element (leading coefficient) in each non-zero row is 1.\n",
    "            * The leading coefficient of each row is to the right of the leading coefficient of the row above it.\n",
    "        * **Reduced row-echelon form (RREF):**\n",
    "            * In addition to the conditions of row-echelon form:\n",
    "                * Each column containing a leading 1 has zeros in all other positions.\n",
    "\n",
    "* **Determine Rank:**\n",
    "    * The rank of the matrix is equal to the number of non-zero rows in its row-echelon form or RREF.\n",
    "\n",
    "**Example**\n",
    "\n",
    "Let's find the rank of the following matrix:\n",
    "\n",
    "```\n",
    "A = | 1  2  3 |\n",
    "    | 2  4  6 |\n",
    "    | 1  1  1 | \n",
    "```\n",
    "\n",
    "1. **Row Operations:**\n",
    "   * R2 = R2 - 2*R1 \n",
    "   * R3 = R3 - R1\n",
    "   ```\n",
    "   | 1  2  3 |\n",
    "   | 0  0  0 | \n",
    "   | 0 -1 -2 |\n",
    "   ```\n",
    "   * Swap R2 and R3\n",
    "   ```\n",
    "   | 1  2  3 |\n",
    "   | 0 -1 -2 |\n",
    "   | 0  0  0 |\n",
    "   ```\n",
    "   * R2 = -R2\n",
    "   ```\n",
    "   | 1  2  3 |\n",
    "   | 0  1  2 |\n",
    "   | 0  0  0 |\n",
    "   ```\n",
    "   * R1 = R1 - 2*R2\n",
    "   ```\n",
    "   | 1  0 -1 |\n",
    "   | 0  1  2 |\n",
    "   | 0  0  0 |\n",
    "   ```\n",
    "\n",
    "2. **Determine Rank:** \n",
    "   * The matrix is now in row-echelon form.\n",
    "   * There are 2 non-zero rows.\n",
    "   * **Therefore, the rank of matrix A is 2.**\n",
    "\n",
    "**Key Points:**\n",
    "\n",
    "* Row reduction is a systematic method for determining the rank of a matrix.\n",
    "* The rank of a matrix remains unchanged under elementary row operations.\n",
    "* The rank of a matrix provides valuable information about its properties, such as the number of linearly independent solutions to a system of linear equations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rank of Matrix using Row Reduction\n",
    "    # The rank of a matrix is the number of linearly independent rows or columns in the matrix.\n",
    "\n",
    "\n",
    "def matrixRank(matrix):\n",
    "\n",
    "\n",
    "  rows, columns = matrix.shape\n",
    "  rank = rows\n",
    "  print(\"Rows:\",rows,\"\\n\",\"Columns\", columns)\n",
    "\n",
    "  for i in range(rows):\n",
    "    # Find the pivot row (a row with a non-zero leading coefficient)\n",
    "    pivotRow = i\n",
    "    while pivotRow < rows and np.all(matrix[pivotRow, :i] == 0):\n",
    "      pivotRow += 1\n",
    "\n",
    "    if pivotRow == rows:  # No non-zero pivot found in remaining rows\n",
    "      break\n",
    "\n",
    "    # Swap current row with the pivot row\n",
    "    matrix[[i, pivotRow]] = matrix[[pivotRow, i]]\n",
    "\n",
    "    # Make the leading coefficient 1\n",
    "    matrix[i, :] /= matrix[i, i]\n",
    "\n",
    "    # Eliminate elements below the pivot\n",
    "    for j in range(i + 1, rows):\n",
    "      matrix[j, :] -= matrix[j, i] * matrix[i, :]\n",
    "\n",
    "  return rank\n",
    "\n",
    "m = np.array([[1, 2, 3], \n",
    "                   [2, 4, 6], \n",
    "                   [1, 1, 1],\n",
    "                   [3, 4, 5]])\n",
    "\n",
    "\n",
    "\n",
    "rank = matrixRank(m)\n",
    "print(\"Rank of the matrix:\", rank)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Null Space of a Matrix**\n",
    "\n",
    "**Definition:**\n",
    "\n",
    "The null space of an m x n matrix A, denoted as Nul A, is the set of all vectors **x** in Rⁿ such that:\n",
    "\n",
    "**A x = 0**\n",
    "\n",
    "where **0** is the zero vector in Rᵐ. In simpler terms:\n",
    "\n",
    "* The null space of A consists of all vectors that, when multiplied by A, result in the zero vector.\n",
    "* It represents the set of solutions to the homogeneous system of linear equations represented by A**x** = **0**.\n",
    "\n",
    "**Key Properties:**\n",
    "\n",
    "* **Subspace:** The null space of a matrix is always a subspace of Rⁿ. This means it satisfies the following properties:\n",
    "    * Contains the zero vector.\n",
    "    * Closed under vector addition.\n",
    "    * Closed under scalar multiplication.\n",
    "\n",
    "* **Relationship to Rank:** The dimension of the null space (also called the nullity) is related to the rank of the matrix by the Rank-Nullity Theorem:\n",
    "\n",
    "   **rank(A) + nullity(A) = n** \n",
    "\n",
    "   where n is the number of columns of matrix A.\n",
    "\n",
    "**Finding the Null Space:**\n",
    "\n",
    "1. **Form the Augmented Matrix:** Create the augmented matrix [A | **0**], where **0** is the zero vector with m components.\n",
    "\n",
    "2. **Row Reduce:** Perform row operations (swapping rows, multiplying a row by a scalar, adding a multiple of one row to another) to reduce the augmented matrix to its reduced row-echelon form (RREF).\n",
    "\n",
    "3. **Solve the System:** \n",
    "   - Identify the free variables (variables corresponding to columns without leading 1's in RREF).\n",
    "   - Express the leading variables (variables corresponding to columns with leading 1's) in terms of the free variables.\n",
    "\n",
    "4. **Write the General Solution:** Express the solution vector **x** as a linear combination of vectors involving the free variables. These vectors form a basis for the null space.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Let's find the null space of the following matrix:\n",
    "\n",
    "```\n",
    "A = | 1 2 3 |\n",
    "    | 2 4 6 | \n",
    "```\n",
    "\n",
    "1. **Form the Augmented Matrix:**\n",
    "   ```\n",
    "   [A | 0] = | 1 2 3 | 0 |\n",
    "             | 2 4 6 | 0 |\n",
    "   ```\n",
    "\n",
    "2. **Row Reduce:**\n",
    "   ```\n",
    "   | 1 2 3 | 0 | \n",
    "   | 0 0 0 | 0 | \n",
    "   ```\n",
    "\n",
    "3. **Solve the System:**\n",
    "   - Let x₃ = t (free variable)\n",
    "   - x₂ = s (free variable)\n",
    "   - x₁ = -2s - 3t\n",
    "\n",
    "4. **General Solution:**\n",
    "   ```\n",
    "   x = | -2s - 3t |\n",
    "       |      s     |\n",
    "       |      t     | \n",
    "   ```\n",
    "\n",
    "   This can be written as:\n",
    "\n",
    "   ```\n",
    "   x = s | -2 | + t | -3 |\n",
    "         |  1 |     |  0 |\n",
    "         |  0 |     |  1 |\n",
    "   ```\n",
    "\n",
    "Therefore, the null space of A is the set of all vectors of the form:\n",
    "\n",
    "```\n",
    "s | -2 | + t | -3 | \n",
    "  |  1 |     |  0 |\n",
    "  |  0 |     |  1 |\n",
    "```\n",
    "\n",
    "where s and t are any real numbers.\n",
    "\n",
    "**In Summary:**\n",
    "\n",
    "The null space of a matrix provides valuable insights into the properties of the matrix and the associated linear transformation. It plays a crucial role in various areas of linear algebra and its applications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find Null Space of the Matrix\n",
    "\n",
    "def nullSpaceOfMatrix(matrix, tolerance = 1e-12):\n",
    "    # formula of single value decomposition\n",
    "    _, s, vh = np.linalg.svd(matrix)\n",
    "    nullMask = (s<=tolerance)\n",
    "    nullSpace = np.compress(nullMask, vh, axis=0)\n",
    "    return nullSpace.T\n",
    "\n",
    "\n",
    "m = np.array([[1, 2, 3], [2, 4, 6]])\n",
    "print(nullSpaceOfMatrix(matrix=m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def columnSpaceOfMatrix(matrix, tolerance = 1e-12):\n",
    "    u, s, _ = np.linalg.svd(matrix)\n",
    "    rank = np.sum(s > tolerance)\n",
    "    columnSpace = u[:, :rank]\n",
    "    return columnSpace\n",
    "\n",
    "matrix = np.array([[2, 4, 1], [6, 12, 3], [4, 8, 2]])\n",
    "print(\"ColumnSpace:\\n\", columnSpaceOfMatrix(matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Gaussian Elimination**\n",
    "\n",
    "**What is it?**\n",
    "\n",
    "Gaussian elimination is an algorithm for solving systems of linear equations. It's a systematic method for transforming a system of equations into an equivalent system that is easier to solve. \n",
    "\n",
    "**Key Idea:**\n",
    "\n",
    "The core idea is to use a series of operations to manipulate the equations (or their corresponding augmented matrix) until they are in a simpler form, typically **row-echelon form** or **reduced row-echelon form**.\n",
    "\n",
    "**Elementary Row Operations:**\n",
    "\n",
    "These are the fundamental operations used in Gaussian elimination:\n",
    "\n",
    "1. **Swapping two rows:** Interchanging the positions of two equations.\n",
    "2. **Multiplying a row by a non-zero constant:** Scaling an entire equation.\n",
    "3. **Adding a multiple of one row to another:** Combining equations to eliminate variables.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. **Represent the system as an augmented matrix:** \n",
    "   - Write the coefficients of the variables and the constants in a matrix form.\n",
    "\n",
    "2. **Perform row operations:**\n",
    "   - **Forward Elimination:** Use row operations to create zeros below the leading coefficients (the first non-zero element in each row) in each column. This transforms the matrix into row-echelon form.\n",
    "   - **Back Substitution (Optional):** \n",
    "      - Further reduce the matrix to reduced row-echelon form (where each leading coefficient is 1 and all other entries in the same column are zero).\n",
    "      - Solve for the variables using back-substitution.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Consider the following system of equations:\n",
    "\n",
    "```\n",
    "x + 2y - z = 1\n",
    "2x - y + z = 2\n",
    "x - y - 2z = -1\n",
    "```\n",
    "\n",
    "1. **Augmented Matrix:**\n",
    "\n",
    "   ```\n",
    "   | 1  2 -1 | 1 |\n",
    "   | 2 -1  1 | 2 |\n",
    "   | 1 -1 -2 | -1 |\n",
    "   ```\n",
    "\n",
    "2. **Row Operations:**\n",
    "\n",
    "   - R2 = R2 - 2*R1 \n",
    "   - R3 = R3 - R1\n",
    "\n",
    "   ```\n",
    "   | 1  2 -1 | 1 |\n",
    "   | 0 -5  3 | 0 |\n",
    "   | 0 -3 -1 | -2 |\n",
    "   ```\n",
    "\n",
    "   - R2 = (-1/5) * R2\n",
    "\n",
    "   ```\n",
    "   | 1  2 -1 | 1 |\n",
    "   | 0  1 -3/5 | 0 |\n",
    "   | 0 -3 -1 | -2 |\n",
    "   ```\n",
    "\n",
    "   - R1 = R1 - 2*R2 \n",
    "   - R3 = R3 + 3*R2\n",
    "\n",
    "   ```\n",
    "   | 1  0  1/5 | 1 |\n",
    "   | 0  1 -3/5 | 0 |\n",
    "   | 0  0 -14/5 | -2 |\n",
    "   ```\n",
    "\n",
    "   - R3 = (-5/14) * R3\n",
    "\n",
    "   ```\n",
    "   | 1  0  1/5 | 1 |\n",
    "   | 0  1 -3/5 | 0 |\n",
    "   | 0  0  1 | 5/7 | \n",
    "   ```\n",
    "\n",
    "   - R1 = R1 - (1/5)*R3 \n",
    "   - R2 = R2 + (3/5)*R3\n",
    "\n",
    "   ```\n",
    "   | 1  0  0 | 2/7 |\n",
    "   | 0  1  0 | 3/7 |\n",
    "   | 0  0  1 | 5/7 |\n",
    "   ```\n",
    "\n",
    "3. **Solution:**\n",
    "\n",
    "   x = 2/7, y = 3/7, z = 5/7\n",
    "\n",
    "**Applications:**\n",
    "\n",
    "* Solving systems of linear equations\n",
    "* Finding the inverse of a matrix\n",
    "* Calculating determinants\n",
    "* Linear programming\n",
    "\n",
    "**Key Advantages:**\n",
    "\n",
    "* Systematic and algorithmic approach.\n",
    "* Can be easily implemented on computers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve System of Linear Equation using Gaussian Elimination\n",
    "def gaussianElimination(matrixA, matrixB):\n",
    "    n = len(matrixB)\n",
    "    # Augment the matrix A with b\n",
    "    augmentMatrix = np.hstack([matrixA, matrixB.reshape(-1, 1)])\n",
    "    \n",
    "    # Forward elimination to get row echelon form\n",
    "    for i in range(n):\n",
    "        # Partial pivoting\n",
    "        max_row = np.argmax(abs(augmentMatrix[i:, i])) + i\n",
    "        augmentMatrix[[i, max_row]] = augmentMatrix[[max_row, i]]\n",
    "        \n",
    "        # Make the diagonal element 1 and eliminate below rows\n",
    "        for j in range(i + 1, n):\n",
    "            factor = augmentMatrix[j, i] / augmentMatrix[i, i]\n",
    "            augmentMatrix[j, i:] -= factor * augmentMatrix[i, i:]\n",
    "    \n",
    "    # Back substitution to solve for x\n",
    "    x = np.zeros(n)\n",
    "    for i in range(n - 1, -1, -1):\n",
    "        x[i] = (augmentMatrix[i, -1] - np.dot(augmentMatrix[i, i + 1:n], x[i + 1:])) / augmentMatrix[i, i]\n",
    "    \n",
    "    return x\n",
    "\n",
    "matrix = np.array([[2, -1, 1], [1, 3, 2], [1, -1, 2]], dtype=float)\n",
    "B = np.array([8, 13, 17], dtype=float)\n",
    "\n",
    "print(\"GaussianElimination:\", gaussianElimination(matrix, B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **LU Decomposition**\n",
    "\n",
    "**What is it?**\n",
    "\n",
    "LU decomposition is a matrix factorization technique that decomposes a given square matrix into the product of two triangular matrices:\n",
    "\n",
    "* **L:** A lower triangular matrix (all elements above the main diagonal are zero).\n",
    "* **U:** An upper triangular matrix (all elements below the main diagonal are zero).\n",
    "\n",
    "**Mathematically:**\n",
    "\n",
    "If A is a square matrix, then its LU decomposition can be expressed as:\n",
    "\n",
    "**A = L * U**\n",
    "\n",
    "where:\n",
    "\n",
    "* A is the original matrix.\n",
    "* L is the lower triangular matrix.\n",
    "* U is the upper triangular matrix.\n",
    "\n",
    "**How does it work?**\n",
    "\n",
    "The process of finding the LU decomposition is closely related to Gaussian elimination. In essence, the row operations performed during Gaussian elimination can be represented as matrix multiplications by elementary matrices. These elementary matrices can be combined to form the lower triangular matrix (L).\n",
    "\n",
    "**Applications:**\n",
    "\n",
    "* **Solving systems of linear equations:**\n",
    "    - If you have a system of linear equations represented as Ax = b, you can decompose A into LU.\n",
    "    - This allows you to solve the system in two steps:\n",
    "        - Solve Ly = b for y (forward substitution).\n",
    "        - Solve Ux = y for x (backward substitution).\n",
    "    - This is often more efficient than solving the system directly using Gaussian elimination.\n",
    "\n",
    "* **Computing the determinant:**\n",
    "    - The determinant of a matrix A is equal to the product of the diagonal elements of its U factor in the LU decomposition.\n",
    "\n",
    "* **Matrix inversion:**\n",
    "    - LU decomposition can be used to efficiently compute the inverse of a matrix.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Let's consider the following matrix:\n",
    "\n",
    "```\n",
    "A = | 2 1 |\n",
    "    | 4 3 |\n",
    "```\n",
    "\n",
    "The LU decomposition of A is:\n",
    "\n",
    "```\n",
    "L = | 1 0 |\n",
    "    | 2 1 |\n",
    "\n",
    "U = | 2 1 |\n",
    "    | 0 1 |\n",
    "```\n",
    "\n",
    "**Key Points:**\n",
    "\n",
    "* Not all matrices have an LU decomposition.\n",
    "* Permutation matrices are often used to ensure that the decomposition exists. This leads to a decomposition of the form PA = LU, where P is a permutation matrix.\n",
    "* LU decomposition is a fundamental technique in numerical linear algebra and has various applications in scientific and engineering fields.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LU Decomposition\n",
    "\n",
    "from numpy import ndarray\n",
    "\n",
    "# Input Matrix Must be Square\n",
    "def luDecomposition(matrix)-> ndarray:\n",
    "    n: ndarray = matrix.shape[0]\n",
    "    L: ndarray = np.zeros_like(matrix, dtype=float)\n",
    "    U: ndarray = np.zeros_like(matrix, dtype=float)\n",
    "    \n",
    "    for i in range(n):\n",
    "        # Upper triangular matrix U\n",
    "        for k in range(i, n):\n",
    "            U[i, k] = matrix[i, k] - sum(L[i, j] * U[j, k] for j in range(i))\n",
    "        \n",
    "        # Lower triangular matrix L\n",
    "        for k in range(i, n):\n",
    "            if i == k:\n",
    "                L[i, i] = 1  # Diagonal elements of L are 1\n",
    "            else:\n",
    "                L[k, i] = (matrix[k, i] - sum(L[k, j] * U[j, i] for j in range(i))) / U[i, i]\n",
    "    \n",
    "    return L, U\n",
    "# Example usage\n",
    "matrix = np.array([[2, -1, -2],\n",
    "              [-4, 6, 3],\n",
    "              [-4, -2, 8]], dtype=float)\n",
    "\n",
    "L, U = luDecomposition(matrix)\n",
    "\n",
    "print(\"Matrix A:\\n\", matrix)\n",
    "print(\"\\nLower triangular matrix L:\\n\", L)\n",
    "print(\"\\nUpper triangular matrix U:\\n\", U)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Eigenvalues and Eigenvectors**\n",
    "\n",
    "In linear algebra, eigenvalues and eigenvectors are fundamental concepts that describe the behavior of linear transformations. \n",
    "\n",
    "**Eigenvector:**\n",
    "\n",
    "* **Definition:** An eigenvector of a square matrix A is a non-zero vector **v** that, when multiplied by the matrix A, results in a scalar multiple of itself. \n",
    "* **Mathematically:**\n",
    "   A **v** = λ **v** \n",
    "   where:\n",
    "      * A is the square matrix\n",
    "      * **v** is the eigenvector (a non-zero vector)\n",
    "      * λ is the eigenvalue (a scalar)\n",
    "\n",
    "**Eigenvalue:**\n",
    "\n",
    "* **Definition:** The eigenvalue λ is the scalar factor that scales the eigenvector when the matrix A is applied to it.\n",
    "\n",
    "**In simpler terms:**\n",
    "\n",
    "Imagine a linear transformation represented by a matrix. When you apply this transformation to a vector, it usually changes both the direction and magnitude of the vector. However, eigenvectors are special vectors that only change in magnitude (they are scaled) when the transformation is applied. The eigenvalue represents the scaling factor.\n",
    "\n",
    "**Key Concepts:**\n",
    "\n",
    "* **Characteristic Equation:** To find the eigenvalues of a matrix A, you need to solve the characteristic equation:\n",
    "   **det(A - λI) = 0**\n",
    "   where:\n",
    "      * det() represents the determinant of the matrix\n",
    "      * I is the identity matrix\n",
    "      * λ is the eigenvalue\n",
    "\n",
    "* **Eigenspaces:** For each eigenvalue, there is a corresponding eigenspace, which is the set of all eigenvectors associated with that eigenvalue.\n",
    "\n",
    "**Applications:**\n",
    "\n",
    "Eigenvalues and eigenvectors have numerous applications in various fields, including:\n",
    "\n",
    "* **Physics:** \n",
    "    * Describing the vibrational modes of molecules\n",
    "    * Analyzing the stability of systems\n",
    "    * Quantum mechanics \n",
    "* **Engineering:** \n",
    "    * Structural analysis\n",
    "    * Control systems\n",
    "* **Machine Learning:**\n",
    "    * Principal Component Analysis (PCA)\n",
    "    * Face recognition\n",
    "    * Natural Language Processing\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Consider the matrix:\n",
    "\n",
    "```\n",
    "A = | 2 1 |\n",
    "    | 1 2 |\n",
    "```\n",
    "\n",
    "One of its eigenvalues is λ = 3, and the corresponding eigenvector is:\n",
    "\n",
    "```\n",
    "v = | 1 |\n",
    "    | 1 |\n",
    "```\n",
    "\n",
    "This means that when you multiply the matrix A by the vector **v**, you get:\n",
    "\n",
    "```\n",
    "A * v = | 2 1 | * | 1 | = | 3 | = 3 * | 1 |\n",
    "           | 1 2 |   | 1 |   | 3 |     | 1 |\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 x 2 EigenValue Computation\n",
    "m = np.array([[2,1],[1, 2]])\n",
    "eigenvalue, eigenvector = np.linalg.eig(m)\n",
    "print(\"Eigenvalues:\\n\",eigenvalue,\"\\nEigenvector:\\n\", eigenvector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find Eigenvectors corresponding to eigenvalues\n",
    "m = np.array([1,2,3,4])\n",
    "eigenvalue, eigenvector = np.linalg.eig(np.diag(m))\n",
    "print(\"Eigenvalues:\\n\",eigenvalue,\"\\nEigenvector:\\n\", eigenvector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Power Iteration Algorithm on Largest Eigenvalue\n",
    "The **Power Iteration** method is a simple algorithm to approximate the largest eigenvalue (in magnitude) of a matrix and its corresponding eigenvector. Here's how to implement it in Python.\n",
    "\n",
    "---\n",
    "\n",
    "### **Algorithm Steps**\n",
    "1. Start with a random initial vector $b_0$.\n",
    "2. Iterate:\n",
    "   $\n",
    "   b_{k+1} = \\frac{A b_k}{\\|A b_k\\|}\n",
    "   $\n",
    "   where $ \\| \\cdot \\| $ is the norm of the vector.\n",
    "3. After sufficient iterations, $ b_k $ will converge to the eigenvector corresponding to the largest eigenvalue.\n",
    "4. The eigenvalue can be approximated using:\n",
    "   $\n",
    "   \\lambda = \\frac{b_k^T A b_k}{b_k^T b_k}\n",
    "   $\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "\n",
    "### **Explanation**\n",
    "1. **Initialization:** Start with a random vector $ b_0 $ and normalize it.\n",
    "2. **Matrix-vector multiplication:** Multiply the current vector by the matrix to \"pull\" the vector in the direction of the dominant eigenvector.\n",
    "3. **Normalization:** Keep the vector's magnitude constant to avoid overflow or underflow.\n",
    "4. **Convergence Check:** Stop when the change between consecutive vectors is below the tolerance $ \\text{tol} $.\n",
    "5. **Eigenvalue Computation:** Use the Rayleigh quotient to approximate the eigenvalue.\n",
    "\n",
    "---\n",
    "\n",
    "### **Output**\n",
    "For the matrix:\n",
    "\n",
    "$A = \\begin{bmatrix} 4 & 1 \\\\ 2 & 3 \\end{bmatrix}$\n",
    "The output might look like:\n",
    "```\n",
    "Approximate largest eigenvalue: 5.0\n",
    "Corresponding eigenvector: [0.70710678 0.70710678]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Notes**\n",
    "1. **Dominant Eigenvalue:** This method finds the eigenvalue with the largest absolute magnitude.\n",
    "2. **Convergence:** The method converges if the matrix has a dominant eigenvalue (i.e., one eigenvalue with a strictly greater magnitude than the others).\n",
    "3. **Random Initialization:** Different initializations may result in slightly different results due to numerical stability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Power Iteration to approximate the largest eigenvalue\n",
    "\n",
    "def powerIterationOnLargestEigenvalue(matrix, iteration, tolerance= 1e-9):\n",
    "    randomVector = np.random.rand(matrix.shape[0])  # Initialize with a random vector\n",
    "\n",
    "    for _ in range(iteration):\n",
    "        newVector = np.dot(matrix, randomVector)  # Matrix-vector multiplication\n",
    "        newVector = newVector / np.linalg.norm(newVector)  # Normalize the vector\n",
    "\n",
    "        if np.linalg.norm(newVector - randomVector) < tolerance:  # Check for convergence\n",
    "         break\n",
    "\n",
    "        randomVector = newVector\n",
    "\n",
    "    eigenvalue = np.dot(randomVector, np.dot(matrix, randomVector))  # Rayleigh quotient for eigenvalue approximation\n",
    "\n",
    "    return eigenvalue, randomVector\n",
    "\n",
    "m = np.array([[4,1],[2,3]])\n",
    "eigValue, rVector = powerIterationOnLargestEigenvalue(m, iteration=1000)\n",
    "print(\"Eigenvalue:\\n\", eigValue, \"\\nRandomVector:\\n\", rVector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Spectral Radius of a Matrix**\n",
    "\n",
    "The **spectral radius** of a square matrix $ A $ is defined as the largest absolute value of its eigenvalues. Mathematically, if $ \\lambda_1, \\lambda_2, \\ldots, \\lambda_n $ are the eigenvalues of $ A $, then the spectral radius $ \\rho(A) $ is:\n",
    "\n",
    "$\n",
    "\\rho(A) = \\max \\{ |\\lambda_1|, |\\lambda_2|, \\ldots, |\\lambda_n| \\}\n",
    "$\n",
    "\n",
    "The spectral radius is used in various numerical and theoretical contexts, such as analyzing the convergence of iterative methods or the stability of dynamical systems.\n",
    "\n",
    "---\n",
    "\n",
    "### **How to Compute Spectral Radius in Python**\n",
    "\n",
    "You can compute the spectral radius using NumPy's `linalg.eigvals` or `linalg.eig` to obtain the eigenvalues and then find the maximum absolute value.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **Explanation**\n",
    "1. **Eigenvalues Calculation:** `np.linalg.eigvals(matrix)` computes all eigenvalues of the matrix.\n",
    "2. **Absolute Values:** Take the absolute value of each eigenvalue.\n",
    "3. **Maximum Value:** The spectral radius is the maximum of these absolute values.\n",
    "\n",
    "---\n",
    "\n",
    "### **Output**\n",
    "For the matrix:\n",
    "$\n",
    "A = \\begin{bmatrix} 4 & 1 \\\\ 2 & 3 \\end{bmatrix}\n",
    "$\n",
    "The output is:\n",
    "```\n",
    "Spectral Radius: 5.0\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Using Power Iteration**\n",
    "If you only need the spectral radius (not all eigenvalues), you can use the **Power Iteration** method for an efficient approximation:\n",
    "This method is useful when the matrix is large or sparse, and you only need the largest eigenvalue in magnitude.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the Spectral Radius of matrix\n",
    "def spectralRadiusOfMatrix(matrix)-> float:\n",
    "    eigenValues = np.linalg.eigvals(matrix)\n",
    "    sRadius: float = max(abs(eigenValues))\n",
    "    print(\"SpectralRadius:\",sRadius)\n",
    "    return sRadius\n",
    "\n",
    "m = np.array([[4,1],[2,3]])\n",
    "spectralRadiusOfMatrix(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification of Diagonalization of a Matrix using eigenvectors and eigenvalues\n",
    "    # If the eigenvectors are not linearly independent (i.e., P is not invertible), the matrix is not diagonalizable.\n",
    "\n",
    "def diagonalizationVerification(matrix)->bool | ndarray | ndarray:\n",
    "\n",
    "    # Compute eigenvalues and eigenvectors\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(matrix)\n",
    "    \n",
    "    # Form P (eigenvectors) and D (diagonal matrix of eigenvalues)\n",
    "    eigVector: ndarray = eigenvectors\n",
    "    diagonalMatrixOfEigenValues: ndarray = np.diag(eigenvalues)\n",
    "    \n",
    "    # Compute P * D * P^(-1)\n",
    "    eigenvectorInverse = np.linalg.inv(eigVector)\n",
    "    matrixReconstructed = np.dot(eigVector, np.dot(diagonalMatrixOfEigenValues, eigenvectorInverse))\n",
    "    \n",
    "    # Check if A is approximately equal to P * D * P^(-1)\n",
    "    isDiagonalizable: bool = np.allclose(matrix, matrixReconstructed)\n",
    "    \n",
    "    return isDiagonalizable, eigVector, diagonalMatrixOfEigenValues\n",
    "\n",
    "\n",
    "# Example matrix\n",
    "matrix = np.array([[6, 2], [2, 3]], dtype=float)\n",
    "\n",
    "# Verify diagonalization\n",
    "isDiagonal, P, D = diagonalizationVerification(matrix)\n",
    "\n",
    "print(\"Is the matrix diagonalizable?\", isDiagonal)\n",
    "print(\"\\nMatrix P (eigenvectors):\\n\", P)\n",
    "print(\"\\nMatrix D (diagonal matrix of eigenvalues):\\n\", D)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -- Single Value Decomposition --\n",
    "**Singular Value Decomposition (SVD)**\n",
    "\n",
    "**What is it?**\n",
    "\n",
    "SVD is a powerful matrix factorization technique that decomposes any matrix **A** into the product of three matrices:\n",
    "\n",
    "**A = U Σ V<sup>T</sup>**\n",
    "\n",
    "where:\n",
    "\n",
    "* **U:** An orthogonal matrix (U<sup>T</sup>U = I) \n",
    "* **Σ:** A diagonal matrix containing the singular values of A (non-negative real numbers)\n",
    "* **V<sup>T</sup>:** The transpose of an orthogonal matrix (VV<sup>T</sup> = I)\n",
    "\n",
    "**Key Concepts:**\n",
    "\n",
    "* **Singular Values:** The diagonal entries of Σ are called the singular values of A. They represent the \"strengths\" of the linear transformations encoded in the matrix.\n",
    "* **Left Singular Vectors:** The columns of U are called the left singular vectors of A.\n",
    "* **Right Singular Vectors:** The columns of V are called the right singular vectors of A.\n",
    "\n",
    "**Applications:**\n",
    "\n",
    "* **Dimensionality Reduction:** Techniques like Principal Component Analysis (PCA) rely heavily on SVD for dimensionality reduction.\n",
    "* **Image Compression:** SVD can be used to compress images by discarding small singular values, which contribute less to the overall image information.\n",
    "* **Recommender Systems:** SVD plays a crucial role in collaborative filtering algorithms for recommending products or services.\n",
    "* **Natural Language Processing:** Used in techniques like Latent Semantic Analysis (LSA) for analyzing text data.\n",
    "* **Solving Linear Equations:** SVD can be used to find the least-squares solution to systems of linear equations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def singularValueDecomposition(matrix):\n",
    "    U, s, vH = np.linalg.svd(matrix)\n",
    "    print(\"LeftSingularVector(U):\\n\",U,\"\\n\\nSingularValue(s):\", s, \"\\n\\nTransposeOfRightSingularVector(vH):\\n\",vH)\n",
    "    return U, s, vH\n",
    "\n",
    "# singularValueDecomposition(np.array([[1, 2],[2, 1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement 2 x 2 Matrix SVD manually\n",
    "\n",
    "\n",
    "def computeTwoByTwoSVD(matrix):\n",
    "    # Step 1: Compute A^T A and A A^T\n",
    "    ATA = np.dot(matrix.T, matrix)\n",
    "    AAT = np.dot(matrix, matrix.T)\n",
    "\n",
    "    # Step 2: Compute eigenvalues and eigenvectors\n",
    "    eigenvaluesATA, eigenvectorsATA = np.linalg.eig(ATA)  # For V\n",
    "    eigenvaluesAAT, eigenvectorsAAT = np.linalg.eig(AAT)  # For U\n",
    "\n",
    "    # Step 3: Singular values (sqrt of eigenvalues, sorted in descending order)\n",
    "    singularValues = np.sqrt(np.abs(eigenvaluesATA))\n",
    "    sortedIndices = np.argsort(-singularValues)  # Sort descending\n",
    "    singularValues = singularValues[sortedIndices]\n",
    "\n",
    "    # Form Sigma\n",
    "    Sigma = np.zeros_like(matrix, dtype=float)\n",
    "    np.fill_diagonal(Sigma, singularValues)\n",
    "\n",
    "    # Step 4: Sort eigenvectors of V (right singular vectors)\n",
    "    V = eigenvectorsATA[:, sortedIndices]\n",
    "\n",
    "    # Step 5: Sort eigenvectors of U (left singular vectors)\n",
    "    U = eigenvectorsAAT[:, sortedIndices]\n",
    "\n",
    "    # Ensure U and V are orthonormal (normalize if needed)\n",
    "    U = U / np.linalg.norm(U, axis=0)\n",
    "    V = V / np.linalg.norm(V, axis=0)\n",
    "\n",
    "    return U, Sigma, V\n",
    "\n",
    "\n",
    "A = np.array([[3, 1], [1, 3]], dtype=float)\n",
    "U, Sigma, V = computeTwoByTwoSVD(A)\n",
    "\n",
    "print(\"Matrix A:\\n\", A)\n",
    "print(\"\\nMatrix U (left singular vectors):\\n\", U)\n",
    "print(\"\\nMatrix Sigma (diagonal singular values):\\n\", Sigma)\n",
    "print(\"\\nMatrix V (right singular vectors):\\n\", V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
