{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Algebra with Python\n",
    "Because all the theory are available on the internet so we will basically dive into the challenges. these are some random basic challenges that i implemented to improve my Linear Algebra Skills Most Specially those that are important in machines learning, I also wrote what i searched during learning process like notes to remember things.\n",
    "One I know the basic all you can do is find whatever you wanna implement any algorithm any equations any Computation, Sky is the Limit\n",
    "## Resources\n",
    "\n",
    "[Algebra](https://youtu.be/i7vOAcUo5iA?si=WlBd7R5MwLpQIugX)\n",
    "\n",
    "[LinearAlgebraTheory](https://youtube.com/playlist?list=PLztBpqftvzxWT5z53AxSqkSaWDhAeToDG&si=DqSXG2IAQLcamlkV)\n",
    "\n",
    "[MatrixDecompositions](https://youtube.com/playlist?list=PLBh2i93oe2quJ__clFXblfzPHWd-AiwRF&si=9u3m18JcQIEQa0h2)\n",
    "\n",
    "## Libraries\n",
    "A beautiful Library - but I also tried to learn ew algorithms that can be written without numpy\n",
    "\n",
    "[Numpy](https://numpy.org/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **Basic Operations**\n",
    "1. Implement scalar multiplication of a vector.  \n",
    "2. Write a function to add two vectors.  \n",
    "3. Compute the dot product of two vectors.  \n",
    "4. Implement vector normalization (L2 norm).  \n",
    "5. Write a function to compute the cross product of two 3D vectors.  \n",
    "6. Multiply two matrices using nested loops.  \n",
    "7. Write a function to calculate the trace of a matrix.  \n",
    "8. Implement matrix transposition.  \n",
    "9. Compute the determinant of a 2x2 matrix.  \n",
    "10. Check if a matrix is symmetric.\n",
    "\n",
    "---\n",
    "\n",
    "### **Intermediate Operations**\n",
    "11. Implement matrix-vector multiplication.  \n",
    "12. Write a function to perform matrix-matrix multiplication.  \n",
    "13. Check if a matrix is orthogonal.  \n",
    "14. Compute the inverse of a 2x2 matrix.  \n",
    "15. Implement Gram-Schmidt orthogonalization for a set of vectors.  \n",
    "16. Calculate the rank of a matrix using row-reduction.  \n",
    "17. Write a function to find the null space of a matrix.  \n",
    "18. Compute the column space of a matrix.  \n",
    "19. Solve a system of linear equations using Gaussian elimination.  \n",
    "20. Implement LU decomposition of a matrix.\n",
    "\n",
    "---\n",
    "\n",
    "### **Eigenvalues and Eigenvectors**\n",
    "21. Write a function to compute eigenvalues of a 2x2 matrix.  \n",
    "22. Find eigenvectors corresponding to eigenvalues.  \n",
    "23. Implement power iteration to approximate the largest eigenvalue.  \n",
    "24. Compute the spectral radius of a matrix.  \n",
    "25. Verify the diagonalization of a matrix using its eigenvalues and eigenvectors.\n",
    "\n",
    "---\n",
    "\n",
    "### **Singular Value Decomposition (SVD)**\n",
    "26. Implement SVD manually for a 2x2 matrix.  \n",
    "27. Write a function to compute the rank of a matrix using SVD.  \n",
    "28. Perform dimensionality reduction using SVD on a dataset.  \n",
    "29. Reconstruct a matrix using its singular values and vectors.  \n",
    "30. Use SVD to approximate the largest singular value of a matrix.\n",
    "\n",
    "---\n",
    "\n",
    "### **Projections and Orthogonality**\n",
    "31. Write a function to project a vector onto another vector.  \n",
    "32. Implement projection of a vector onto a subspace.  \n",
    "33. Verify if two vectors are orthogonal.  \n",
    "34. Compute the orthogonal complement of a vector in 3D space.  \n",
    "35. Find the closest point in a subspace to a given vector.\n",
    "\n",
    "---\n",
    "\n",
    "### **Linear Transformations**\n",
    "36. Apply a rotation transformation to a 2D vector.  \n",
    "37. Implement a scaling transformation for 2D and 3D vectors.  \n",
    "38. Write a function to perform shearing transformation in 2D.  \n",
    "39. Combine rotation, scaling, and translation into an affine transformation.  \n",
    "40. Verify if a given transformation is linear.\n",
    "\n",
    "---\n",
    "\n",
    "### **Applications in ML**\n",
    "41. Implement Principal Component Analysis (PCA) on a dataset.  \n",
    "42. Perform dimensionality reduction using PCA and visualize results.  \n",
    "43. Write a function to compute cosine similarity between two vectors.  \n",
    "44. Implement a recommendation system using matrix factorization.  \n",
    "45. Solve a least squares problem for linear regression using matrix operations.\n",
    "\n",
    "---\n",
    "\n",
    "### **Advanced Challenges**\n",
    "46. Implement the Cholesky decomposition for a symmetric positive-definite matrix.  \n",
    "47. Compute the Moore-Penrose pseudoinverse of a matrix.  \n",
    "48. Solve a linear system using the conjugate gradient method.  \n",
    "49. Verify the stability of a system by checking eigenvalues of its matrix.  \n",
    "50. Simulate a Markov process using transition matrices.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scalar Multiplication of vector\n",
    "# For creativity I created random values \n",
    "vector = np.random.rand(9)\n",
    "scalar = np.random.randint(99)\n",
    "print(vector)\n",
    "print(scalar)\n",
    "\n",
    "print(np.multiply(scalar, vector))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding Two Vector\n",
    "\n",
    "vectorOne = np.random.rand(9)\n",
    "vectorTwo = np.random.rand(9)\n",
    "print(vectorOne)\n",
    "print(vectorTwo)\n",
    "\n",
    "\n",
    "print(np.add(vectorOne,vectorTwo))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dot Product of Two Vectors\n",
    "\n",
    "vectorOne = np.random.rand(9)\n",
    "vectorTwo = np.random.rand(9)\n",
    "print(vectorOne)\n",
    "print(vectorTwo)\n",
    "\n",
    "print(np.dot(vectorOne,vectorTwo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector Normalization (L2 Norm)\n",
    "## Explanation\n",
    "    # The L2 norm is for the shortest distance indicated by a vector. It is called a Euclidean norm too. As in Definition 1.2, substituting 2 for p, the l 2 norm is the square root of the summation of vector/distance squared element magnitudes:\n",
    "\n",
    "    # The L1 norm is a measure of distance or magnitude in vector spaces. For a matrix, the L1 norm is calculated as the sum of the absolute values of its elements.\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "\n",
    "def euclideanNorm(vec)-> float:\n",
    "    norm = 0.0\n",
    "    for _,v in enumerate(vec):\n",
    "        norm += v*v\n",
    "    return sqrt(norm)\n",
    "\n",
    "\n",
    "\n",
    "vector = np.array([-3,2,1,-1,1])\n",
    "# vec = [-3,2,1,-1,1]\n",
    "print(euclideanNorm(vector))\n",
    "print(np.linalg.norm(vector))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Product of Two 3D Vector\n",
    "\n",
    "v13D = np.array([3,2,1])\n",
    "v23D = np.array([1,2,3])\n",
    "\n",
    "print(np.cross(v13D,v23D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiply Two Matrices using nested loops\n",
    "\n",
    "\n",
    "\n",
    "mOne = [[1,2],[3,4]]\n",
    "mTwo = [[5,6],[7,8]]\n",
    "print(mOne)\n",
    "print(mTwo)\n",
    "print(np.multiply(mOne,mTwo))\n",
    "\n",
    "def matrixMultiplication(matrixOne,matrixTwo)-> list[list[int]]:\n",
    "    rows_A:int = len(matrixOne)\n",
    "    cols_A:int = len(matrixOne[0])\n",
    "    rows_B:int = len(matrixTwo)\n",
    "    cols_B:int = len(matrixTwo[0])\n",
    "\n",
    "    if cols_A != rows_B:\n",
    "        raise ValueError(\"Matrices cannot be multiplied. Number of columns in A must be equal to the number of rows in B.\")\n",
    "\n",
    "    result: list[list[int]] = [[0 for _ in range(cols_B)] for _ in range(rows_A)]\n",
    "\n",
    "    for i in range(rows_A):\n",
    "        for j in range(cols_B):\n",
    "            for k in range(cols_A):\n",
    "                result[i][j] += matrixOne[i][k] * matrixTwo[k][j]\n",
    "    print(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the trace of Matrix\n",
    "    # The trace of a matrix is the sum of the diagonal elements of the matrix\n",
    "            # Formula\n",
    "                # tr(A) = \\sum_{i=1}^n a_{ii}\n",
    "m = np.identity(4, dtype=int)\n",
    "print(m)\n",
    "print(np.trace(m))\n",
    "\n",
    "def tracing(matrix)-> int:\n",
    "    diagonalValues =0\n",
    "    for i in range(len(matrix)):\n",
    "            diagonalValues += matrix[i][i]\n",
    "    print(diagonalValues)\n",
    "\n",
    "tracing(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix Transpositions Implementation\n",
    "    # Matrix transposition is the process of switching the rows and columns of a matrix.\n",
    "        # AT)ij = aji\n",
    "\n",
    "m = np.arange(1, 28).reshape(3,3,3)\n",
    "\n",
    "print(m)\n",
    "\n",
    "print(\"Transposed: \\n\",np.transpose(m))\n",
    "\n",
    "# def transposition(matrix):\n",
    "\n",
    "    # return transposed\n",
    "\n",
    "# print(\"FunctionalTransposition:\\n\",transposition(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Compute the determent of 2x2 Matrix\n",
    "    # The determinant of a matrix is the scalar value or number calculated using a square matrix.\n",
    "        # The determinant is only defined for square matrices. A matrix is said to be singular if its determinant is zero. The general formula for the determinant of matrices of any size is very complicated.  \n",
    "\n",
    "print(np.linalg.det(np.arange(1,10).reshape(3,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Matrix Symmetrical\n",
    "    # A symmetric matrix is a square matrix that remains unchanged when its rows and columns are interchanged. In other words, it is equal to its transpose.\n",
    "\n",
    "def isSymmetrical(matrixOne,matrixTwo):\n",
    "    print(np.array_equal(matrixOne,matrixTwo.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --- intermediate ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix Vector Multiplication\n",
    "\n",
    "vec = np.random.rand(3)\n",
    "print(\"Vector:\\n\",vec)\n",
    "mat = np.arange(1,10).reshape(3,3)\n",
    "print(\"3 by 3 Matrix\\n\",mat)\n",
    "\n",
    "# Multiplication shoulD be based on SAME SIZE\n",
    "print(\"\\n Matrix Vector Multiplication: \\n\",mat * vec)\n",
    "print(\"\\n\\n Numpy Multiplication:\",np.multiply(vec, mat))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix Matrix Multiplication\n",
    "matrixOne = np.arange(1,10).reshape(3,3)\n",
    "matrixTwo = np.arange(1,10).reshape(3,3)\n",
    "print(matrixOne * matrixTwo)\n",
    "print(np.multiply(matrixOne, matrixTwo))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Orthogonal Matrix\n",
    "    # A square matrix with real numbers or values is termed as an orthogonal matrix if its transpose is equal to the inverse matrix of it.\n",
    "\n",
    "def orthogonality(m)-> bool:\n",
    "    inverse = np.linalg.inv(m)\n",
    "    transpose = np.transpose(m)\n",
    "    print(transpose == inverse)\n",
    "    return transpose == inverse\n",
    "\n",
    "orthogonality(np.arange(1,5).reshape(2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inverse of Inverse matrix\n",
    "\n",
    "print(np.linalg.inv(np.arange(1,5).reshape(2,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gram-Schmidt Orthogonalization\n",
    "The Gram-Schmidt process is an algorithm in linear algebra that takes a set of linearly independent vectors and produces an orthonormal set of vectors that span the same subspace.\n",
    "\n",
    "- Key Concepts:\n",
    "\n",
    "  - Orthonormal Vectors:\n",
    "    -   Orthogonal: Vectors are orthogonal if their dot product is zero (they are perpendicular).\n",
    "    -   Normalized: A vector is normalized if its length (magnitude) is one.\n",
    "    - Linear Independence: A set of vectors is linearly independent if no vector in the set can be written as a linear combination of the other vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gram-Schmidt Orthogonalization for set of Vectors\n",
    "\n",
    "\n",
    "\n",
    "def gramSchmidt(vectors)-> np.float64:\n",
    "    orthogonalVectors: np.ndarray = []\n",
    "    for v in range(len(vectors)):\n",
    "        u = np.array(v)\n",
    "        for w in range(len(orthogonalVectors)):\n",
    "            u -= np.dot(v, w) * w\n",
    "        if np.linalg.norm(u) > 1e-10:\n",
    "            orthogonalVectors.append(u / np.linalg.norm(u))\n",
    "    return orthogonalVectors\n",
    "\n",
    "vectors = [np.array([1, 1]), np.array([2, 0])] \n",
    "orthonormalBasis = gramSchmidt([[2,2],[2,0]])\n",
    "\n",
    "print(\"Orthonormal Basis:\", np.float64(orthonormalBasis)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrix Rank\n",
    "\n",
    "**1. Understanding Matrix Rank**\n",
    "\n",
    "* **Definition:** The rank of a matrix is the maximum number of linearly independent rows or columns in the matrix. \n",
    "\n",
    "* **Key Concepts:**\n",
    "    * **Linear Independence:** A set of vectors is linearly independent if no vector in the set can be expressed as a linear combination of the other vectors.\n",
    "    * **Row Equivalence:** Two matrices are row equivalent if one can be obtained from the other by a sequence of elementary row operations (swapping rows, multiplying a row by a non-zero scalar, adding a multiple of one row to another).\n",
    "\n",
    "**2. Finding Rank Using Row Reduction**\n",
    "\n",
    "* **Row Reduction:** The process of transforming a matrix into its row-echelon form or reduced row-echelon form (RREF) using elementary row operations.\n",
    "\n",
    "* **Steps:**\n",
    "    1. **Start with the given matrix.**\n",
    "    2. **Perform row operations:**\n",
    "        * **Swap rows:** Interchange the positions of two rows.\n",
    "        * **Multiply a row by a scalar:** Multiply all elements of a row by a non-zero constant.\n",
    "        * **Add a multiple of one row to another:** Add a multiple of one row to another row.\n",
    "    3. **Continue row operations until the matrix is in row-echelon form or RREF:**\n",
    "        * **Row-echelon form:**\n",
    "            * All rows consisting entirely of zeros are located at the bottom of the matrix.\n",
    "            * The first non-zero element (leading coefficient) in each non-zero row is 1.\n",
    "            * The leading coefficient of each row is to the right of the leading coefficient of the row above it.\n",
    "        * **Reduced row-echelon form (RREF):**\n",
    "            * In addition to the conditions of row-echelon form:\n",
    "                * Each column containing a leading 1 has zeros in all other positions.\n",
    "\n",
    "* **Determine Rank:**\n",
    "    * The rank of the matrix is equal to the number of non-zero rows in its row-echelon form or RREF.\n",
    "\n",
    "**Example**\n",
    "\n",
    "Let's find the rank of the following matrix:\n",
    "\n",
    "```\n",
    "A = | 1  2  3 |\n",
    "    | 2  4  6 |\n",
    "    | 1  1  1 | \n",
    "```\n",
    "\n",
    "1. **Row Operations:**\n",
    "   * R2 = R2 - 2*R1 \n",
    "   * R3 = R3 - R1\n",
    "   ```\n",
    "   | 1  2  3 |\n",
    "   | 0  0  0 | \n",
    "   | 0 -1 -2 |\n",
    "   ```\n",
    "   * Swap R2 and R3\n",
    "   ```\n",
    "   | 1  2  3 |\n",
    "   | 0 -1 -2 |\n",
    "   | 0  0  0 |\n",
    "   ```\n",
    "   * R2 = -R2\n",
    "   ```\n",
    "   | 1  2  3 |\n",
    "   | 0  1  2 |\n",
    "   | 0  0  0 |\n",
    "   ```\n",
    "   * R1 = R1 - 2*R2\n",
    "   ```\n",
    "   | 1  0 -1 |\n",
    "   | 0  1  2 |\n",
    "   | 0  0  0 |\n",
    "   ```\n",
    "\n",
    "2. **Determine Rank:** \n",
    "   * The matrix is now in row-echelon form.\n",
    "   * There are 2 non-zero rows.\n",
    "   * **Therefore, the rank of matrix A is 2.**\n",
    "\n",
    "**Key Points:**\n",
    "\n",
    "* Row reduction is a systematic method for determining the rank of a matrix.\n",
    "* The rank of a matrix remains unchanged under elementary row operations.\n",
    "* The rank of a matrix provides valuable information about its properties, such as the number of linearly independent solutions to a system of linear equations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rank of Matrix using Row Reduction\n",
    "    # The rank of a matrix is the number of linearly independent rows or columns in the matrix.\n",
    "\n",
    "\n",
    "def matrixRank(matrix):\n",
    "\n",
    "\n",
    "  rows, columns = matrix.shape\n",
    "  rank = rows\n",
    "  print(\"Rows:\",rows,\"\\n\",\"Columns\", columns)\n",
    "\n",
    "  for i in range(rows):\n",
    "    # Find the pivot row (a row with a non-zero leading coefficient)\n",
    "    pivotRow = i\n",
    "    while pivotRow < rows and np.all(matrix[pivotRow, :i] == 0):\n",
    "      pivotRow += 1\n",
    "\n",
    "    if pivotRow == rows:  # No non-zero pivot found in remaining rows\n",
    "      break\n",
    "\n",
    "    # Swap current row with the pivot row\n",
    "    matrix[[i, pivotRow]] = matrix[[pivotRow, i]]\n",
    "\n",
    "    # Make the leading coefficient 1\n",
    "    matrix[i, :] /= matrix[i, i]\n",
    "\n",
    "    # Eliminate elements below the pivot\n",
    "    for j in range(i + 1, rows):\n",
    "      matrix[j, :] -= matrix[j, i] * matrix[i, :]\n",
    "\n",
    "  return rank\n",
    "\n",
    "m = np.array([[1, 2, 3], \n",
    "                   [2, 4, 6], \n",
    "                   [1, 1, 1],\n",
    "                   [3, 4, 5]])\n",
    "\n",
    "\n",
    "\n",
    "rank = matrixRank(m)\n",
    "print(\"Rank of the matrix:\", rank)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Null Space of a Matrix**\n",
    "\n",
    "**Definition:**\n",
    "\n",
    "The null space of an m x n matrix A, denoted as Nul A, is the set of all vectors **x** in Rⁿ such that:\n",
    "\n",
    "**A x = 0**\n",
    "\n",
    "where **0** is the zero vector in Rᵐ. In simpler terms:\n",
    "\n",
    "* The null space of A consists of all vectors that, when multiplied by A, result in the zero vector.\n",
    "* It represents the set of solutions to the homogeneous system of linear equations represented by A**x** = **0**.\n",
    "\n",
    "**Key Properties:**\n",
    "\n",
    "* **Subspace:** The null space of a matrix is always a subspace of Rⁿ. This means it satisfies the following properties:\n",
    "    * Contains the zero vector.\n",
    "    * Closed under vector addition.\n",
    "    * Closed under scalar multiplication.\n",
    "\n",
    "* **Relationship to Rank:** The dimension of the null space (also called the nullity) is related to the rank of the matrix by the Rank-Nullity Theorem:\n",
    "\n",
    "   **rank(A) + nullity(A) = n** \n",
    "\n",
    "   where n is the number of columns of matrix A.\n",
    "\n",
    "**Finding the Null Space:**\n",
    "\n",
    "1. **Form the Augmented Matrix:** Create the augmented matrix [A | **0**], where **0** is the zero vector with m components.\n",
    "\n",
    "2. **Row Reduce:** Perform row operations (swapping rows, multiplying a row by a scalar, adding a multiple of one row to another) to reduce the augmented matrix to its reduced row-echelon form (RREF).\n",
    "\n",
    "3. **Solve the System:** \n",
    "   - Identify the free variables (variables corresponding to columns without leading 1's in RREF).\n",
    "   - Express the leading variables (variables corresponding to columns with leading 1's) in terms of the free variables.\n",
    "\n",
    "4. **Write the General Solution:** Express the solution vector **x** as a linear combination of vectors involving the free variables. These vectors form a basis for the null space.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Let's find the null space of the following matrix:\n",
    "\n",
    "```\n",
    "A = | 1 2 3 |\n",
    "    | 2 4 6 | \n",
    "```\n",
    "\n",
    "1. **Form the Augmented Matrix:**\n",
    "   ```\n",
    "   [A | 0] = | 1 2 3 | 0 |\n",
    "             | 2 4 6 | 0 |\n",
    "   ```\n",
    "\n",
    "2. **Row Reduce:**\n",
    "   ```\n",
    "   | 1 2 3 | 0 | \n",
    "   | 0 0 0 | 0 | \n",
    "   ```\n",
    "\n",
    "3. **Solve the System:**\n",
    "   - Let x₃ = t (free variable)\n",
    "   - x₂ = s (free variable)\n",
    "   - x₁ = -2s - 3t\n",
    "\n",
    "4. **General Solution:**\n",
    "   ```\n",
    "   x = | -2s - 3t |\n",
    "       |      s     |\n",
    "       |      t     | \n",
    "   ```\n",
    "\n",
    "   This can be written as:\n",
    "\n",
    "   ```\n",
    "   x = s | -2 | + t | -3 |\n",
    "         |  1 |     |  0 |\n",
    "         |  0 |     |  1 |\n",
    "   ```\n",
    "\n",
    "Therefore, the null space of A is the set of all vectors of the form:\n",
    "\n",
    "```\n",
    "s | -2 | + t | -3 | \n",
    "  |  1 |     |  0 |\n",
    "  |  0 |     |  1 |\n",
    "```\n",
    "\n",
    "where s and t are any real numbers.\n",
    "\n",
    "**In Summary:**\n",
    "\n",
    "The null space of a matrix provides valuable insights into the properties of the matrix and the associated linear transformation. It plays a crucial role in various areas of linear algebra and its applications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find Null Space of the Matrix\n",
    "\n",
    "def nullSpaceOfMatrix(matrix, tolerance = 1e-12):\n",
    "    # formula of single value decomposition\n",
    "    _, s, vh = np.linalg.svd(matrix)\n",
    "    nullMask = (s<=tolerance)\n",
    "    nullSpace = np.compress(nullMask, vh, axis=0)\n",
    "    return nullSpace.T\n",
    "\n",
    "\n",
    "m = np.array([[1, 2, 3], [2, 4, 6]])\n",
    "print(nullSpaceOfMatrix(matrix=m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def columnSpaceOfMatrix(matrix, tolerance = 1e-12):\n",
    "    u, s, _ = np.linalg.svd(matrix)\n",
    "    rank = np.sum(s > tolerance)\n",
    "    columnSpace = u[:, :rank]\n",
    "    return columnSpace\n",
    "\n",
    "matrix = np.array([[2, 4, 1], [6, 12, 3], [4, 8, 2]])\n",
    "print(\"ColumnSpace:\\n\", columnSpaceOfMatrix(matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Gaussian Elimination**\n",
    "\n",
    "**What is it?**\n",
    "\n",
    "Gaussian elimination is an algorithm for solving systems of linear equations. It's a systematic method for transforming a system of equations into an equivalent system that is easier to solve. \n",
    "\n",
    "**Key Idea:**\n",
    "\n",
    "The core idea is to use a series of operations to manipulate the equations (or their corresponding augmented matrix) until they are in a simpler form, typically **row-echelon form** or **reduced row-echelon form**.\n",
    "\n",
    "**Elementary Row Operations:**\n",
    "\n",
    "These are the fundamental operations used in Gaussian elimination:\n",
    "\n",
    "1. **Swapping two rows:** Interchanging the positions of two equations.\n",
    "2. **Multiplying a row by a non-zero constant:** Scaling an entire equation.\n",
    "3. **Adding a multiple of one row to another:** Combining equations to eliminate variables.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. **Represent the system as an augmented matrix:** \n",
    "   - Write the coefficients of the variables and the constants in a matrix form.\n",
    "\n",
    "2. **Perform row operations:**\n",
    "   - **Forward Elimination:** Use row operations to create zeros below the leading coefficients (the first non-zero element in each row) in each column. This transforms the matrix into row-echelon form.\n",
    "   - **Back Substitution (Optional):** \n",
    "      - Further reduce the matrix to reduced row-echelon form (where each leading coefficient is 1 and all other entries in the same column are zero).\n",
    "      - Solve for the variables using back-substitution.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Consider the following system of equations:\n",
    "\n",
    "```\n",
    "x + 2y - z = 1\n",
    "2x - y + z = 2\n",
    "x - y - 2z = -1\n",
    "```\n",
    "\n",
    "1. **Augmented Matrix:**\n",
    "\n",
    "   ```\n",
    "   | 1  2 -1 | 1 |\n",
    "   | 2 -1  1 | 2 |\n",
    "   | 1 -1 -2 | -1 |\n",
    "   ```\n",
    "\n",
    "2. **Row Operations:**\n",
    "\n",
    "   - R2 = R2 - 2*R1 \n",
    "   - R3 = R3 - R1\n",
    "\n",
    "   ```\n",
    "   | 1  2 -1 | 1 |\n",
    "   | 0 -5  3 | 0 |\n",
    "   | 0 -3 -1 | -2 |\n",
    "   ```\n",
    "\n",
    "   - R2 = (-1/5) * R2\n",
    "\n",
    "   ```\n",
    "   | 1  2 -1 | 1 |\n",
    "   | 0  1 -3/5 | 0 |\n",
    "   | 0 -3 -1 | -2 |\n",
    "   ```\n",
    "\n",
    "   - R1 = R1 - 2*R2 \n",
    "   - R3 = R3 + 3*R2\n",
    "\n",
    "   ```\n",
    "   | 1  0  1/5 | 1 |\n",
    "   | 0  1 -3/5 | 0 |\n",
    "   | 0  0 -14/5 | -2 |\n",
    "   ```\n",
    "\n",
    "   - R3 = (-5/14) * R3\n",
    "\n",
    "   ```\n",
    "   | 1  0  1/5 | 1 |\n",
    "   | 0  1 -3/5 | 0 |\n",
    "   | 0  0  1 | 5/7 | \n",
    "   ```\n",
    "\n",
    "   - R1 = R1 - (1/5)*R3 \n",
    "   - R2 = R2 + (3/5)*R3\n",
    "\n",
    "   ```\n",
    "   | 1  0  0 | 2/7 |\n",
    "   | 0  1  0 | 3/7 |\n",
    "   | 0  0  1 | 5/7 |\n",
    "   ```\n",
    "\n",
    "3. **Solution:**\n",
    "\n",
    "   x = 2/7, y = 3/7, z = 5/7\n",
    "\n",
    "**Applications:**\n",
    "\n",
    "* Solving systems of linear equations\n",
    "* Finding the inverse of a matrix\n",
    "* Calculating determinants\n",
    "* Linear programming\n",
    "\n",
    "**Key Advantages:**\n",
    "\n",
    "* Systematic and algorithmic approach.\n",
    "* Can be easily implemented on computers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve System of Linear Equation using Gaussian Elimination\n",
    "def gaussianElimination(matrixA, matrixB):\n",
    "    n = len(matrixB)\n",
    "    # Augment the matrix A with b\n",
    "    augmentMatrix = np.hstack([matrixA, matrixB.reshape(-1, 1)])\n",
    "    \n",
    "    # Forward elimination to get row echelon form\n",
    "    for i in range(n):\n",
    "        # Partial pivoting\n",
    "        max_row = np.argmax(abs(augmentMatrix[i:, i])) + i\n",
    "        augmentMatrix[[i, max_row]] = augmentMatrix[[max_row, i]]\n",
    "        \n",
    "        # Make the diagonal element 1 and eliminate below rows\n",
    "        for j in range(i + 1, n):\n",
    "            factor = augmentMatrix[j, i] / augmentMatrix[i, i]\n",
    "            augmentMatrix[j, i:] -= factor * augmentMatrix[i, i:]\n",
    "    \n",
    "    # Back substitution to solve for x\n",
    "    x = np.zeros(n)\n",
    "    for i in range(n - 1, -1, -1):\n",
    "        x[i] = (augmentMatrix[i, -1] - np.dot(augmentMatrix[i, i + 1:n], x[i + 1:])) / augmentMatrix[i, i]\n",
    "    \n",
    "    return x\n",
    "\n",
    "matrix = np.array([[2, -1, 1], [1, 3, 2], [1, -1, 2]], dtype=float)\n",
    "vectorB = np.array([8, 13, 17], dtype=float)\n",
    "\n",
    "print(\"GaussianElimination:\", gaussianElimination(matrix, vectorB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **LU Decomposition**\n",
    "\n",
    "**What is it?**\n",
    "\n",
    "LU decomposition is a matrix factorization technique that decomposes a given square matrix into the product of two triangular matrices:\n",
    "\n",
    "* **L:** A lower triangular matrix (all elements above the main diagonal are zero).\n",
    "* **U:** An upper triangular matrix (all elements below the main diagonal are zero).\n",
    "\n",
    "**Mathematically:**\n",
    "\n",
    "If A is a square matrix, then its LU decomposition can be expressed as:\n",
    "\n",
    "**A = L * U**\n",
    "\n",
    "where:\n",
    "\n",
    "* A is the original matrix.\n",
    "* L is the lower triangular matrix.\n",
    "* U is the upper triangular matrix.\n",
    "\n",
    "**How does it work?**\n",
    "\n",
    "The process of finding the LU decomposition is closely related to Gaussian elimination. In essence, the row operations performed during Gaussian elimination can be represented as matrix multiplications by elementary matrices. These elementary matrices can be combined to form the lower triangular matrix (L).\n",
    "\n",
    "**Applications:**\n",
    "\n",
    "* **Solving systems of linear equations:**\n",
    "    - If you have a system of linear equations represented as Ax = b, you can decompose A into LU.\n",
    "    - This allows you to solve the system in two steps:\n",
    "        - Solve Ly = b for y (forward substitution).\n",
    "        - Solve Ux = y for x (backward substitution).\n",
    "    - This is often more efficient than solving the system directly using Gaussian elimination.\n",
    "\n",
    "* **Computing the determinant:**\n",
    "    - The determinant of a matrix A is equal to the product of the diagonal elements of its U factor in the LU decomposition.\n",
    "\n",
    "* **Matrix inversion:**\n",
    "    - LU decomposition can be used to efficiently compute the inverse of a matrix.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Let's consider the following matrix:\n",
    "\n",
    "```\n",
    "A = | 2 1 |\n",
    "    | 4 3 |\n",
    "```\n",
    "\n",
    "The LU decomposition of A is:\n",
    "\n",
    "```\n",
    "L = | 1 0 |\n",
    "    | 2 1 |\n",
    "\n",
    "U = | 2 1 |\n",
    "    | 0 1 |\n",
    "```\n",
    "\n",
    "**Key Points:**\n",
    "\n",
    "* Not all matrices have an LU decomposition.\n",
    "* Permutation matrices are often used to ensure that the decomposition exists. This leads to a decomposition of the form PA = LU, where P is a permutation matrix.\n",
    "* LU decomposition is a fundamental technique in numerical linear algebra and has various applications in scientific and engineering fields.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LU Decomposition\n",
    "\n",
    "from numpy import ndarray\n",
    "\n",
    "# Input Matrix Must be Square\n",
    "def luDecomposition(matrix)-> ndarray:\n",
    "    n: ndarray = matrix.shape[0]\n",
    "    L: ndarray = np.zeros_like(matrix, dtype=float)\n",
    "    U: ndarray = np.zeros_like(matrix, dtype=float)\n",
    "    \n",
    "    for i in range(n):\n",
    "        # Upper triangular matrix U\n",
    "        for k in range(i, n):\n",
    "            U[i, k] = matrix[i, k] - sum(L[i, j] * U[j, k] for j in range(i))\n",
    "        \n",
    "        # Lower triangular matrix L\n",
    "        for k in range(i, n):\n",
    "            if i == k:\n",
    "                L[i, i] = 1  # Diagonal elements of L are 1\n",
    "            else:\n",
    "                L[k, i] = (matrix[k, i] - sum(L[k, j] * U[j, i] for j in range(i))) / U[i, i]\n",
    "    \n",
    "    return L, U\n",
    "# Example usage\n",
    "matrix = np.array([[2, -1, -2],\n",
    "              [-4, 6, 3],\n",
    "              [-4, -2, 8]], dtype=float)\n",
    "\n",
    "lowerTriangularMatrix, U = luDecomposition(matrix)\n",
    "\n",
    "print(\"Matrix A:\\n\", matrix)\n",
    "print(\"\\nLower triangular matrix L:\\n\", lowerTriangularMatrix)\n",
    "print(\"\\nUpper triangular matrix U:\\n\", U)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Eigenvalues and Eigenvectors**\n",
    "\n",
    "In linear algebra, eigenvalues and eigenvectors are fundamental concepts that describe the behavior of linear transformations. \n",
    "\n",
    "**Eigenvector:**\n",
    "\n",
    "* **Definition:** An eigenvector of a square matrix A is a non-zero vector **v** that, when multiplied by the matrix A, results in a scalar multiple of itself. \n",
    "* **Mathematically:**\n",
    "   A **v** = λ **v** \n",
    "   where:\n",
    "      * A is the square matrix\n",
    "      * **v** is the eigenvector (a non-zero vector)\n",
    "      * λ is the eigenvalue (a scalar)\n",
    "\n",
    "**Eigenvalue:**\n",
    "\n",
    "* **Definition:** The eigenvalue λ is the scalar factor that scales the eigenvector when the matrix A is applied to it.\n",
    "\n",
    "**In simpler terms:**\n",
    "\n",
    "Imagine a linear transformation represented by a matrix. When you apply this transformation to a vector, it usually changes both the direction and magnitude of the vector. However, eigenvectors are special vectors that only change in magnitude (they are scaled) when the transformation is applied. The eigenvalue represents the scaling factor.\n",
    "\n",
    "**Key Concepts:**\n",
    "\n",
    "* **Characteristic Equation:** To find the eigenvalues of a matrix A, you need to solve the characteristic equation:\n",
    "   **det(A - λI) = 0**\n",
    "   where:\n",
    "      * det() represents the determinant of the matrix\n",
    "      * I is the identity matrix\n",
    "      * λ is the eigenvalue\n",
    "\n",
    "* **Eigenspaces:** For each eigenvalue, there is a corresponding eigenspace, which is the set of all eigenvectors associated with that eigenvalue.\n",
    "\n",
    "**Applications:**\n",
    "\n",
    "Eigenvalues and eigenvectors have numerous applications in various fields, including:\n",
    "\n",
    "* **Physics:** \n",
    "    * Describing the vibrational modes of molecules\n",
    "    * Analyzing the stability of systems\n",
    "    * Quantum mechanics \n",
    "* **Engineering:** \n",
    "    * Structural analysis\n",
    "    * Control systems\n",
    "* **Machine Learning:**\n",
    "    * Principal Component Analysis (PCA)\n",
    "    * Face recognition\n",
    "    * Natural Language Processing\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Consider the matrix:\n",
    "\n",
    "```\n",
    "A = | 2 1 |\n",
    "    | 1 2 |\n",
    "```\n",
    "\n",
    "One of its eigenvalues is λ = 3, and the corresponding eigenvector is:\n",
    "\n",
    "```\n",
    "v = | 1 |\n",
    "    | 1 |\n",
    "```\n",
    "\n",
    "This means that when you multiply the matrix A by the vector **v**, you get:\n",
    "\n",
    "```\n",
    "A * v = | 2 1 | * | 1 | = | 3 | = 3 * | 1 |\n",
    "           | 1 2 |   | 1 |   | 3 |     | 1 |\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 x 2 EigenValue Computation\n",
    "m = np.array([[2,1],[1, 2]])\n",
    "eigenvalue, eigenvector = np.linalg.eig(m)\n",
    "print(\"Eigenvalues:\\n\",eigenvalue,\"\\nEigenvector:\\n\", eigenvector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find Eigenvectors corresponding to eigenvalues\n",
    "m = np.array([1,2,3,4])\n",
    "eigenvalue, eigenvector = np.linalg.eig(np.diag(m))\n",
    "print(\"Eigenvalues:\\n\",eigenvalue,\"\\nEigenvector:\\n\", eigenvector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Power Iteration Algorithm on Largest Eigenvalue\n",
    "The **Power Iteration** method is a simple algorithm to approximate the largest eigenvalue (in magnitude) of a matrix and its corresponding eigenvector. Here's how to implement it in Python.\n",
    "\n",
    "---\n",
    "\n",
    "### **Algorithm Steps**\n",
    "1. Start with a random initial vector $b_0$.\n",
    "2. Iterate:\n",
    "   $\n",
    "   b_{k+1} = \\frac{A b_k}{\\|A b_k\\|}\n",
    "   $\n",
    "   where $ \\| \\cdot \\| $ is the norm of the vector.\n",
    "3. After sufficient iterations, $ b_k $ will converge to the eigenvector corresponding to the largest eigenvalue.\n",
    "4. The eigenvalue can be approximated using:\n",
    "   $\n",
    "   \\lambda = \\frac{b_k^T A b_k}{b_k^T b_k}\n",
    "   $\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "\n",
    "### **Explanation**\n",
    "1. **Initialization:** Start with a random vector $ b_0 $ and normalize it.\n",
    "2. **Matrix-vector multiplication:** Multiply the current vector by the matrix to \"pull\" the vector in the direction of the dominant eigenvector.\n",
    "3. **Normalization:** Keep the vector's magnitude constant to avoid overflow or underflow.\n",
    "4. **Convergence Check:** Stop when the change between consecutive vectors is below the tolerance $ \\text{tol} $.\n",
    "5. **Eigenvalue Computation:** Use the Rayleigh quotient to approximate the eigenvalue.\n",
    "\n",
    "---\n",
    "\n",
    "### **Output**\n",
    "For the matrix:\n",
    "\n",
    "$A = \\begin{bmatrix} 4 & 1 \\\\ 2 & 3 \\end{bmatrix}$\n",
    "The output might look like:\n",
    "```\n",
    "Approximate largest eigenvalue: 5.0\n",
    "Corresponding eigenvector: [0.70710678 0.70710678]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Notes**\n",
    "1. **Dominant Eigenvalue:** This method finds the eigenvalue with the largest absolute magnitude.\n",
    "2. **Convergence:** The method converges if the matrix has a dominant eigenvalue (i.e., one eigenvalue with a strictly greater magnitude than the others).\n",
    "3. **Random Initialization:** Different initializations may result in slightly different results due to numerical stability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Power Iteration to approximate the largest eigenvalue\n",
    "\n",
    "def powerIterationOnLargestEigenvalue(matrix, iteration, tolerance= 1e-9):\n",
    "    randomVector = np.random.rand(matrix.shape[0])  # Initialize with a random vector\n",
    "\n",
    "    for _ in range(iteration):\n",
    "        newVector = np.dot(matrix, randomVector)  # Matrix-vector multiplication\n",
    "        newVector = newVector / np.linalg.norm(newVector)  # Normalize the vector\n",
    "\n",
    "        if np.linalg.norm(newVector - randomVector) < tolerance:  # Check for convergence\n",
    "         break\n",
    "\n",
    "        randomVector = newVector\n",
    "\n",
    "    eigenvalue = np.dot(randomVector, np.dot(matrix, randomVector))  # Rayleigh quotient for eigenvalue approximation\n",
    "\n",
    "    return eigenvalue, randomVector\n",
    "\n",
    "m = np.array([[4,1],[2,3]])\n",
    "eigValue, rVector = powerIterationOnLargestEigenvalue(m, iteration=1000)\n",
    "print(\"Eigenvalue:\\n\", eigValue, \"\\nRandomVector:\\n\", rVector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Spectral Radius of a Matrix**\n",
    "\n",
    "The **spectral radius** of a square matrix $ A $ is defined as the largest absolute value of its eigenvalues. Mathematically, if $ \\lambda_1, \\lambda_2, \\ldots, \\lambda_n $ are the eigenvalues of $ A $, then the spectral radius $ \\rho(A) $ is:\n",
    "\n",
    "$\n",
    "\\rho(A) = \\max \\{ |\\lambda_1|, |\\lambda_2|, \\ldots, |\\lambda_n| \\}\n",
    "$\n",
    "\n",
    "The spectral radius is used in various numerical and theoretical contexts, such as analyzing the convergence of iterative methods or the stability of dynamical systems.\n",
    "\n",
    "---\n",
    "\n",
    "### **How to Compute Spectral Radius in Python**\n",
    "\n",
    "You can compute the spectral radius using NumPy's `linalg.eigvals` or `linalg.eig` to obtain the eigenvalues and then find the maximum absolute value.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **Explanation**\n",
    "1. **Eigenvalues Calculation:** `np.linalg.eigvals(matrix)` computes all eigenvalues of the matrix.\n",
    "2. **Absolute Values:** Take the absolute value of each eigenvalue.\n",
    "3. **Maximum Value:** The spectral radius is the maximum of these absolute values.\n",
    "\n",
    "---\n",
    "\n",
    "### **Output**\n",
    "For the matrix:\n",
    "$\n",
    "A = \\begin{bmatrix} 4 & 1 \\\\ 2 & 3 \\end{bmatrix}\n",
    "$\n",
    "The output is:\n",
    "```\n",
    "Spectral Radius: 5.0\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Using Power Iteration**\n",
    "If you only need the spectral radius (not all eigenvalues), you can use the **Power Iteration** method for an efficient approximation:\n",
    "This method is useful when the matrix is large or sparse, and you only need the largest eigenvalue in magnitude.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the Spectral Radius of matrix\n",
    "def spectralRadiusOfMatrix(matrix)-> float:\n",
    "    eigenValues = np.linalg.eigvals(matrix)\n",
    "    sRadius: float = max(abs(eigenValues))\n",
    "    print(\"SpectralRadius:\",sRadius)\n",
    "    return sRadius\n",
    "\n",
    "m = np.array([[4,1],[2,3]])\n",
    "spectralRadiusOfMatrix(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification of Diagonalization of a Matrix using eigenvectors and eigenvalues\n",
    "    # If the eigenvectors are not linearly independent (i.e., P is not invertible), the matrix is not diagonalizable.\n",
    "\n",
    "def diagonalizationVerification(matrix)->bool | ndarray | ndarray:\n",
    "\n",
    "    # Compute eigenvalues and eigenvectors\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(matrix)\n",
    "    \n",
    "    # Form P (eigenvectors) and D (diagonal matrix of eigenvalues)\n",
    "    eigVector: ndarray = eigenvectors\n",
    "    diagonalMatrixOfEigenValues: ndarray = np.diag(eigenvalues)\n",
    "    \n",
    "    # Compute P * D * P^(-1)\n",
    "    eigenvectorInverse = np.linalg.inv(eigVector)\n",
    "    matrixReconstructed = np.dot(eigVector, np.dot(diagonalMatrixOfEigenValues, eigenvectorInverse))\n",
    "    \n",
    "    # Check if A is approximately equal to P * D * P^(-1)\n",
    "    isDiagonalizable: bool = np.allclose(matrix, matrixReconstructed)\n",
    "    \n",
    "    return isDiagonalizable, eigVector, diagonalMatrixOfEigenValues\n",
    "\n",
    "\n",
    "# Example matrix\n",
    "matrix = np.array([[6, 2], [2, 3]], dtype=float)\n",
    "\n",
    "# Verify diagonalization\n",
    "isDiagonal, matrix, D = diagonalizationVerification(matrix)\n",
    "\n",
    "print(\"Is the matrix diagonalizable?\", isDiagonal)\n",
    "print(\"\\nMatrix P (eigenvectors):\\n\", matrix)\n",
    "print(\"\\nMatrix D (diagonal matrix of eigenvalues):\\n\", D)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -- Single Value Decomposition --\n",
    "**Singular Value Decomposition (SVD)**\n",
    "\n",
    "**What is it?**\n",
    "\n",
    "SVD is a powerful matrix factorization technique that decomposes any matrix **A** into the product of three matrices:\n",
    "\n",
    "**A = U Σ V<sup>T</sup>**\n",
    "\n",
    "where:\n",
    "\n",
    "* **U:** An orthogonal matrix (U<sup>T</sup>U = I) \n",
    "* **Σ:** A diagonal matrix containing the singular values of A (non-negative real numbers)\n",
    "* **V<sup>T</sup>:** The transpose of an orthogonal matrix (VV<sup>T</sup> = I)\n",
    "\n",
    "**Key Concepts:**\n",
    "\n",
    "* **Singular Values:** The diagonal entries of Σ are called the singular values of A. They represent the \"strengths\" of the linear transformations encoded in the matrix.\n",
    "* **Left Singular Vectors:** The columns of U are called the left singular vectors of A.\n",
    "* **Right Singular Vectors:** The columns of V are called the right singular vectors of A.\n",
    "\n",
    "**Applications:**\n",
    "\n",
    "* **Dimensionality Reduction:** Techniques like Principal Component Analysis (PCA) rely heavily on SVD for dimensionality reduction.\n",
    "* **Image Compression:** SVD can be used to compress images by discarding small singular values, which contribute less to the overall image information.\n",
    "* **Recommender Systems:** SVD plays a crucial role in collaborative filtering algorithms for recommending products or services.\n",
    "* **Natural Language Processing:** Used in techniques like Latent Semantic Analysis (LSA) for analyzing text data.\n",
    "* **Solving Linear Equations:** SVD can be used to find the least-squares solution to systems of linear equations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def singularValueDecomposition(matrix):\n",
    "    U, s, vH = np.linalg.svd(matrix)\n",
    "    print(\"LeftSingularVector(U):\\n\",U,\"\\n\\nSingularValue(s):\", s, \"\\n\\nTransposeOfRightSingularVector(vH):\\n\",vH)\n",
    "    return U, s, vH\n",
    "\n",
    "# singularValueDecomposition(np.array([[1, 2],[2, 1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement 2 x 2 Matrix SVD manually\n",
    "\n",
    "\n",
    "def computeTwoByTwoSVD(matrix):\n",
    "    # Step 1: Compute A^T A and A A^T\n",
    "    ATA = np.dot(matrix.T, matrix)\n",
    "    AAT = np.dot(matrix, matrix.T)\n",
    "\n",
    "    # Step 2: Compute eigenvalues and eigenvectors\n",
    "    eigenvaluesATA, eigenvectorsATA = np.linalg.eig(ATA)  # For V\n",
    "    eigenvaluesAAT, eigenvectorsAAT = np.linalg.eig(AAT)  # For U\n",
    "\n",
    "    # Step 3: Singular values (sqrt of eigenvalues, sorted in descending order)\n",
    "    singularValues = np.sqrt(np.abs(eigenvaluesATA))\n",
    "    sortedIndices = np.argsort(-singularValues)  # Sort descending\n",
    "    singularValues = singularValues[sortedIndices]\n",
    "\n",
    "    # Form Sigma\n",
    "    Sigma = np.zeros_like(matrix, dtype=float)\n",
    "    np.fill_diagonal(Sigma, singularValues)\n",
    "\n",
    "    # Step 4: Sort eigenvectors of V (right singular vectors)\n",
    "    V = eigenvectorsATA[:, sortedIndices]\n",
    "\n",
    "    # Step 5: Sort eigenvectors of U (left singular vectors)\n",
    "    U = eigenvectorsAAT[:, sortedIndices]\n",
    "\n",
    "    # Ensure U and V are orthonormal (normalize if needed)\n",
    "    U = U / np.linalg.norm(U, axis=0)\n",
    "    V = V / np.linalg.norm(V, axis=0)\n",
    "\n",
    "    return U, Sigma, V\n",
    "\n",
    "\n",
    "matrix = np.array([[3, 1], [1, 3]], dtype=float)\n",
    "U, Sigma, V = computeTwoByTwoSVD(matrix)\n",
    "\n",
    "print(\"Matrix A:\\n\", matrix)\n",
    "print(\"\\nMatrix U (left singular vectors):\\n\", U)\n",
    "print(\"\\nMatrix Sigma (diagonal singular values):\\n\", Sigma)\n",
    "print(\"\\nMatrix V (right singular vectors):\\n\", V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the Rank of Matrix Using the SVD\n",
    "\n",
    "def rankOfMatrixViaSVD(matrix, tolerance= 1e-10):\n",
    "    # because we we are only interested in the computing the rank than we are only interested in the diagonal values of matrix\n",
    "    _, s, _ = np.linalg.svd(matrix)\n",
    "    rank = sum(s > tolerance)\n",
    "    print(rank)\n",
    "    return rank\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction Using SVD\n",
    "\n",
    "Singular Value Decomposition (SVD) can be used for **dimensionality reduction** by projecting data onto a lower-dimensional space while retaining as much variance as possible. Here's how it works:\n",
    "\n",
    "1. Compute the SVD of the dataset $ X $:\n",
    "   $\n",
    "   X = U \\Sigma V^T\n",
    "   $\n",
    "   Here, $ X $ is the $ m \\times n $ data matrix, where $ m $ is the number of samples and $ n $ is the number of features.\n",
    "\n",
    "2. Select the top $ k $ singular values and corresponding vectors from $ U $ and $ V $ to reduce the dimensionality.\n",
    "\n",
    "3. Transform the data to a $ k $-dimensional space:\n",
    "   $\n",
    "   X_{reduced} = X V_k\n",
    "   $\n",
    "   where $ V_k $ contains the top $ k $ columns of $ V $.\n",
    "\n",
    "---\n",
    "\n",
    "### **Explanation**\n",
    "\n",
    "1. **Input Matrix $ X $:**\n",
    "   - Rows represent samples, and columns represent features.\n",
    "   - $ X $ should be mean-centered (subtract the mean from each column) if variance preservation is the goal.\n",
    "\n",
    "2. **SVD:**\n",
    "   - `np.linalg.svd(X, full_matrices=False)` computes $ U $, $ \\Sigma $, and $ V^T $.\n",
    "   - $ V^T $ is the transpose of $ V $, where $ V $ contains the principal directions.\n",
    "\n",
    "3. **Select Top $ k $ Components:**\n",
    "   - Use the first $ k $ singular values and vectors from $ V^T $ to reduce the dimensions.\n",
    "\n",
    "4. **Projection:**\n",
    "   - Multiply the original matrix $ X $ by $ V_k $ to obtain the reduced data.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### **Notes**\n",
    "1. The reduced data matrix $ X_{reduced} $ is of shape $ m \\times k $, where $ k $ is the number of dimensions.\n",
    "2. If you want to reconstruct the data from the reduced dimensions:\n",
    "   $\n",
    "   X_{approx} = X_{reduced} V_k^T\n",
    "   $\n",
    "3. This method is equivalent to **Principal Component Analysis (PCA)** when applied to mean-centered data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensionality Reduction Using SVD\n",
    "def dimensionalityReductionViaSVD(data, k):\n",
    "\n",
    "   # Step 1: Center the data (mean subtraction for each feature)\n",
    "    dataMean = np.mean(data, axis=0)\n",
    "    centeredData = data - dataMean\n",
    "\n",
    "    # Step 2: Perform SVD\n",
    "    _, _, Vt = np.linalg.svd(centeredData, full_matrices=False)\n",
    "\n",
    "    # Step 3: Select the top-k components\n",
    "    topComponents = Vt[:k, :].T  # Take the first k rows of V^T and transpose\n",
    "\n",
    "    # Step 4: Project the data onto the k-dimensional space\n",
    "    reducedData = np.dot(centeredData, topComponents)\n",
    "    reconstructedData = np.dot(reducedData, topComponents.T)\n",
    "\n",
    "    # Add the mean back to approximate the original data\n",
    "    approximateData = reconstructedData + dataMean\n",
    "\n",
    "    print(\"Reconstructed Data (X_approx):\")\n",
    "    print(approximateData)\n",
    "    return reducedData\n",
    "\n",
    "# Example Usage:\n",
    "matrix = np.array([[1, 2, 3],\n",
    "              [4, 5, 6],\n",
    "              [7, 8, 9],\n",
    "              [10, 11, 12],[13, 14, 15]], dtype=float)\n",
    "\n",
    "reduced_data = dimensionalityReductionViaSVD(matrix, k=2) \n",
    "print(\"\\nReduced Data:\\n\", reduced_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reconstruct a matrix using Singular Value Decomposition (SVD)**\n",
    "\n",
    "you combine the three components $ U $, $ \\Sigma $, and $ V^T $ as follows:\n",
    "\n",
    "$\n",
    "A = U \\Sigma V^T\n",
    "$\n",
    "\n",
    "Here’s how each component contributes:\n",
    "- $ U $: Contains the left singular vectors (columns).\n",
    "- $ \\Sigma $: A diagonal matrix with singular values.\n",
    "- $ V^T $: The transpose of $ V $, which contains the right singular vectors.\n",
    "\n",
    "---\n",
    "\n",
    "### **Steps for Reconstruction**\n",
    "1. Perform SVD:\n",
    "   $\n",
    "   A = U \\Sigma V^T\n",
    "   $\n",
    "2. If you truncate the decomposition to $ k $ components:\n",
    "   - Use the top $ k $ singular values from $ \\Sigma $.\n",
    "   - Use the corresponding $ k $ columns of $ U $ and rows of $ V^T $.\n",
    "\n",
    "3. Reconstruct the matrix:\n",
    "   $\n",
    "   A_{\\text{reconstructed}} = U_k \\Sigma_k V_k^T\n",
    "   $\n",
    "\n",
    "---\n",
    "\n",
    "### **Explanation of the Code**\n",
    "1. **Full SVD Reconstruction**:\n",
    "   - $ U $: All left singular vectors.\n",
    "   - $ \\Sigma $: All singular values.\n",
    "   - $ V^T $: All right singular vectors.\n",
    "   - Reconstruction:\n",
    "     $\n",
    "     A = U \\Sigma V^T\n",
    "     $\n",
    "\n",
    "2. **Truncated SVD Reconstruction**:\n",
    "   - Use only the top $ k $ singular values and corresponding singular vectors.\n",
    "   - This reduces the rank of the reconstructed matrix, capturing only the most significant features.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### **Key Notes**\n",
    "1. **Exact Reconstruction**: Using all singular values and vectors will perfectly reconstruct the original matrix.\n",
    "2. **Truncated Reconstruction**: Using fewer singular values approximates the original matrix, effectively reducing its rank while preserving most of its structure.\n",
    "3. **Applications**:\n",
    "   - Dimensionality reduction (e.g., in PCA).\n",
    "   - Noise reduction in signals/images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def matrixReconstructViaSVD(matrix, component):\n",
    "\n",
    "\n",
    "# Step 1: Perform SVD\n",
    "    U, Sigma, Vt = np.linalg.svd(matrix, full_matrices=False)\n",
    "\n",
    "# Step 2: Reconstruct the matrix using full SVD\n",
    "# Convert Sigma (1D array) into a diagonal matrix\n",
    "    diagonalValues = np.diag(Sigma)\n",
    "\n",
    "# Full reconstruction\n",
    "    fullReconstructedMatrix = np.dot(U, np.dot(diagonalValues, Vt))\n",
    "\n",
    "    print(\"Original Matrix (A):\")\n",
    "    print(matrix)\n",
    "    print(\"\\nReconstructed Matrix (A_full_reconstructed):\")\n",
    "    print(fullReconstructedMatrix)\n",
    "\n",
    "# Step 3: Reconstruct using truncated SVD (e.g., k=2)\n",
    "  # Number of singular values to use\n",
    "    topValues = U[:, :component]\n",
    "    topDiagonalValues = np.diag(Sigma[:component])  # Only top k singular values\n",
    "    Vt_k = Vt[:component, :]\n",
    "\n",
    "# Truncated reconstruction\n",
    "    truncatedReconstructed = np.dot(topValues, np.dot(topDiagonalValues, Vt_k))\n",
    "\n",
    "    print(\"\\nTruncated Reconstructed Matrix (A_truncated_reconstructed):\")\n",
    "    print(truncatedReconstructed)\n",
    "    return fullReconstructedMatrix, truncatedReconstructed\n",
    "\n",
    "\n",
    "# Example matrix\n",
    "matrix = np.array([[1, 2, 3],\n",
    "              [4, 5, 6],\n",
    "              [7, 8, 9]], dtype=float)\n",
    "\n",
    "matrixReconstructViaSVD(matrix, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Largest Single Value Approximation using SVD\n",
    "\n",
    "def largestValueApproximationViaSVD(matrix):\n",
    "    _, singularValue,_ = np.linalg.svd(matrix)\n",
    "    largestValue = singularValue[0]\n",
    "    print(\"Largest Value Approximation(SVD):\", largestValue)\n",
    "    return largestValue\n",
    "\n",
    "def largestValueApproximationViaPowerIteration(matrix, iterations, tolerance = 1e-6):\n",
    "    b = np.random.rand(matrix.shape[1])\n",
    "    \n",
    "    for _ in range(iterations):\n",
    "        # Multiply by A^T A\n",
    "        nextDominantEigenVector = np.dot(matrix.T @ matrix, b)\n",
    "        b_next_norm = np.linalg.norm(nextDominantEigenVector)\n",
    "        \n",
    "        # Normalize the vector\n",
    "        nextDominantEigenVector /= b_next_norm\n",
    "        \n",
    "        # Check for convergence\n",
    "        if np.linalg.norm(b - nextDominantEigenVector) < tolerance:\n",
    "            break\n",
    "        \n",
    "        b = nextDominantEigenVector\n",
    "\n",
    "    # Compute the approximate largest singular value\n",
    "    largestSingularValue = np.sqrt(np.dot(b.T, np.dot(matrix.T @ matrix, b)))\n",
    "    print(\"Largest Value Approximation(PI):\", largestSingularValue)\n",
    "    return largestSingularValue\n",
    "\n",
    "vectorA = np.array([[1, 2, 3],\n",
    "              [4, 5, 6],\n",
    "              [7, 8, 9]], dtype=float)\n",
    "\n",
    "largestValueApproximationViaSVD(vectorA)\n",
    "largestValueApproximationViaPowerIteration(vectorA, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Projection and Orthogonality**  \n",
    "Projection and orthogonality are fundamental concepts in linear algebra and geometry, especially useful in understanding spaces, solving equations, and performing dimensionality reduction in machine learning. Let’s break them down clearly.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Projection**  \n",
    "A **projection** is the process of mapping a vector onto another vector or subspace, such that the result is **the closest point in that subspace to the original vector**.\n",
    "\n",
    "#### **Projection of a Vector onto Another Vector**\n",
    "Let $ \\mathbf{a} $ and $ \\mathbf{b} $ be vectors in the same space. The projection of $ \\mathbf{a} $ onto $ \\mathbf{b} $ is denoted as $ \\text{proj}_{\\mathbf{b}}(\\mathbf{a}) $ and is calculated as:\n",
    "$\n",
    "\\text{proj}_{\\mathbf{b}}(\\mathbf{a}) = \\frac{\\mathbf{a} \\cdot \\mathbf{b}}{\\mathbf{b} \\cdot \\mathbf{b}} \\mathbf{b}\n",
    "$\n",
    "Where:\n",
    "- $ \\mathbf{a} \\cdot \\mathbf{b} $: Dot product of $ \\mathbf{a} $ and $ \\mathbf{b} $.\n",
    "- $ \\mathbf{b} \\cdot \\mathbf{b} $: Squared norm (length) of $ \\mathbf{b} $.\n",
    "\n",
    "This gives the vector along $ \\mathbf{b} $ that is closest to $ \\mathbf{a} $.\n",
    "\n",
    "#### **Projection of a Vector onto a Subspace**\n",
    "If $ \\mathbf{a} $ is a vector and $ V $ is a subspace spanned by basis vectors $ \\mathbf{v}_1, \\mathbf{v}_2, \\dots, \\mathbf{v}_k $, the projection of $ \\mathbf{a} $ onto $ V $ is the vector in $ V $ that is closest to $ \\mathbf{a} $. This can be computed using:\n",
    "$\n",
    "\\text{proj}_V(\\mathbf{a}) = P \\mathbf{a}\n",
    "$\n",
    "Where $ P $ is the **projection matrix**, typically calculated as:\n",
    "$\n",
    "P = V (V^T V)^{-1} V^T\n",
    "$\n",
    "Here, $ V $ is a matrix whose columns are the basis vectors of the subspace.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Orthogonality**\n",
    "Two vectors $ \\mathbf{u} $ and $ \\mathbf{v} $ are said to be **orthogonal** if their dot product is zero:\n",
    "$\n",
    "\\mathbf{u} \\cdot \\mathbf{v} = 0\n",
    "$\n",
    "\n",
    "#### **Key Properties of Orthogonality**:\n",
    "1. **Right Angle**: Orthogonal vectors are at a $ 90^\\circ $ angle to each other.\n",
    "2. **Zero Dot Product**: $ \\mathbf{u} \\cdot \\mathbf{v} = 0 $ implies no overlap in direction.\n",
    "3. **Orthogonal Basis**: A set of vectors is an orthogonal basis for a subspace if:\n",
    "   - Each pair of vectors is orthogonal.\n",
    "   - The vectors span the subspace.\n",
    "\n",
    "#### **Orthogonal Complement**:\n",
    "The **orthogonal complement** of a subspace $ V $ is the set of all vectors orthogonal to every vector in $ V $. If $ V $ is a subspace of $ \\mathbb{R}^n $, its orthogonal complement $ V^\\perp $ satisfies:\n",
    "$\n",
    "\\mathbf{v} \\cdot \\mathbf{w} = 0 \\quad \\forall \\mathbf{v} \\in V, \\, \\mathbf{w} \\in V^\\perp\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "### **Connection Between Projection and Orthogonality**\n",
    "1. When projecting a vector $ \\mathbf{a} $ onto a subspace $ V $:\n",
    "   - The projection $ \\text{proj}_V(\\mathbf{a}) $ lies in the subspace $ V $.\n",
    "   - The **error vector** $ \\mathbf{e} = \\mathbf{a} - \\text{proj}_V(\\mathbf{a}) $ is **orthogonal** to $ V $.\n",
    "\n",
    "2. This means:\n",
    "   $\n",
    "   \\mathbf{e} \\cdot \\mathbf{v} = 0 \\quad \\forall \\mathbf{v} \\in V\n",
    "   $\n",
    "\n",
    "---\n",
    "\n",
    "### **Applications**\n",
    "1. **Projections**:\n",
    "   - Dimensionality reduction (e.g., Principal Component Analysis).\n",
    "   - Solving least squares problems.\n",
    "   - Calculating distances between a point and a subspace.\n",
    "\n",
    "2. **Orthogonality**:\n",
    "   - Defining coordinate systems (orthogonal bases like in Gram-Schmidt).\n",
    "   - Simplifying matrix computations (e.g., orthogonal matrices are easier to invert).\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project One Vector to Another\n",
    "\n",
    "def projectionToVector(vectorA, vectorB):\n",
    "    projectionOfAToB = (np.dot(vectorA, vectorB) / np.dot(vectorB, vectorB) / vectorB)\n",
    "    projectionOfBToA = (np.dot(vectorA, vectorB) / np.dot(vectorA, vectorA) / vectorA)\n",
    "    print(\"Projection(A-onTo-B):\", projectionOfAToB, \"\\nProjection(B-onTo-A):\", projectionOfBToA)\n",
    "    return projectionOfAToB, projectionOfBToA\n",
    "\n",
    "\n",
    "projectionToVector(np.array([1, 2]), np.array([3, 4]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Projection of Vector into Sub-Space\n",
    "\n",
    "def projectionIntoSubSpace(identityMatrix, vector):\n",
    "    # Identity Matrix is our basis - i used this matrix rather than two vectors\n",
    "    projectionMatrix = identityMatrix @ np.linalg.inv(identityMatrix.T @ identityMatrix) @ identityMatrix.T\n",
    "    projectionOnToSubspace = projectionMatrix @ vector\n",
    "    print(\"ProjectionOnToSubspace:\", projectionOnToSubspace)\n",
    "    return projectionOnToSubspace\n",
    "\n",
    "idM = np.identity(2)\n",
    "vector = np.array([3, 4])\n",
    "projectionIntoSubSpace(identityMatrix= idM, vector= vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Orthogonal Verification\n",
    "\n",
    "def isOrthogonal(vectorA, vectorB, tolerance=1e-10)-> bool:\n",
    "    dProduct = np.dot(vectorA, vectorB)\n",
    "    return np.abs(dProduct) < tolerance\n",
    "\n",
    "vectorU = np.array([1, 3, 1])\n",
    "vector = np.array([0, 2, 0])\n",
    "isOrtho = isOrthogonal(vector, vectorU)\n",
    "print(\"IsOrthogonal:\", isOrtho)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "#### **1. Definition of Orthogonal Complement**\n",
    "Given a vector $ \\mathbf{v} \\in \\mathbb{R}^3 $, the orthogonal complement of $ \\mathbf{v} $ is the set of all vectors $ \\mathbf{x} \\in \\mathbb{R}^3 $ such that:\n",
    "$\n",
    "\\mathbf{v} \\cdot \\mathbf{x} = 0\n",
    "$\n",
    "\n",
    "This orthogonal complement forms a plane in $ \\mathbb{R}^3 $.\n",
    "\n",
    "---\n",
    "To compute the **orthogonal complement** of a vector (or a set of vectors) in a 3D subspace, you essentially find the set of all vectors in $ \\mathbb{R}^3 $ that are orthogonal to the given vector(s). This can be achieved using the following steps:\n",
    "\n",
    "### **2. Steps to Compute the Orthogonal Complement**\n",
    "1. Let $ \\mathbf{v} = [v_1, v_2, v_3]^T $. Any vector $ \\mathbf{x} = [x_1, x_2, x_3]^T $ in the orthogonal complement satisfies:\n",
    "   $\n",
    "   v_1 x_1 + v_2 x_2 + v_3 x_3 = 0\n",
    "   $\n",
    "\n",
    "2. This equation represents a plane in $ \\mathbb{R}^3 $, and the orthogonal complement can be expressed in terms of a basis for that plane.\n",
    "\n",
    "3. Find two independent vectors that satisfy this equation. These two vectors will span the orthogonal complement of $ \\mathbf{v} $.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Computing Orthogonal Complement Using Cross Product**\n",
    "In $ \\mathbb{R}^3 $, the orthogonal complement of a single vector $ \\mathbf{v} $ can be found using the **cross product**:\n",
    "- The cross product of $ \\mathbf{v} $ with any other non-parallel vector gives a vector orthogonal to $ \\mathbf{v} $.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the Orthogonal complement of vector in 3D Sub-Space\n",
    "\n",
    "def orthogonal3DComplement(vector):\n",
    "    # Ensure the input vector is a 1D numpy array\n",
    "    vector = np.array(vector, dtype=float)\n",
    "    \n",
    "    # Choose two non-parallel vectors to calculate orthogonal vectors\n",
    "    if vector[0] == 0 and vector[1] == 0:  # If v is along the z-axis\n",
    "        firstBasis = np.array([1, 0, 0])  # x-axis\n",
    "        secondBasis = np.array([0, 1, 0])  # y-axis\n",
    "    else:\n",
    "        # First orthogonal vector (cross product of v with [1, 0, 0])\n",
    "        firstBasis = np.cross(vector, np.array([1, 0, 0]))\n",
    "        \n",
    "        # If basis1 is zero, use [0, 1, 0] instead\n",
    "        if np.linalg.norm(firstBasis) < 1e-10:\n",
    "            firstBasis = np.cross(vector, np.array([0, 1, 0]))\n",
    "        \n",
    "        # Second orthogonal vector (cross product of v with basis1)\n",
    "        secondBasis = np.cross(vector, firstBasis)\n",
    "    \n",
    "    # Normalize the basis vectors\n",
    "    firstBasis /= np.linalg.norm(firstBasis)\n",
    "    secondBasis /= np.linalg.norm(secondBasis)\n",
    "    \n",
    "    return firstBasis, secondBasis\n",
    "\n",
    "# Example vector\n",
    "vectorV = np.array([1, 2, 3])\n",
    "\n",
    "# Compute the orthogonal complement\n",
    "basis1, basis2 = orthogonal3DComplement(vectorV)\n",
    "\n",
    "print(\"Orthogonal complement basis vectors:\")\n",
    "print(\"Basis 1:\", basis1)\n",
    "print(\"Basis 2:\", basis2)\n",
    "print(\"Dot product with basis 1:\", np.dot(vectorV, basis1))\n",
    "print(\"Dot product with basis 2:\", np.dot(vectorV, basis2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Closest point in a subspace** \n",
    "\n",
    "---\n",
    "\n",
    "### **Mathematical Background**\n",
    "Given:\n",
    "- A vector $ \\mathbf{a} \\in \\mathbb{R}^n $.\n",
    "- A subspace $ V \\subset \\mathbb{R}^n $ spanned by basis vectors $ \\mathbf{v}_1, \\mathbf{v}_2, \\dots, \\mathbf{v}_k $.\n",
    "\n",
    "The projection of $ \\mathbf{a} $ onto $ V $ is given by:\n",
    "$\n",
    "\\text{proj}_V(\\mathbf{a}) = P \\mathbf{a}\n",
    "$\n",
    "Where $ P $ is the **projection matrix** defined as:\n",
    "$\n",
    "P = V (V^T V)^{-1} V^T\n",
    "$\n",
    "- $ V $ is a matrix whose columns are the basis vectors of the subspace $ V $.\n",
    "\n",
    "The closest point to $ \\mathbf{a} $ in the subspace is:\n",
    "$\n",
    "\\mathbf{p} = \\text{proj}_V(\\mathbf{a})\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "### **Steps**\n",
    "1. Construct the matrix $ V $ from the basis vectors of the subspace.\n",
    "2. Compute the projection matrix $ P $ using $ P = V (V^T V)^{-1} V^T $.\n",
    "3. Compute the projection $ \\mathbf{p} = P \\mathbf{a} $.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Closest Point in a Sub-Space to Give Vector\n",
    "\n",
    "def closestPointInSubSpace(vector, basisVectors):\n",
    "    # basis vectors almost similar to identity Matrix\n",
    "    V = np.column_stack(basisVectors)\n",
    "    \n",
    "    # Compute the projection matrix P\n",
    "    P = V @ np.linalg.inv(V.T @ V) @ V.T\n",
    "    \n",
    "    # Compute the projection of a onto the subspace\n",
    "    projection = P @ vector\n",
    "    print(\"ClosestPointInSubSpace:\", projection)\n",
    "    return projection\n",
    "\n",
    "# Example usage:\n",
    "# Given vector\n",
    "a = np.array([3, 4, 5])\n",
    "\n",
    "# Basis vectors of the subspace\n",
    "v1 = np.array([1, 0, 0])\n",
    "v2 = np.array([0, 1, 0])\n",
    "\n",
    "# Compute the closest point\n",
    "\n",
    "closestPointInSubSpace(a, [v1, v2])\n",
    "\n",
    "# Verifying Result\n",
    "error = a - closestPointInSubSpace(a, [v1, v2])\n",
    "\n",
    "print(\"Error vector:\", error)\n",
    "\n",
    "# Verify orthogonality with basis vectors\n",
    "print(\"Dot product with v1:\", np.dot(error, v1))\n",
    "print(\"Dot product with v2:\", np.dot(error, v2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -- Linear Transformation --\n",
    "\n",
    "**What is a Linear Transformation?**\n",
    "\n",
    "A **linear transformation** is a function between two vector spaces that preserves the operations of **vector addition** and **scalar multiplication**. In simpler terms, it is a rule that maps one vector to another while maintaining the structure of the space.\n",
    "\n",
    "Formally, if $ T: V \\to W $ is a linear transformation, it must satisfy the following properties for all vectors $ \\mathbf{u}, \\mathbf{v} \\in V $ and scalars $ c \\in \\mathbb{R} $ (or $ \\mathbb{C} $):\n",
    "\n",
    "1. **Additivity** (Preservation of vector addition):\n",
    "   $\n",
    "   T(\\mathbf{u} + \\mathbf{v}) = T(\\mathbf{u}) + T(\\mathbf{v})\n",
    "   $\n",
    "\n",
    "2. **Homogeneity** (Preservation of scalar multiplication):\n",
    "   $\n",
    "   T(c \\mathbf{u}) = c T(\\mathbf{u})\n",
    "   $\n",
    "\n",
    "---\n",
    "\n",
    "### **Examples of Linear Transformations**\n",
    "1. **Scaling**: $ T(\\mathbf{x}) = c \\mathbf{x} $, where $ c $ is a scalar.\n",
    "2. **Rotation**: Rotating a vector by a fixed angle in 2D or 3D space.\n",
    "3. **Reflection**: Reflecting vectors across a line (2D) or plane (3D).\n",
    "4. **Projection**: Projecting vectors onto a subspace (e.g., a plane or a line).\n",
    "5. **Identity Transformation**: $ T(\\mathbf{x}) = \\mathbf{x} $, which maps every vector to itself.\n",
    "\n",
    "---\n",
    "\n",
    "### **Matrix Representation**\n",
    "Every linear transformation $ T: \\mathbb{R}^n \\to \\mathbb{R}^m $ can be represented as a **matrix multiplication**. If $ T(\\mathbf{x}) = \\mathbf{y} $, then:\n",
    "$\n",
    "T(\\mathbf{x}) = A \\mathbf{x}\n",
    "$\n",
    "where:\n",
    "- $ \\mathbf{x} $ is the input vector in $ \\mathbb{R}^n $,\n",
    "- $ \\mathbf{y} $ is the output vector in $ \\mathbb{R}^m $,\n",
    "- $ A $ is an $ m \\times n $ matrix representing the linear transformation.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Properties**\n",
    "1. **Zero Vector Mapping**:\n",
    "   A linear transformation always maps the zero vector $ \\mathbf{0} $ in $ V $ to the zero vector $ \\mathbf{0} $ in $ W $:\n",
    "   $\n",
    "   T(\\mathbf{0}) = \\mathbf{0}\n",
    "   $\n",
    "\n",
    "2. **Linearity**:\n",
    "   The image of any linear combination of vectors is the linear combination of their images:\n",
    "   $\n",
    "   T(a \\mathbf{u} + b \\mathbf{v}) = a T(\\mathbf{u}) + b T(\\mathbf{v})\n",
    "   $\n",
    "\n",
    "---\n",
    "\n",
    "### **Geometric Interpretation**\n",
    "Linear transformations can be understood as operations that modify vectors in predictable ways:\n",
    "1. **Stretching or Shrinking**: Scaling vectors by a fixed factor.\n",
    "2. **Rotating**: Changing the direction of vectors while preserving their relative angles.\n",
    "3. **Shearing**: Slanting vectors in a specific direction.\n",
    "4. **Projecting**: Flattening vectors onto a lower-dimensional subspace.\n",
    "\n",
    "---\n",
    "\n",
    "### **Applications of Linear Transformations**\n",
    "1. **Graphics and Computer Vision**: Scaling, rotating, reflecting, or translating objects.\n",
    "2. **Machine Learning**: Dimensionality reduction (e.g., PCA) and neural networks (linear layers).\n",
    "3. **Physics**: Modeling forces, transformations, and coordinate systems.\n",
    "4. **Engineering**: Signal processing and control systems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Rotation Transformation** to a 2D vector, you multiply the vector by a **2D rotation matrix**. The rotation matrix for rotating a vector by an angle $ \\theta $ (in radians) counterclockwise around the origin is given by:\n",
    "\n",
    "$\n",
    "R(\\theta) =\n",
    "\\begin{bmatrix}\n",
    "\\cos(\\theta) & -\\sin(\\theta) \\\\\n",
    "\\sin(\\theta) & \\cos(\\theta)\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "If the vector is $ \\mathbf{v} = [x, y]^T $, the rotated vector $ \\mathbf{v'} $ is:\n",
    "$\n",
    "\\mathbf{v'} = R(\\theta) \\cdot \\mathbf{v}\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "### **Steps to Rotate a 2D Vector**\n",
    "1. **Define the rotation angle $ \\theta $** in radians. (To convert degrees to radians, use $ \\text{radians} = \\text{degrees} \\cdot \\frac{\\pi}{180} $.)\n",
    "2. **Construct the rotation matrix $ R(\\theta) $** using $ \\cos(\\theta) $ and $ \\sin(\\theta) $.\n",
    "3. **Multiply the rotation matrix by the vector $ \\mathbf{v} $.**\n",
    "\n",
    "---\n",
    "\n",
    "### **Applications of 2D Rotation**\n",
    "1. **Computer Graphics**: Rotating objects in a 2D space.\n",
    "2. **Physics Simulations**: Modeling motion and forces in a plane.\n",
    "3. **Robotics**: Rotating coordinate frames for sensors or robot arms.\n",
    "4. **Signal Processing**: Analyzing signals in polar coordinates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying Rotation Transformation on 2D Vector\n",
    "\n",
    "def vectorRotation2DTransformation(vector, degreeAngle):\n",
    "    radianAngle = np.radians(degreeAngle)\n",
    "    # 2D Matrix for 2D Vectors\n",
    "    matrixRotation = np.array([[np.cos(radianAngle), -np.sin(radianAngle)],[np.sin(radianAngle),  np.cos(radianAngle)]]) \n",
    "    rotatedVector = np.dot(matrixRotation, vector)\n",
    "    print(\"2D-VectorRotationTransformation: \", rotatedVector)\n",
    "    return rotatedVector\n",
    "\n",
    "# Example usage:\n",
    "vector = np.array([1, 0])  # A vector along the x-axis\n",
    "angle = 90  # Rotate by 90 degrees\n",
    "print(\"Original vector:\", vector)\n",
    "vectorRotation2DTransformation(vector, angle)\n",
    "\n",
    "# Multiple Angles\n",
    "angles = [0, 30, 45, 90, 180]  # Angles in degrees\n",
    "vector = np.array([1, 0])  # Original vector\n",
    "\n",
    "for angle in angles:\n",
    "    rotated_vector = vectorRotation2DTransformation(vector, angle)\n",
    "    print(f\"Rotated by {angle} degrees: {rotated_vector}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Scaling Transformation for 2D and 3D Vectors**\n",
    "\n",
    "A **scaling transformation** adjusts the magnitude of a vector without changing its direction (if the scaling factors are positive). In a geometric sense:\n",
    "- **Uniform Scaling** applies the same scaling factor to all axes, stretching or shrinking the object equally in all directions.\n",
    "- **Non-Uniform Scaling** applies different scaling factors to different axes.\n",
    "\n",
    "The scaling transformation is represented by a **diagonal matrix**. \n",
    "\n",
    "---\n",
    "\n",
    "### **Scaling Matrix**\n",
    "1. For **2D vectors**, the scaling matrix is:\n",
    "   $\n",
    "   S = \n",
    "   \\begin{bmatrix}\n",
    "   s_x & 0 \\\\\n",
    "   0 & s_y\n",
    "   \\end{bmatrix}\n",
    "   $\n",
    "   where $ s_x $ and $ s_y $ are the scaling factors along the $ x $- and $ y $-axes, respectively.\n",
    "\n",
    "   The transformed vector $ \\mathbf{v'} $ is:\n",
    "   $\n",
    "   \\mathbf{v'} = S \\cdot \\mathbf{v}\n",
    "   $\n",
    "\n",
    "2. For **3D vectors**, the scaling matrix is:\n",
    "   $\n",
    "   S = \n",
    "   \\begin{bmatrix}\n",
    "   s_x & 0 & 0 \\\\\n",
    "   0 & s_y & 0 \\\\\n",
    "   0 & 0 & s_z\n",
    "   \\end{bmatrix}\n",
    "   $\n",
    "   where $ s_x $, $ s_y $, and $ s_z $ are the scaling factors along the $ x $-, $ y $-, and $ z $-axes, respectively.\n",
    "\n",
    "   The transformed vector $ \\mathbf{v'} $ is:\n",
    "   $\n",
    "   \\mathbf{v'} = S \\cdot \\mathbf{v}\n",
    "   $\n",
    "\n",
    "---\n",
    "\n",
    "### **Applications of Scaling**\n",
    "1. **Graphics and Animation**: Resizing objects in 2D/3D space.\n",
    "2. **Data Normalization**: Rescaling data for better machine learning performance.\n",
    "3. **Physics Simulations**: Modeling transformations like stretching materials.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Scaling Transformation for 2D and 3D Vector\n",
    "\n",
    "def scalingTransformation2DVector(vector, sX, sY):\n",
    "    matrix2D = np.array([[sX, 0], [0, sY]])\n",
    "    scaled2DVector = np.dot(matrix2D,vector)\n",
    "    print(\"2D-VectorScalingTransformation:\", scaled2DVector)\n",
    "    return scaled2DVector\n",
    "\n",
    "\n",
    "def scalingTransformation3DVector(vector, sX, sY,sZ):\n",
    "    matrix3D =  np.array([[sX, 0, 0], [0, sY ,0],[0, 0, sZ]])\n",
    "    scaled3DVector = np.dot(matrix3D,vector)\n",
    "    print(\"3D-VectorScalingTransformation:\", scaled3DVector)\n",
    "    return scaled3DVector\n",
    "\n",
    "v2D = np.array([3, 4])\n",
    "scalingTransformation2DVector(v2D, 2, 3)\n",
    "v3D = np.array([1,2,3])\n",
    "scalingTransformation3DVector(v3D, 2,3,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Shearing Transformation in 2D**\n",
    "\n",
    "A **shearing transformation** in 2D skews the shape of an object along one or both axes. Unlike scaling, which uniformly stretches or shrinks, shearing displaces points in a direction proportional to their distance from a reference axis.\n",
    "\n",
    "\n",
    "\n",
    "### **Shearing Matrix**\n",
    "\n",
    "The shearing transformation is represented by a shear matrix. Depending on the axis of shearing:\n",
    "\n",
    "1. **Shear along the $ x $-axis**:\n",
    "   $\n",
    "   S_x =\n",
    "   \\begin{bmatrix}\n",
    "   1 & k_x \\\\\n",
    "   0 & 1\n",
    "   \\end{bmatrix}\n",
    "   $\n",
    "   Here, $ k_x $ is the shear factor along the $ x $-axis.\n",
    "\n",
    "   For a vector $ \\mathbf{v} = [x, y]^T $:\n",
    "   $\n",
    "   \\mathbf{v'} = S_x \\cdot \\mathbf{v} =\n",
    "   \\begin{bmatrix}\n",
    "   x + k_x y \\\\\n",
    "   y\n",
    "   \\end{bmatrix}\n",
    "   $\n",
    "\n",
    "2. **Shear along the $ y $-axis**:\n",
    "   $\n",
    "   S_y =\n",
    "   \\begin{bmatrix}\n",
    "   1 & 0 \\\\\n",
    "   k_y & 1\n",
    "   \\end{bmatrix}\n",
    "   $\n",
    "   Here, $ k_y $ is the shear factor along the $ y $-axis.\n",
    "\n",
    "   For a vector $ \\mathbf{v} = [x, y]^T $:\n",
    "   $\n",
    "   \\mathbf{v'} = S_y \\cdot \\mathbf{v} =\n",
    "   \\begin{bmatrix}\n",
    "   x \\\\\n",
    "   y + k_y x\n",
    "   \\end{bmatrix}\n",
    "   $\n",
    "\n",
    "3. **Combined Shear (both axes)**:\n",
    "   $\n",
    "   S =\n",
    "   \\begin{bmatrix}\n",
    "   1 & k_x \\\\\n",
    "   k_y & 1\n",
    "   \\end{bmatrix}\n",
    "   $\n",
    "\n",
    "----\n",
    "\n",
    "### **Applications of Shearing**\n",
    "1. **Computer Graphics**: Skewing shapes to simulate perspective or motion.\n",
    "2. **Data Transformation**: Modifying datasets geometrically for analysis or visualization.\n",
    "3. **Image Processing**: Skewing images for special effects.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Shearing Transformation in 2D\n",
    "def shearing2DTransformation(vector, kX=0, kY=0):\n",
    "    shearMatrix = np.array([[1, kX], [kY, 1]])\n",
    "    shearedVector = np.dot(shearMatrix, vector)\n",
    "    print(\"Shearing Transformation(2D):\", shearedVector)\n",
    "    return shearedVector\n",
    "vec2D = np.array([3, 4])\n",
    "shearedVectorX = shearing2DTransformation(vec2D, kX=2)  # Shear along x-axis\n",
    "shearedVectorY = shearing2DTransformation(vec2D, kY=1.5)  # Shear along y-axis\n",
    "\n",
    "print(\"Original vector:\", vec2D)\n",
    "print(\"Sheared along x-axis:\", shearedVectorX)\n",
    "print(\"Sheared along y-axis:\", shearedVectorY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Affine Transformation: Combining Rotation, Scaling, and Translation**\n",
    "\n",
    "An **affine transformation** is a combination of linear transformations (like rotation, scaling, or shearing) and translation. It preserves points, straight lines, and planes, and can be expressed as a matrix multiplication followed by a translation.\n",
    "\n",
    "For 2D transformations, the general form of an affine transformation is:\n",
    "$\n",
    "T(\\mathbf{v}) = A \\cdot \\mathbf{v} + \\mathbf{t}\n",
    "$\n",
    "where:\n",
    "- $ A $ is a $ 2 \\times 2 $ matrix representing linear transformations (rotation, scaling, etc.).\n",
    "- $ \\mathbf{t} = [t_x, t_y] $ is the translation vector.\n",
    "- $ \\mathbf{v} = [x, y]^T $ is the input vector.\n",
    "\n",
    "To simplify this, we use **homogeneous coordinates** to represent the transformation as a single $ 3 \\times 3 $ matrix:\n",
    "$\n",
    "\\mathbf{T} =\n",
    "\\begin{bmatrix}\n",
    "a_{11} & a_{12} & t_x \\\\\n",
    "a_{21} & a_{22} & t_y \\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "$\n",
    "and the input vector is represented as:\n",
    "$\n",
    "\\mathbf{v}_{homogeneous} = \n",
    "\\begin{bmatrix}\n",
    "x \\\\\n",
    "y \\\\\n",
    "1\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "The transformation becomes:\n",
    "$\n",
    "\\mathbf{v'} = \\mathbf{T} \\cdot \\mathbf{v}_{homogeneous}\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "### **Steps to Combine Rotation, Scaling, and Translation**\n",
    "\n",
    "1. **Rotation Matrix** ($ R(\\theta) $):\n",
    "   $\n",
    "   R(\\theta) =\n",
    "   \\begin{bmatrix}\n",
    "   \\cos(\\theta) & -\\sin(\\theta) \\\\\n",
    "   \\sin(\\theta) & \\cos(\\theta)\n",
    "   \\end{bmatrix}\n",
    "   $\n",
    "\n",
    "2. **Scaling Matrix** ($ S $):\n",
    "   $\n",
    "   S =\n",
    "   \\begin{bmatrix}\n",
    "   s_x & 0 \\\\\n",
    "   0 & s_y\n",
    "   \\end{bmatrix}\n",
    "   $\n",
    "\n",
    "3. **Combine Rotation and Scaling**:\n",
    "   $\n",
    "   A = R(\\theta) \\cdot S\n",
    "   $\n",
    "\n",
    "4. **Add Translation**:\n",
    "   Include the translation vector $ \\mathbf{t} = [t_x, t_y] $ in the affine matrix.\n",
    "\n",
    "---\n",
    "\n",
    "### **Applications of Affine Transformation**\n",
    "1. **Computer Graphics**: Modeling object transformations in 2D and 3D spaces.\n",
    "2. **Image Processing**: Geometric transformations like rotation, scaling, and translation.\n",
    "3. **Robotics**: Transforming coordinate frames for sensors and actuators.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def affine2DTransformation(vector, degreesAngle, scalingAlongX, scalingAlongY, transformationAlongX, TransformationAlongY):\n",
    "\n",
    "    # Convert angle to radians\n",
    "    radianAngle = np.radians(degreesAngle)\n",
    "    \n",
    "    # Rotation matrix\n",
    "    matrixRotation = np.array([\n",
    "        [np.cos(radianAngle), -np.sin(radianAngle)],\n",
    "        [np.sin(radianAngle),  np.cos(radianAngle)]\n",
    "    ])\n",
    "    \n",
    "    # Scaling matrix\n",
    "    scalingMatrix = np.array([\n",
    "        [scalingAlongX, 0],\n",
    "        [0, scalingAlongY]\n",
    "    ])\n",
    "    \n",
    "    # Combined rotation and scaling\n",
    "    linearTransformation = np.dot(matrixRotation, scalingMatrix)\n",
    "    \n",
    "    # Affine transformation matrix\n",
    "    affine_matrix = np.array([\n",
    "        [linearTransformation[0, 0], linearTransformation[0, 1], transformationAlongX],\n",
    "        [linearTransformation[1, 0], linearTransformation[1, 1], TransformationAlongY],\n",
    "        [0, 0, 1]\n",
    "    ])\n",
    "    \n",
    "    # Convert the vector to homogeneous coordinates\n",
    "    homogeneousVector = np.array([vector[0], vector[1], 1])\n",
    "    \n",
    "    # Apply the affine transformation\n",
    "    transformedVector = np.dot(affine_matrix, homogeneousVector)\n",
    "    \n",
    "    # Return the transformed vector (drop the homogeneous coordinate)\n",
    "    return transformedVector[:2]\n",
    "\n",
    "# Example usage\n",
    "vector = np.array([1, 1])  # Original vector\n",
    "angle = 45  # Rotation angle in degrees\n",
    "sx, sy = 2, 3  # Scaling factors\n",
    "tx, ty = 5, 10  # Translation\n",
    "\n",
    "transformedVector = affine2DTransformation(vector, angle, sx, sy, tx, ty)\n",
    "print(\"Original vector:\", vector)\n",
    "print(\"Transformed vector:\", transformedVector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**How to Verify if a Given Transformation is Linear?**\n",
    "\n",
    "A transformation $ T: \\mathbb{R}^n \\to \\mathbb{R}^m $ is **linear** if it satisfies the following two properties for all vectors $ \\mathbf{u}, \\mathbf{v} \\in \\mathbb{R}^n $ and any scalar $ c $:\n",
    "\n",
    "1. **Additivity (Preservation of Vector Addition)**:\n",
    "   $\n",
    "   T(\\mathbf{u} + \\mathbf{v}) = T(\\mathbf{u}) + T(\\mathbf{v})\n",
    "   $\n",
    "\n",
    "2. **Homogeneity (Preservation of Scalar Multiplication)**:\n",
    "   $\n",
    "   T(c \\mathbf{u}) = c T(\\mathbf{u})\n",
    "   $\n",
    "\n",
    "If both properties hold, the transformation is linear. Otherwise, it is not.\n",
    "\n",
    "---\n",
    "\n",
    "### **Steps to Verify Linearity**\n",
    "\n",
    "1. **Check Additivity**:\n",
    "   - Pick two arbitrary vectors $ \\mathbf{u} $ and $ \\mathbf{v} $.\n",
    "   - Compute $ T(\\mathbf{u} + \\mathbf{v}) $ and compare it with $ T(\\mathbf{u}) + T(\\mathbf{v}) $.\n",
    "\n",
    "2. **Check Homogeneity**:\n",
    "   - Pick a scalar $ c $ and an arbitrary vector $ \\mathbf{u} $.\n",
    "   - Compute $ T(c \\mathbf{u}) $ and compare it with $ c T(\\mathbf{u}) $.\n",
    "\n",
    "If both conditions are satisfied for any choice of $ \\mathbf{u} $, $ \\mathbf{v} $, and $ c $, the transformation is linear.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Points**\n",
    "1. If **either additivity or homogeneity fails**, the transformation is not linear.\n",
    "2. If a transformation includes translation (e.g., $ T([x, y]) = [x + 1, y + 2] $), it is **not linear** because translation violates both properties.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify given transformation is Linear\n",
    "def transformation(vector):\n",
    "    \"\"\"\n",
    "    Example transformation function T([x, y]) = [2x + y, x - 3y].\n",
    "    \"\"\"\n",
    "    x, y = vector\n",
    "    return np.array([2 * x + y, x - 3 * y])\n",
    "\n",
    "def isTransformationLinear(transformation, vectorU, vectorV, scalar):\n",
    "        # Check additivity: T(u + v) == T(u) + T(v)\n",
    "    additivity = np.allclose(\n",
    "        transformation(vectorU + vectorV), \n",
    "        transformation(vectorU) + transformation(vectorV)\n",
    "    )\n",
    "    \n",
    "    # Check homogeneity: T(c * u) == c * T(u)\n",
    "    homogeneity = np.allclose(\n",
    "        transformation(scalar * vectorU), \n",
    "        scalar * transformation(vectorU)\n",
    "    )\n",
    "    \n",
    "    return additivity and homogeneity\n",
    "\n",
    "# Test vectors and scalar\n",
    "u = np.array([1, 2])\n",
    "v = np.array([3, 4])\n",
    "c = 2\n",
    "\n",
    "# Verify linearity\n",
    "isLinear = isTransformationLinear(transformation, u, v, c)\n",
    "print(\"IsTransformationLinear:\", isLinear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -- Applications in Machine Learning ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Principal Component Analysis (PCA)**\n",
    "\n",
    "PCA is a **linear transformation** technique that aims to find the directions (called **principal components**) in which the variance of the data is maximized. PCA reduces the number of features in the data while preserving as much information (variance) as possible. It does so by projecting the data onto a new set of axes corresponding to the eigenvectors of the covariance matrix of the data.\n",
    "\n",
    "---\n",
    "\n",
    "### **Steps in PCA (or Linear Component Analysis)**\n",
    "\n",
    "1. **Center the Data**:  \n",
    "   Subtract the mean of each feature from the data points to center the dataset around the origin. This ensures that the data has zero mean.\n",
    "\n",
    "2. **Calculate the Covariance Matrix**:  \n",
    "   Compute the covariance matrix to understand how different features in the dataset vary with respect to each other.\n",
    "\n",
    "3. **Find the Eigenvalues and Eigenvectors**:  \n",
    "   The eigenvectors of the covariance matrix represent the directions (principal components) in which the data varies the most, while the eigenvalues represent the magnitude of variance in each direction.\n",
    "\n",
    "4. **Sort Eigenvalues and Eigenvectors**:  \n",
    "   Sort the eigenvectors by their corresponding eigenvalues in descending order. The eigenvector with the largest eigenvalue corresponds to the **first principal component** (PC1), and so on.\n",
    "\n",
    "5. **Project Data onto Principal Components**:  \n",
    "   Choose the top $k$ eigenvectors (principal components) and project the original data onto these new axes. The result is a reduced-dimensional representation of the data.\n",
    "\n",
    "---\n",
    "\n",
    "### **Applications of PCA (Linear Component Analysis)**\n",
    "\n",
    "- **Dimensionality Reduction**: Reduce the number of features in the dataset while retaining as much information as possible.\n",
    "- **Noise Reduction**: By discarding lower-variance components, PCA can help eliminate noise in the data.\n",
    "- **Visualization**: For high-dimensional data, PCA allows visualization by projecting data onto 2D or 3D space.\n",
    "- **Feature Extraction**: PCA helps in identifying the most important features that capture the underlying structure of the data.\n",
    "\n",
    "---\n",
    "\n",
    "### **Mathematical Representation of PCA**\n",
    "\n",
    "1. **Data Centering**:  \n",
    "   Let $ X $ be a data matrix of shape $ (n \\times m) $, where $ n $ is the number of samples and $ m $ is the number of features. The data is centered by subtracting the mean of each feature:\n",
    "   $\n",
    "   X_{\\text{centered}} = X - \\mu\n",
    "   $\n",
    "   where $ \\mu $ is the mean of each column in $ X $.\n",
    "\n",
    "2. **Covariance Matrix**:  \n",
    "   The covariance matrix $ C $ is computed as:\n",
    "   $\n",
    "   C = \\frac{1}{n-1} X_{\\text{centered}}^T X_{\\text{centered}}\n",
    "   $\n",
    "\n",
    "3. **Eigenvectors and Eigenvalues**:  \n",
    "   Solve the eigenvalue problem:\n",
    "   $\n",
    "   C v = \\lambda v\n",
    "   $\n",
    "   where $ \\lambda $ is the eigenvalue and $ v $ is the eigenvector.\n",
    "\n",
    "4. **Projection**:  \n",
    "   To project the data onto the new subspace, we multiply the centered data by the matrix of the top $ k $ eigenvectors:\n",
    "   $\n",
    "   X_{\\text{reduced}} = X_{\\text{centered}} V_k\n",
    "   $\n",
    "   where $ V_k $ contains the top $ k $ eigenvectors.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### **Summary**\n",
    "- **Linear Component Analysis** is often a reference to **Principal Component Analysis (PCA)**, a technique for dimensionality reduction, feature extraction, and data analysis.\n",
    "- PCA identifies the **principal components** that capture the most variance in the data, and projects the data onto these components.\n",
    "- It has applications in reducing the complexity of large datasets, improving the performance of machine learning models, and aiding in data visualization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def principleComponentAnalysis(matrix, components):\n",
    "    # Step 1: Center the data (subtract the mean)\n",
    "    centeredMatrix = matrix - np.mean(matrix, axis=0)\n",
    "    \n",
    "    # Step 2: Compute the covariance matrix\n",
    "    matrixCovariance = np.cov(centeredMatrix.T)\n",
    "    \n",
    "    # Step 3: Compute eigenvalues and eigenvectors\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(matrixCovariance)\n",
    "    \n",
    "    # Step 4: Sort eigenvalues and eigenvectors in descending order\n",
    "    sortedIndices = np.argsort(eigenvalues)[::-1]  # Indices for descending order\n",
    "    eigenvalues = eigenvalues[sortedIndices]       # Sort eigenvalues\n",
    "    eigenvectors = eigenvectors[:, sortedIndices]  # Sort eigenvectors\n",
    "    \n",
    "    # Step 5: Select the top `components` eigenvectors\n",
    "    topEigenvectorsComponents = eigenvectors[:, :components]\n",
    "    \n",
    "    # Step 6: Project the data onto the new subspace\n",
    "    reducedMatrix = centeredMatrix.dot(topEigenvectorsComponents)\n",
    "    print(\"PrincipleComponentAnalysis(PCA):\\n\", reducedMatrix)\n",
    "    return reducedMatrix\n",
    "\n",
    "data = np.array([\n",
    "    [2.5, 2.4],\n",
    "    [0.5, 0.7],\n",
    "    [2.2, 2.9],\n",
    "    [1.9, 2.2],\n",
    "    [3.1, 3.0],\n",
    "    [2.3, 2.7],\n",
    "    [2.0, 1.6],\n",
    "    [1.0, 1.1],\n",
    "    [1.5, 1.6],\n",
    "    [1.1, 0.9]\n",
    "])\n",
    "X = np.array([\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6],\n",
    "    [7, 8, 9],\n",
    "    [10, 11, 12]\n",
    "])\n",
    "principleComponentAnalysis(X, components=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- Notice 2nd Question ---\n",
    "we will implement visualization later for everything"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Cosine Similarity Between Two Vectors**\n",
    "\n",
    "Cosine similarity is a measure of similarity between two non-zero vectors in an inner product space. It is defined as the cosine of the angle between the two vectors, which can be computed using the dot product formula. It is commonly used in text analysis, information retrieval, and machine learning to measure the similarity between two vectors, especially in high-dimensional spaces.\n",
    "\n",
    "The formula for cosine similarity between two vectors $ \\mathbf{A} $ and $ \\mathbf{B} $ is:\n",
    "$\n",
    "\\text{Cosine Similarity} = \\frac{\\mathbf{A} \\cdot \\mathbf{B}}{||\\mathbf{A}|| \\, ||\\mathbf{B}||}\n",
    "$\n",
    "Where:\n",
    "- $ \\mathbf{A} \\cdot \\mathbf{B} $ is the dot product of the two vectors.\n",
    "- $ ||\\mathbf{A}|| $ and $ ||\\mathbf{B}|| $ are the magnitudes (or Euclidean norms) of the vectors $ \\mathbf{A} $ and $ \\mathbf{B} $, respectively.\n",
    "\n",
    "The cosine similarity ranges from:\n",
    "- $ 1 $ when the vectors are exactly the same (or have the same direction).\n",
    "- $ 0 $ when the vectors are orthogonal (i.e., no similarity).\n",
    "- $ -1 $ when the vectors are diametrically opposite.\n",
    "\n",
    "---\n",
    "\n",
    "### **Steps to Compute Cosine Similarity**\n",
    "1. **Compute the Dot Product**: The dot product of two vectors $ \\mathbf{A} $ and $ \\mathbf{B} $ is given by:\n",
    "   $\n",
    "   \\mathbf{A} \\cdot \\mathbf{B} = \\sum_{i=1}^{n} A_i \\cdot B_i\n",
    "   $\n",
    "   \n",
    "2. **Compute the Magnitudes (Norms)** of the vectors $ \\mathbf{A} $ and $ \\mathbf{B} $:\n",
    "   $\n",
    "   ||\\mathbf{A}|| = \\sqrt{\\sum_{i=1}^{n} A_i^2}, \\quad ||\\mathbf{B}|| = \\sqrt{\\sum_{i=1}^{n} B_i^2}\n",
    "   $\n",
    "   \n",
    "3. **Compute the Cosine Similarity** using the formula:\n",
    "   $\n",
    "   \\text{Cosine Similarity} = \\frac{\\mathbf{A} \\cdot \\mathbf{B}}{||\\mathbf{A}|| \\, ||\\mathbf{B}||}\n",
    "   $\n",
    "\n",
    "---\n",
    "### SpecialCases:\n",
    "\n",
    "1. Identical Vectors:\n",
    "If $A=B$, the cosine similarity is 1.\n",
    "\n",
    "2. Orthogonal Vectors:\n",
    "If $A$ and #B$ are orthogonal (dot product is 0), the cosine similarity is 0.\n",
    "\n",
    "3. Opposite Vectors:\n",
    "If $A$ and $B$ point in opposite directions, the cosine similarity is -1.\n",
    "\n",
    "---\n",
    "\n",
    "### **Use Cases of Cosine Similarity**\n",
    "- **Text Analysis**: Cosine similarity is frequently used in Natural Language Processing (NLP) to compare text documents, as each document is often represented as a vector of word frequencies or term weights.\n",
    "- **Recommendation Systems**: Cosine similarity is used to measure the similarity between items or users based on their preferences or ratings.\n",
    "- **Clustering**: In clustering tasks, cosine similarity is used to group similar data points together.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Cosine similarity between two vectors\n",
    "def cosineSimilarity(vectorA, vectorB):\n",
    "    # Compute the dot product of A and B\n",
    "    dotProduct = np.dot(vectorA, vectorB)\n",
    "    \n",
    "    # Compute the magnitudes (norms) of A and B\n",
    "    vectorANorm = np.linalg.norm(vectorA)\n",
    "    vectorBNorm = np.linalg.norm(vectorB)\n",
    "    \n",
    "    # Compute cosine similarity\n",
    "    cosineSimilarities = dotProduct / (vectorANorm * vectorBNorm)\n",
    "    print(\"Cosine Similarity:\", cosineSimilarities)\n",
    "    return cosineSimilarities\n",
    "\n",
    "# Define two vectors\n",
    "matrix = np.array([1, 2, 3])\n",
    "B = np.array([4, 5, 6])\n",
    "\n",
    "cosineSimilarity(matrix, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **Matrix Factorization Overview**\n",
    "Matrix factorization decomposes the user-item interaction matrix into two lower-dimensional matrices: one representing users and the other representing items. The most popular algorithm for this is **Singular Value Decomposition (SVD)** or its variants like **Funk SVD** (used in the Netflix Prize).\n",
    "\n",
    "implementing a recommendation system using **matrix factorization** with **gradient descent**. \n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "1. **User-Item Interaction Matrix**:\n",
    "   - Let $ R $ be a matrix of size $ m \\times n $, where $ m $ is the number of users and $ n $ is the number of items.\n",
    "   - Each entry $ R_{ij} $ represents the rating given by user $ i $ to item $ j $. If no rating exists, the entry is typically set to 0 or left as missing.\n",
    "\n",
    "2. **Matrix Factorization**:\n",
    "   - Factorize $ R $ into two matrices:\n",
    "     - $ P $ (user matrix) of size $ m \\times k $, where $ k $ is the number of latent factors.\n",
    "     - $ Q $ (item matrix) of size $ n \\times k $.\n",
    "   - The goal is to approximate $ R $ as $ R \\approx P \\cdot Q^T $.\n",
    "\n",
    "3. **Objective**:\n",
    "   - Minimize the difference between the actual ratings $ R $ and the predicted ratings $ \\hat{R} = P \\cdot Q^T $.\n",
    "   - Use a loss function like **Mean Squared Error (MSE)**:\n",
    "     $\n",
    "     \\text{Loss} = \\sum_{(i,j) \\in \\text{observed}} (R_{ij} - \\hat{R}_{ij})^2 + \\lambda (\\|P\\|^2 + \\|Q\\|^2)\n",
    "     $\n",
    "     where $ \\lambda $ is the regularization parameter to prevent over-fitting.\n",
    "\n",
    "4. **Optimization**:\n",
    "   - Use **gradient descent** to update $ P $ and $ Q $ iteratively.\n",
    "\n",
    "---\n",
    "\n",
    "### **Implementation Steps**\n",
    "\n",
    "1. **Initialize Matrices**:\n",
    "   - Randomly initialize $ P $ and $ Q $.\n",
    "\n",
    "2. **Define Loss Function**:\n",
    "   - Compute the predicted ratings $ \\hat{R} = P \\cdot Q^T $.\n",
    "   - Calculate the MSE loss between $ R $ and $ \\hat{R} $.\n",
    "\n",
    "3. **Gradient Descent**:\n",
    "   - Compute gradients of the loss with respect to $ P $ and $ Q $.\n",
    "   - Update $ P $ and $ Q $ using the gradients.\n",
    "\n",
    "4. **Make Predictions**:\n",
    "   - After training, use $ \\hat{R} = P \\cdot Q^T $ to predict missing ratings.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **Explanation**\n",
    "\n",
    "1. **Training**:\n",
    "   - The model learns the latent factors (user preferences and item characteristics) by minimizing the loss function.\n",
    "\n",
    "2. **Prediction**:\n",
    "   - The predicted ratings are computed as $ \\hat{R} = P \\cdot Q^T $.\n",
    "\n",
    "3. **Recommendation**:\n",
    "   - For a given user, recommend items with the highest predicted ratings that they haven’t interacted with yet.\n",
    "\n",
    "---\n",
    "\n",
    "### **Improvements**\n",
    "\n",
    "1. **Stochastic Gradient Descent (SGD)**:\n",
    "   - Instead of updating the matrices for each rating, use SGD for faster convergence.\n",
    "\n",
    "2. **Bias Terms**:\n",
    "   - Add user and item bias terms to account for systematic differences in ratings.\n",
    "\n",
    "3. **Advanced Algorithms**:\n",
    "   - Use libraries like **Surprise** or **LightFM** for more advanced matrix factorization techniques.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MatrixFactorization:\n",
    "    def __init__(self, matrix, latentFactor, gdLearningRate=0.01, regularizationPram=0.01, iterations=100):\n",
    "        self.R = matrix\n",
    "        self.m, self.n = matrix.shape\n",
    "        self.k = latentFactor\n",
    "        self.learningRate = gdLearningRate\n",
    "        self.regParam = regularizationPram\n",
    "        self.iterations = iterations\n",
    "\n",
    "        # Initialize user and item matrices with random values\n",
    "        self.P = np.random.normal(scale=1./self.k, size=(self.m, self.k))\n",
    "        self.Q = np.random.normal(scale=1./self.k, size=(self.n, self.k))\n",
    "\n",
    "    def train(self):\n",
    "        # Training using gradient descent\n",
    "        for iteration in range(self.iterations):\n",
    "            for i in range(self.m):\n",
    "                for j in range(self.n):\n",
    "                    if self.R[i, j] > 0:  # Ignore missing ratings\n",
    "                        # Compute the prediction error\n",
    "                        error = self.R[i, j] - np.dot(self.P[i, :], self.Q[j, :].T)\n",
    "\n",
    "                        # Update user and item matrices\n",
    "                        self.P[i, :] += self.learningRate * (error * self.Q[j, :] - self.regParam * self.P[i, :])\n",
    "                        self.Q[j, :] += self.learningRate * (error * self.P[i, :] - self.regParam * self.Q[j, :])\n",
    "\n",
    "            # Print loss for monitoring\n",
    "            if (iteration + 1) % 10 == 0:\n",
    "                loss = self.computeLoss()\n",
    "                print(f\"Iterations: {iteration + 1}, Loss: {loss}\")\n",
    "\n",
    "    def computeLoss(self):\n",
    "        # Compute the total loss (MSE + regularization)\n",
    "        predictedRegularization = np.dot(self.P, self.Q.T)\n",
    "        error = 0\n",
    "        for i in range(self.m):\n",
    "            for j in range(self.n):\n",
    "                if self.R[i, j] > 0:\n",
    "                    error += (self.R[i, j] - predictedRegularization[i, j]) ** 2\n",
    "        # Add regularization terms\n",
    "        regularizationTerm = (self.regParam / 2) * (np.linalg.norm(self.P) ** 2 + np.linalg.norm(self.Q) ** 2)\n",
    "        return error + regularizationTerm\n",
    "\n",
    "    def predict(self):\n",
    "        # Predict the full rating matrix\n",
    "        return np.dot(self.P, self.Q.T)\n",
    "\n",
    "\n",
    "# Example user-item interaction matrix (4 users, 5 items)\n",
    "# R = np.array([\n",
    "#     [5, 3, 0, 1, 4],\n",
    "#     [4, 0, 0, 1, 5],\n",
    "#     [1, 1, 0, 5, 0],\n",
    "#     [0, 0, 4, 4, 3]\n",
    "# ])\n",
    "R = np.array([\n",
    "    [5, 3, 0, 1],\n",
    "    [4, 0, 0, 1],\n",
    "    [1, 1, 0, 5],\n",
    "    [1, 0, 0, 4],\n",
    "    [0, 1, 5, 4],\n",
    "])\n",
    "\n",
    "# Initialize and train the model\n",
    "mf = MatrixFactorization(R, latentFactor=2, gdLearningRate=0.01, regularizationPram=0.01, iterations=100)\n",
    "mf.train()\n",
    "\n",
    "# Make predictions\n",
    "predicted = mf.predict()\n",
    "print(\"Predicted Ratings:\\n\", predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solve a Least Square problem for linear regression Using matrix Operations\n",
    "\n",
    "-- Notice Linear Regression is part of machine Learning algorithm (Not specifically related to Linear Algebra)so we will learn it when we will learn about algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --- Advance Challenges ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Cholesky decomposition**\n",
    "is a method for decomposing a **symmetric positive-definite (SPD) matrix** into the product of a lower triangular matrix and its transpose. Specifically, for an SPD matrix $ A $, the Cholesky decomposition is:\n",
    "\n",
    "$\n",
    "A = L L^T\n",
    "$\n",
    "\n",
    "where:\n",
    "- $ L $ is a **lower triangular matrix**.\n",
    "- $ L^T $ is the **transpose of $ L $**.\n",
    "\n",
    "Cholesky decomposition is widely used in numerical linear algebra, optimization, and solving systems of linear equations.\n",
    "\n",
    "---\n",
    "\n",
    "### **Steps for Cholesky Decomposition**\n",
    "\n",
    "Given a symmetric positive-definite matrix $ A $, the Cholesky decomposition can be computed as follows:\n",
    "\n",
    "1. **Check if $ A $ is symmetric and positive-definite**:\n",
    "   - $ A $ must satisfy $ A = A^T $ (symmetric).\n",
    "   - All eigenvalues of $ A $ must be positive (positive-definite).\n",
    "\n",
    "2. **Compute the Cholesky factor $ L $**:\n",
    "   - Use the following algorithm to compute the elements of $ L $:\n",
    "\n",
    "     $\n",
    "     L_{ii} = \\sqrt{A_{ii} - \\sum_{k=1}^{i-1} L_{ik}^2}\n",
    "     $\n",
    "     $\n",
    "     L_{ji} = \\frac{1}{L_{ii}} \\left( A_{ji} - \\sum_{k=1}^{i-1} L_{jk} L_{ik} \\right) \\quad \\text{for } j > i\n",
    "     $\n",
    "\n",
    "3. **Verify the decomposition**:\n",
    "   - Ensure that $ A = L L^T $.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Points**\n",
    "\n",
    "1. **Symmetric Positive-Definite Matrix**:\n",
    "   - Cholesky decomposition only works for symmetric positive-definite matrices.\n",
    "\n",
    "2. **Efficiency**:\n",
    "   - Cholesky decomposition is more efficient than LU decomposition for solving systems of linear equations with SPD matrices.\n",
    "\n",
    "3. **Applications**:\n",
    "   - Solving linear systems $ Ax = b $.\n",
    "   - Generating correlated random variables in Monte Carlo simulations.\n",
    "   - Optimization problems (e.g., in quadratic programming).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation for Cholesky Decomposition for Symmetric positive-definite matrix\n",
    "\n",
    "def choleskyDecomposition(matrix):\n",
    "    n = matrix.shape[0]\n",
    "    lowerTriangularMatrix = np.zeros_like(matrix)\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1):\n",
    "            if i == j:\n",
    "                # Diagonal elements\n",
    "                lowerTriangularMatrix[i, j] = np.sqrt(matrix[i, j] - np.sum(lowerTriangularMatrix[i, :j] ** 2))\n",
    "            else:\n",
    "                # Off-diagonal elements\n",
    "                lowerTriangularMatrix[i, j] = (matrix[i, j] - np.sum(lowerTriangularMatrix[i, :j] * lowerTriangularMatrix[j, :j])) / lowerTriangularMatrix[j, j]\n",
    "\n",
    "    return lowerTriangularMatrix\n",
    "\n",
    "matrix = np.array([\n",
    "    [4, 12, -16],\n",
    "    [12, 37, -43],\n",
    "    [-16, -43, 98]\n",
    "])\n",
    "\n",
    "print(\"Cholesky Decomposition:\\n\", choleskyDecomposition(matrix))\n",
    "print(\"Numpy Cholesky Decomposition:\\n\", np.linalg.cholesky(matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **Moore-PenRose PseudoInverse**\n",
    "is a generalization of the matrix inverse that can be applied to any matrix, including rectangular and singular matrices. It is widely used in solving linear systems, least squares problems, and in machine learning (e.g., in linear regression).\n",
    "\n",
    "For a matrix $ A $, the pseudoinverse $ A^+ $ satisfies the following properties:\n",
    "1. $ A A^+ A = A $\n",
    "2. $ A^+ A A^+ = A^+ $\n",
    "3. $ (A A^+)^T = A A^+ $\n",
    "4. $ (A^+ A)^T = A^+ A $\n",
    "\n",
    "---\n",
    "\n",
    "### **Methods to Compute the PseudoInverse**\n",
    "\n",
    "There are several ways to compute the pseudoinverse:\n",
    "\n",
    "1. **Singular Value Decomposition (SVD)**:\n",
    "   - Decompose $ A $ into $ U \\Sigma V^T $, where $ U $ and $ V $ are orthogonal matrices, and $ \\Sigma $ is a diagonal matrix of singular values.\n",
    "   - The pseudoinverse is given by:\n",
    "     $A^+ = V \\Sigma^+ U^T$\n",
    "     where $ \\Sigma^+ $ is obtained by taking the reciprocal of non-zero singular values and transposing the result.\n",
    "\n",
    "1. **Using the Normal Equation**:\n",
    "   - For a full-rank matrix $ A $, the pseudoinverse can be computed as:\n",
    "  \n",
    "   $A^+ = (A^T A)^{-1} A^T \\quad \\text{(if $ A $ has full column rank)}$\n",
    "   $A^+ = A^T (A A^T)^{-1} \\quad \\text{(if $ A $ has full row rank)}$\n",
    "\n",
    "2. **Using QR Decomposition**:\n",
    "   - If $ A $ has full column rank, compute the QR decomposition $ A = Q R $, where $ Q $ is orthogonal and $ R $ is upper triangular.\n",
    "   - The pseudoinverse is:\n",
    "     $A^+ = R^{-1} Q^T $\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Points**\n",
    "\n",
    "1. **Applications**:\n",
    "   - Solving linear systems $ Ax = b $ when $ A $ is not invertible.\n",
    "   - Least squares problems (e.g., linear regression).\n",
    "   - Regularization in machine learning.\n",
    "\n",
    "2. **Numerical Stability**:\n",
    "   - SVD is numerically stable and works for any matrix, including rank-deficient and rectangular matrices.\n",
    "\n",
    "3. **Efficiency**:\n",
    "   - For large matrices, SVD can be computationally expensive. In such cases, iterative methods or approximations may be used.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MooreRose PseudoInverse\n",
    "\n",
    "\n",
    "def moorePenRosePseudoInverse(matrix, tolerance =1e-10):\n",
    "    # Perform SVD\n",
    "    U, S, Vt = np.linalg.svd(matrix, full_matrices=False)\n",
    "    \n",
    "    # Compute the reciprocal of singular values, avoiding division by zero\n",
    "    singularValue = np.diag([1/s if s > tolerance else 0 for s in S])\n",
    "    \n",
    "    # Compute the pseudo-inverse: V * S^+ * U^T\n",
    "    pseudoInverse = Vt.T @ singularValue @ U.T\n",
    "    return pseudoInverse\n",
    "matrix = np.array([\n",
    "    [1, 2],\n",
    "    [3, 4],\n",
    "    [5, 6]\n",
    "])\n",
    "print(\"Moore-PenRose PseudoInverse:\\n\", moorePenRosePseudoInverse(matrix))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **Conjugate Gradient (CG) method**\n",
    "is an iterative algorithm used to solve systems of linear equations of the form:\n",
    "\n",
    "$\n",
    "A x = b\n",
    "$\n",
    "\n",
    "where:\n",
    "- $ A $ is a **symmetric positive-definite (SPD)** matrix.\n",
    "- $ x $ is the solution vector.\n",
    "- $ b $ is the right-hand side vector.\n",
    "\n",
    "The CG method is particularly efficient for large, sparse systems because it avoids the need for matrix factorization and uses only matrix-vector products.\n",
    "\n",
    "---\n",
    "\n",
    "### **Conjugate Gradient Algorithm**\n",
    "\n",
    "The steps of the Conjugate Gradient algorithm are as follows:\n",
    "\n",
    "1. **Initialize**:\n",
    "   - Choose an initial guess $ x_0 $.\n",
    "   - Compute the residual $ r_0 = b - A x_0 $.\n",
    "   - Set the initial search direction $ p_0 = r_0 $.\n",
    "\n",
    "2. **Iterate**:\n",
    "   For $ k = 0, 1, 2, \\dots $:\n",
    "   - Compute the step size $ \\alpha_k $:\n",
    "     $\n",
    "     \\alpha_k = \\frac{r_k^T r_k}{p_k^T A p_k}\n",
    "     $\n",
    "   - Update the solution $ x_{k+1} $:\n",
    "     $\n",
    "     x_{k+1} = x_k + \\alpha_k p_k\n",
    "     $\n",
    "   - Update the residual $ r_{k+1} $:\n",
    "     $\n",
    "     r_{k+1} = r_k - \\alpha_k A p_k\n",
    "     $\n",
    "   - Check for convergence (e.g., $ \\|r_{k+1}\\| < \\text{tolerance} $).\n",
    "   - Compute the new search direction $ p_{k+1} $:\n",
    "     $\n",
    "     p_{k+1} = r_{k+1} + \\beta_k p_k\n",
    "     $\n",
    "     where:\n",
    "     $\n",
    "     \\beta_k = \\frac{r_{k+1}^T r_{k+1}}{r_k^T r_k}\n",
    "     $\n",
    "\n",
    "3. **Terminate**:\n",
    "   - Stop when the residual $ r_{k+1} $ is sufficiently small or a maximum number of iterations is reached.\n",
    "\n",
    "---\n",
    "1. **Convergence**:\n",
    "   - The Conjugate Gradient method converges in **2 iterations** for this small system.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Points**\n",
    "\n",
    "1. **Symmetric Positive-Definite Matrix**:\n",
    "   - The Conjugate Gradient method requires $ A $ to be symmetric and positive-definite.\n",
    "\n",
    "2. **Efficiency**:\n",
    "   - The method is particularly efficient for large, sparse systems because it avoids matrix factorization and uses only matrix-vector products.\n",
    "\n",
    "3. **Convergence**:\n",
    "   - The method converges in at most $ n $ iterations for an $ n \\times n $ system (in exact arithmetic).\n",
    "\n",
    "4. **Applications**:\n",
    "   - Solving large linear systems in numerical simulations, optimization, and machine learning.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def conjugateGradient(matrix, rightHandVector, guess=None, tolerance=1e-10, iterations=None):\n",
    "\n",
    "    n = len(rightHandVector)\n",
    "    if guess is None:\n",
    "        guess = np.zeros_like(rightHandVector)\n",
    "    if iterations is None:\n",
    "        iterations = n\n",
    "\n",
    "    resultVector = guess\n",
    "    residual = rightHandVector - matrix @ resultVector\n",
    "    direction = residual\n",
    "    previousResidual = residual.T @ residual\n",
    "\n",
    "    for numIterations in range(iterations):\n",
    "        Ap = matrix @ direction\n",
    "        alpha = previousResidual / (direction.T @ Ap)\n",
    "        resultVector = resultVector + alpha * direction\n",
    "        residual = residual - alpha * Ap\n",
    "        newResidual = residual.T @ residual\n",
    "\n",
    "        if np.sqrt(newResidual) < tolerance:\n",
    "            break\n",
    "\n",
    "        beta = newResidual / previousResidual\n",
    "        direction = residual + beta * direction\n",
    "        previousResidual = newResidual\n",
    "\n",
    "    return resultVector, numIterations + 1\n",
    "\n",
    "\n",
    "# Example symmetric positive-definite matrix\n",
    "matrix = np.array([\n",
    "    [4, 1],\n",
    "    [1, 3]\n",
    "])\n",
    "\n",
    "# Right-hand side vector\n",
    "b = np.array([1, 2])\n",
    "\n",
    "# Solve the system\n",
    "x, iterNum = conjugateGradient(matrix, b)\n",
    "\n",
    "print(\"Conjugate Gradient:\\n\", x)\n",
    "print(\"Number of iterations:\", iterNum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **Stability Criteria Based on Eigenvalues**\n",
    "\n",
    "For a system described by the state-space equation:\n",
    "\n",
    "$\n",
    "\\dot{x}(t) = A x(t)\n",
    "$\n",
    "\n",
    "where:\n",
    "- $ x(t) $ is the state vector,\n",
    "- $ A $ is the system matrix,\n",
    "\n",
    "the **stability of the system** is determined by the eigenvalues of $ A $:\n",
    "\n",
    "1. **Asymptotically Stable**:\n",
    "   - All eigenvalues of $ A $ have **negative real parts**.\n",
    "   - The system will converge to the equilibrium point (origin) over time.\n",
    "\n",
    "2. **Unstable**:\n",
    "   - At least one eigenvalue of $ A $ has a **positive real part**.\n",
    "   - The system will diverge from the equilibrium point over time.\n",
    "\n",
    "3. **Marginally Stable**:\n",
    "   - All eigenvalues of $ A $ have **non-positive real parts**, and at least one eigenvalue has a **zero real part** (with no repeated eigenvalues on the imaginary axis).\n",
    "   - The system will neither converge nor diverge but may oscillate or remain bounded.\n",
    "\n",
    "---\n",
    "\n",
    "### **Steps to Verify Stability**\n",
    "\n",
    "1. **Compute the Eigenvalues**:\n",
    "   - Solve the characteristic equation:\n",
    "     $\n",
    "     \\det(A - \\lambda I) = 0\n",
    "     $\n",
    "     where $ \\lambda $ represents the eigenvalues and $ I $ is the identity matrix.\n",
    "\n",
    "2. **Analyze the Eigenvalues**:\n",
    "   - Check the real parts of the eigenvalues:\n",
    "     - If all real parts are negative, the system is **asymptotically stable**.\n",
    "     - If any real part is positive, the system is **unstable**.\n",
    "     - If all real parts are non-positive and at least one is zero, the system is **marginally stable**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Points**\n",
    "\n",
    "1. **Eigenvalues and Stability**:\n",
    "   - The real parts of the eigenvalues determine the stability of the system.\n",
    "   - Negative real parts indicate stability, positive real parts indicate instability, and zero real parts indicate marginal stability.\n",
    "\n",
    "2. **Complex Eigenvalues**:\n",
    "   - For complex eigenvalues, only the real parts matter for stability analysis.\n",
    "\n",
    "3. **Applications**:\n",
    "   - Stability analysis is crucial in control systems, dynamical systems, and differential equations.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  verify the stability of a system checking eigenvalues of it's matrix\n",
    "\n",
    "def checkStability(matrix):\n",
    "\n",
    "    eigenvalues = np.linalg.eigvals(matrix)\n",
    "\n",
    "    # Analyze the eigenvalues\n",
    "    real_parts = np.real(eigenvalues)\n",
    "\n",
    "    if all(real_parts < 0):\n",
    "        return \"Asymptotically Stable\"\n",
    "    elif any(real_parts > 0):\n",
    "        return \"Unstable\"\n",
    "    else:\n",
    "        return \"Marginally Stable\"\n",
    "    \n",
    "\n",
    "\n",
    "# Example system matrix\n",
    "A = np.array([\n",
    "    [-2, 1],\n",
    "    [0, -3]\n",
    "])\n",
    "\n",
    "# Check stability\n",
    "stability = checkStability(A)\n",
    "print(\"System Stability:\", stability)\n",
    "# Example system matrix\n",
    "A = np.array([\n",
    "    [0, 1],\n",
    "    [-1, 0]\n",
    "])\n",
    "\n",
    "# Check stability\n",
    "stability = checkStability(A)\n",
    "print(\"System Stability:\", stability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Markov Process**\n",
    "\n",
    "Simulating a **Markov process** using a **transition matrix** involves modeling a system that transitions between states based on probabilities defined by the matrix. A Markov process is a stochastic process where the future state depends only on the current state and not on the sequence of events that preceded it.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Concepts**\n",
    "\n",
    "1. **Transition Matrix**:\n",
    "   - Let $ P $ be a square matrix of size $ n \\times n $, where $ n $ is the number of states.\n",
    "   - Each entry $ P_{ij} $ represents the probability of transitioning from state $ i $ to state $ j $.\n",
    "   - The rows of $ P $ must sum to 1 (i.e., $ \\sum_{j=1}^n P_{ij} = 1 $ for all $ i $).\n",
    "\n",
    "2. **State Vector**:\n",
    "   - Let $ x_t $ be a vector of size $ n $, where each entry $ x_{t,i} $ represents the probability of being in state $ i $ at time $ t $.\n",
    "   - The initial state vector $ x_0 $ represents the starting probabilities.\n",
    "\n",
    "3. **Simulation**:\n",
    "   - At each time step, the state vector is updated using the transition matrix:\n",
    "     $\n",
    "     x_{t+1} = x_t \\cdot P\n",
    "     $\n",
    "   - Alternatively, you can simulate individual state transitions using random sampling.\n",
    "\n",
    "---\n",
    "\n",
    "### **Steps to Simulate a Markov Process**\n",
    "\n",
    "1. **Define the Transition Matrix**:\n",
    "   - Specify the transition probabilities $ P $.\n",
    "\n",
    "2. **Define the Initial State**:\n",
    "   - Specify the initial state vector $ x_0 $.\n",
    "\n",
    "3. **Simulate the Process**:\n",
    "   - For each time step, update the state vector or simulate individual transitions.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Points**\n",
    "\n",
    "1. **Applications**:\n",
    "   - Markov processes are used in modeling systems like weather, finance, biology, and more.\n",
    "\n",
    "2. **Steady-State Distribution**:\n",
    "   - Over time, the state vector may converge to a steady-state distribution, which can be found by solving:\n",
    "     $\n",
    "     x = x \\cdot P\n",
    "     $\n",
    "\n",
    "3. **Extensions**:\n",
    "   - For more complex systems, you can use **hidden Markov models (HMMs)** or **continuous-time Markov processes**.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulating Markov Process\n",
    "\n",
    "def simulateMarkov(matrix, initialVector, steps):\n",
    "    \"\"\"\n",
    "    Simulate a Markov process using a transition matrix.\n",
    "\n",
    "    Parameters:\n",
    "        P: Transition matrix (n x n)\n",
    "        x0: Initial state vector (n,)\n",
    "        num_steps: Number of time steps to simulate\n",
    "\n",
    "    Returns:\n",
    "        states: List of state vectors at each time step\n",
    "    \"\"\"\n",
    "    states = [initialVector]\n",
    "    for _ in range(steps):\n",
    "        nextVector = states[-1] @ matrix  # Update state vector\n",
    "        states.append(nextVector)\n",
    "    return states\n",
    "\n",
    "def simulateMarkovStates(matrix, initialStateIDX, iterations):\n",
    "    \"\"\"\n",
    "    Simulate a Markov process by tracking individual state transitions.\n",
    "\n",
    "    Parameters:\n",
    "        P: Transition matrix (n x n)\n",
    "        initial_state: Initial state (index)\n",
    "        num_steps: Number of time steps to simulate\n",
    "\n",
    "    Returns:\n",
    "        state_sequence: List of states at each time step\n",
    "    \"\"\"\n",
    "    n = matrix.shape[0]  # Number of states\n",
    "    stateSequence = [initialStateIDX]\n",
    "    for _ in range(iterations):\n",
    "        currentState = stateSequence[-1]\n",
    "        nextState = np.random.choice(n, p=matrix[currentState])  # Sample next state\n",
    "        stateSequence.append(nextState)\n",
    "    return stateSequence\n",
    "\n",
    "\n",
    "\n",
    "# Transition matrix (3 states)\n",
    "P = np.array([\n",
    "    [0.9, 0.1, 0.0],  # From state 0\n",
    "    [0.2, 0.7, 0.1],  # From state 1\n",
    "    [0.0, 0.3, 0.7]   # From state 2\n",
    "])\n",
    "\n",
    "# Initial state vector (start in state 0)\n",
    "x0 = np.array([1.0, 0.0, 0.0])\n",
    "\n",
    "# Simulate using state vector evolution\n",
    "iterationSteps = 10\n",
    "states = simulateMarkov(P, x0, iterationSteps)\n",
    "\n",
    "print(\"State vectors over time:\")\n",
    "for t, x in enumerate(states):\n",
    "    print(f\"Step {t}: {x}\")\n",
    "\n",
    "# Simulate using individual state transitions\n",
    "initial_state = 0  # Start in state 0\n",
    "state_sequence = simulateMarkovStates(P, initial_state, iterationSteps)\n",
    "\n",
    "print(\"\\nState sequence over time:\")\n",
    "print(state_sequence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
