{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics & Probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **Basic Descriptive Statistics**\n",
    "1. Write a function to compute the mean of a dataset.  \n",
    "2. Compute the median of a dataset and verify its robustness to outliers.  \n",
    "3. Calculate the mode of a dataset.  \n",
    "4. Implement a function to compute the variance and standard deviation of a dataset.  \n",
    "5. Write a function to calculate the interquartile range (IQR) of a dataset.  \n",
    "6. Detect outliers in a dataset using the IQR method.  \n",
    "7. Compute the correlation coefficient between two variables.  \n",
    "8. Visualize a dataset using a histogram and calculate skewness.  \n",
    "9. Write a function to compute the covariance matrix of a multivariate dataset.  \n",
    "10. Standardize a dataset to have a mean of 0 and a variance of 1.\n",
    "\n",
    "---\n",
    "\n",
    "### **Probability Basics**\n",
    "11. Simulate rolling a fair six-sided die and compute probabilities of outcomes.  \n",
    "12. Implement a function to calculate the probability of an event using relative frequency.  \n",
    "13. Simulate flipping a biased coin and estimate its probability of heads.  \n",
    "14. Write a function to calculate the complement of an event.  \n",
    "15. Use Python to compute conditional probability $ P(A|B) $ given sample data.  \n",
    "16. Verify the Law of Total Probability using simulated data.  \n",
    "17. Simulate and calculate the probability of drawing specific cards from a deck.  \n",
    "18. Compute joint probabilities for two events using Python.  \n",
    "19. Verify Bayesâ€™ Theorem with a real-world example.  \n",
    "20. Visualize a probability distribution (e.g., uniform, normal).\n",
    "\n",
    "---\n",
    "\n",
    "### **Discrete and Continuous Random Variables**\n",
    "21. Generate a binomial random variable and compute its mean and variance.  \n",
    "22. Simulate a Poisson process and calculate probabilities of specific events.  \n",
    "23. Implement and visualize the probability mass function (PMF) of a discrete random variable.  \n",
    "24. Generate a normal random variable and compute probabilities for specific intervals.  \n",
    "25. Compute the cumulative distribution function (CDF) of a normal distribution.  \n",
    "26. Use the Central Limit Theorem to approximate the sum of random variables.  \n",
    "27. Implement and visualize the probability density function (PDF) of a normal distribution.  \n",
    "28. Simulate and compute probabilities for an exponential distribution.  \n",
    "29. Fit a normal distribution to a given dataset and estimate its parameters.  \n",
    "30. Compare the behavior of discrete vs. continuous random variables.\n",
    "\n",
    "---\n",
    "\n",
    "### **Sampling and Estimation**\n",
    "31. Implement simple random sampling on a dataset.  \n",
    "32. Write a function to compute the sample mean and sample variance.  \n",
    "33. Simulate and analyze sampling distributions of the mean.  \n",
    "34. Perform stratified sampling on a dataset.  \n",
    "35. Estimate population parameters using maximum likelihood estimation (MLE).  \n",
    "36. Simulate bootstrap sampling to compute confidence intervals.  \n",
    "37. Compute the bias and variance of an estimator using simulation.  \n",
    "38. Use Python to verify the Law of Large Numbers.  \n",
    "39. Simulate and compute the impact of sample size on estimation accuracy.  \n",
    "40. Write a function to calculate standard error for a sample mean.\n",
    "\n",
    "---\n",
    "\n",
    "### **Hypothesis Testing**\n",
    "41. Perform a one-sample $ t $-test to check if a sample mean differs from a known value.  \n",
    "42. Perform a two-sample $ t $-test to compare the means of two datasets.  \n",
    "43. Implement and interpret a chi-square test for independence.  \n",
    "44. Conduct an ANOVA test to compare means of multiple groups.  \n",
    "45. Perform a permutation test for a given hypothesis.  \n",
    "46. Implement and interpret a Mann-Whitney U test for non-parametric data.  \n",
    "47. Simulate Type I and Type II errors for hypothesis tests.  \n",
    "48. Write a function to compute p-values from test statistics.  \n",
    "49. Visualize the rejection region of a hypothesis test.  \n",
    "50. Perform a hypothesis test to determine if a dataset follows a normal distribution.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics as stats\n",
    "import scipy.stats as sciStats\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -- Basic Descriptive --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the mean of dataset\n",
    "\n",
    "data = np.array([12,4,5,6,7,8,2323])\n",
    "print(np.mean(data))\n",
    "print(stats.mean(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the median of data set and verify robustness of outliers\n",
    "\n",
    "# The median is considered robust to outliers because it is based on the middle value(s) of the dataset, not the actual values of all data points. This means that extreme values (outliers) have little to no effect on the median.\n",
    "\n",
    "data = [1, 2, 3, 4, 100]\n",
    "\n",
    "print(np.median(data))\n",
    "print(stats.median(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mode of dataset\n",
    "\n",
    "data = np.array([12,4,54,5,6,67,8,341])\n",
    "\n",
    "print(stats.mode(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the variance and standard deviation\n",
    "\n",
    "data = [2, 4, 6, 8, 10]\n",
    "\n",
    "# Population variance and standard deviation\n",
    "populationVariance = stats.pvariance(data)\n",
    "populationStandardDeviation = stats.pstdev(data)\n",
    "\n",
    "# Sample variance and standard deviation\n",
    "sampleVariance = stats.variance(data)\n",
    "sampleStandardDeviation = stats.stdev(data)\n",
    "\n",
    "print(\"Population Variance:\", populationVariance)\n",
    "print(\"Population Standard Deviation:\", populationStandardDeviation)\n",
    "print(\"Sample Variance:\", sampleVariance)\n",
    "print(\"Sample Standard Deviation:\", sampleStandardDeviation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Interquartile Range (IQR)** is a measure of statistical dispersion that represents the range between the first quartile (Q1) and the third quartile (Q3). It is used to describe the middle 50% of the data and is robust to outliers.\n",
    "\n",
    "---\n",
    "\n",
    "### Steps to Calculate the Inter-quartile Range (IQR):\n",
    "1. **Sort the dataset** in ascending order.\n",
    "2. **Find the median** of the dataset. This is the **second quartile (Q2)**.\n",
    "3. **Find the first quartile (Q1)**:\n",
    "   - This is the median of the lower half of the data (values below Q2).\n",
    "4. **Find the third quartile (Q3)**:\n",
    "   - This is the median of the upper half of the data (values above Q2).\n",
    "5. **Calculate the IQR**:\n",
    "   $\n",
    "   \\text{IQR} = Q3 - Q1\n",
    "   $\n",
    "\n",
    "---\n",
    "\n",
    "### Example:\n",
    "Dataset: $\\{3, 7, 8, 5, 12, 14, 21, 13, 18$$\n",
    "\n",
    "1. **Sort the data**:\n",
    "   $\n",
    "   \\{3, 5, 7, 8, 12, 13, 14, 18, 21$\n",
    "   $\n",
    "\n",
    "2. **Find the median (Q2)**:\n",
    "   - There are 9 values, so the median is the 5th value:  \n",
    "   $\n",
    "   Q2 = 12\n",
    "   $\n",
    "\n",
    "3. **Find the first quartile (Q1)**:\n",
    "   - Lower half of the data: $\\{3, 5, 7, 8$$  \n",
    "   - Median of the lower half:  \n",
    "     $\n",
    "     Q1 = \\frac{5 + 7}{2} = 6\n",
    "     $\n",
    "\n",
    "4. **Find the third quartile (Q3)**:\n",
    "   - Upper half of the data: $\\{13, 14, 18, 21$$  \n",
    "   - Median of the upper half:  \n",
    "     $\n",
    "     Q3 = \\frac{14 + 18}{2} = 16\n",
    "     $\n",
    "\n",
    "5. **Calculate the IQR**:\n",
    "   $\n",
    "   \\text{IQR} = Q3 - Q1 = 16 - 6 = 10\n",
    "   $\n",
    "\n",
    "So, the inter-quartile range is **10**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the inter-quartile range of data set\n",
    "\n",
    "data = [3, 7, 8, 5, 12, 14, 21, 13, 18]\n",
    "\n",
    "# Calculate Q1 (25th percentile) and Q3 (75th percentile)\n",
    "Q1 = np.percentile(data, 25)\n",
    "Q3 = np.percentile(data, 75)\n",
    "\n",
    "# Calculate IQR\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "print(\"Q1:\", Q1)\n",
    "print(\"Q3:\", Q3)\n",
    "print(\"IQR:\", IQR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect Outliers using IQR\n",
    "# The IQR measures the spread of the middle 50% of the data, and outliers are defined as data points that fall significantly below or above the \"fences\" calculated using the IQR.\n",
    "\n",
    "\n",
    "\n",
    "data = [1, 3, 5, 7, 9, 11, 13, 15, 17, 50]\n",
    "\n",
    "# Calculate Q1, Q3, and IQR\n",
    "Q1 = np.percentile(data, 25)\n",
    "Q3 = np.percentile(data, 75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define fences\n",
    "lowerFence = Q1 - 1.5 * IQR\n",
    "upperFence = Q3 + 1.5 * IQR\n",
    "\n",
    "# Detect outliers\n",
    "outliers = [x for x in data if x < lowerFence or x > upperFence]\n",
    "\n",
    "print(\"Q1:\", Q1)\n",
    "print(\"Q3:\", Q3)\n",
    "print(\"IQR:\", IQR)\n",
    "print(\"Lower Fence:\", lowerFence)\n",
    "print(\"Upper Fence:\", upperFence)\n",
    "print(\"Outliers:\", outliers)\n",
    "\n",
    "# The IQR method is robust for detecting outliers in skewed datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **correlation coefficient** measures the strength and direction of the linear relationship between two variables. The most common measure is the **Pearson correlation coefficient**, which ranges from **-1 to 1**:\n",
    "- **1**: Perfect positive linear relationship,\n",
    "- **-1**: Perfect negative linear relationship,\n",
    "- **0**: No linear relationship.\n",
    "\n",
    "---\n",
    "\n",
    "### Formula for Pearson Correlation Coefficient:\n",
    "The Pearson correlation coefficient ($ r $) between two variables $ X $ and $ Y $ is calculated as:\n",
    "\n",
    "$\n",
    "r = \\frac{\\sum{(x_i - \\bar{x})(y_i - \\bar{y})}}{\\sqrt{\\sum{(x_i - \\bar{x})^2} \\sum{(y_i - \\bar{y})^2}}}\n",
    "$\n",
    "\n",
    "Where:\n",
    "- $ x_i $ and $ y_i $ are individual data points,\n",
    "- $ \\bar{x} $ and $ \\bar{y} $ are the means of $ X $ and $ Y $, respectively.\n",
    "\n",
    "---\n",
    "\n",
    "### Steps to Compute the Correlation Coefficient:\n",
    "1. **Calculate the mean** of $ X $ ($ \\bar{x} $) and the mean of $ Y $ ($ \\bar{y} $).\n",
    "2. **Compute the deviations** from the mean for each data point:\n",
    "   - $ (x_i - \\bar{x}) $ and $ (y_i - \\bar{y}) $.\n",
    "3. **Multiply the deviations** for each pair of data points:\n",
    "   - $ (x_i - \\bar{x})(y_i - \\bar{y}) $.\n",
    "4. **Sum the products** of deviations:\n",
    "   - $ \\sum{(x_i - \\bar{x})(y_i - \\bar{y})} $.\n",
    "5. **Square the deviations** for $ X $ and $ Y $, then sum them:\n",
    "   - $ \\sum{(x_i - \\bar{x})^2} $ and $ \\sum{(y_i - \\bar{y})^2} $.\n",
    "6. **Divide the sum of products** by the square root of the product of the sums of squared deviations:\n",
    "   - $ r = \\frac{\\sum{(x_i - \\bar{x})(y_i - \\bar{y})}}{\\sqrt{\\sum{(x_i - \\bar{x})^2} \\sum{(y_i - \\bar{y})^2}}} $.\n",
    "\n",
    "---\n",
    "\n",
    "### Example:\n",
    "Letâ€™s calculate the correlation coefficient between $ X $ and $ Y $:\n",
    "\n",
    "| $ X $ | $ Y $ |\n",
    "|--------|--------|\n",
    "| 1      | 2      |\n",
    "| 2      | 4      |\n",
    "| 3      | 5      |\n",
    "| 4      | 4      |\n",
    "| 5      | 5      |\n",
    "\n",
    "1. **Calculate the means**:\n",
    "   - $ \\bar{x} = \\frac{1 + 2 + 3 + 4 + 5}{5} = 3 $,\n",
    "   - $ \\bar{y} = \\frac{2 + 4 + 5 + 4 + 5}{5} = 4 $.\n",
    "\n",
    "2. **Compute deviations and their products**:\n",
    "\n",
    "   | $ X $ | $ Y $ | $ x_i - \\bar{x} $ | $ y_i - \\bar{y} $ | $ (x_i - \\bar{x})(y_i - \\bar{y}) $ | $ (x_i - \\bar{x})^2 $ | $ (y_i - \\bar{y})^2 $ |\n",
    "   |--------|--------|---------------------|---------------------|--------------------------------------|-------------------------|-------------------------|\n",
    "   | 1      | 2      | -2                  | -2                  | 4                                    | 4                       | 4                       |\n",
    "   | 2      | 4      | -1                  | 0                   | 0                                    | 1                       | 0                       |\n",
    "   | 3      | 5      | 0                   | 1                   | 0                                    | 0                       | 1                       |\n",
    "   | 4      | 4      | 1                   | 0                   | 0                                    | 1                       | 0                       |\n",
    "   | 5      | 5      | 2                   | 1                   | 2                                    | 4                       | 1                       |\n",
    "\n",
    "3. **Sum the columns**:\n",
    "   - $ \\sum{(x_i - \\bar{x})(y_i - \\bar{y})} = 4 + 0 + 0 + 0 + 2 = 6 $,\n",
    "   - $ \\sum{(x_i - \\bar{x})^2} = 4 + 1 + 0 + 1 + 4 = 10 $,\n",
    "   - $ \\sum{(y_i - \\bar{y})^2} = 4 + 0 + 1 + 0 + 1 = 6 $.\n",
    "\n",
    "4. **Calculate the correlation coefficient**:\n",
    "   $\n",
    "   r = \\frac{6}{\\sqrt{10 \\times 6}} = \\frac{6}{\\sqrt{60}} = \\frac{6}{7.746} \\approx 0.775\n",
    "   $\n",
    "\n",
    "So, the correlation coefficient is approximately **0.775**, indicating a **strong positive linear relationship**.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### Key Takeaways:\n",
    "- The correlation coefficient ($ r $) measures the **linear relationship** between two variables.\n",
    "- It ranges from **-1 to 1**, where:\n",
    "  - $ r = 1 $: Perfect positive correlation,\n",
    "  - $ r = -1 $: Perfect negative correlation,\n",
    "  - $ r = 0 $: No correlation.\n",
    "- Use Python libraries like `numpy`, `pandas`, or `scipy` for quick calculations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation coefficient between two variables \n",
    "X = [1, 2, 3, 4, 5]\n",
    "Y = [2, 4, 5, 4, 5]\n",
    "\n",
    "r = np.corrcoef(X, Y)[0, 1]\n",
    "print(\"Correlation Coefficient (r):\", r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **covariance matrix** is a square matrix that summarizes the variances and covariances of a **multivariate dataset**. It is a key concept in statistics and machine learning, especially in dimensionality reduction techniques like Principal Component Analysis (PCA).\n",
    "\n",
    "---\n",
    "\n",
    "### What is a Covariance Matrix?\n",
    "For a dataset with $ n $ variables (features), the covariance matrix is an $ n \\times n $ matrix where:\n",
    "- The **diagonal elements** represent the **variances** of each variable.\n",
    "- The **off-diagonal elements** represent the **covariances** between pairs of variables.\n",
    "\n",
    "---\n",
    "\n",
    "### Formula for Covariance Matrix:\n",
    "Given a dataset with $ p $ variables and $ n $ observations, the covariance matrix $ \\Sigma $ is calculated as:\n",
    "\n",
    "$\n",
    "\\Sigma = \\frac{1}{n-1} \\cdot (X - \\bar{X})^T (X - \\bar{X})\n",
    "$\n",
    "\n",
    "Where:\n",
    "- $ X $ is the $ n \\times p $ data matrix (each row is an observation, each column is a variable),\n",
    "- $ \\bar{X} $ is the $ 1 \\times p $ vector of means for each variable,\n",
    "- $ (X - \\bar{X}) $ is the mean-centered data matrix,\n",
    "- $ ^T $ denotes the transpose of a matrix.\n",
    "\n",
    "---\n",
    "\n",
    "### Steps to Compute the Covariance Matrix:\n",
    "1. **Center the data** by subtracting the mean of each variable.\n",
    "2. **Compute the product** of the centered data matrix and its transpose.\n",
    "3. **Divide by $ n-1 $** (for sample covariance) or $ n $ (for population covariance).\n",
    "\n",
    "---\n",
    "\n",
    "### Example:\n",
    "Consider a dataset with 3 variables ($ X_1, X_2, X_3 $) and 4 observations:\n",
    "\n",
    "| $ X_1 $ | $ X_2 $ | $ X_3 $ |\n",
    "|----------|----------|----------|\n",
    "| 1        | 2        | 3        |\n",
    "| 4        | 5        | 6        |\n",
    "| 7        | 8        | 9        |\n",
    "| 10       | 11       | 12       |\n",
    "\n",
    "1. **Compute the mean of each variable**:\n",
    "   - $ \\bar{X_1} = \\frac{1 + 4 + 7 + 10}{4} = 5.5 $,\n",
    "   - $ \\bar{X_2} = \\frac{2 + 5 + 8 + 11}{4} = 6.5 $,\n",
    "   - $ \\bar{X_3} = \\frac{3 + 6 + 9 + 12}{4} = 7.5 $.\n",
    "\n",
    "2. **Center the data** by subtracting the means:\n",
    "\n",
    "   | $ X_1 - \\bar{X_1} $ | $ X_2 - \\bar{X_2} $ | $ X_3 - \\bar{X_3} $ |\n",
    "   |-----------------------|-----------------------|-----------------------|\n",
    "   | -4.5                  | -4.5                  | -4.5                  |\n",
    "   | -1.5                  | -1.5                  | -1.5                  |\n",
    "   | 1.5                   | 1.5                   | 1.5                   |\n",
    "   | 4.5                   | 4.5                   | 4.5                   |\n",
    "\n",
    "3. **Compute the product of the centered data matrix and its transpose**:\n",
    "   - Let $ A = X - \\bar{X} $. Then:\n",
    "     $\n",
    "     A^T A = \\begin{bmatrix}\n",
    "     -4.5 & -1.5 & 1.5 & 4.5 \\\\\n",
    "     -4.5 & -1.5 & 1.5 & 4.5 \\\\\n",
    "     -4.5 & -1.5 & 1.5 & 4.5\n",
    "     \\end{bmatrix}\n",
    "     \\begin{bmatrix}\n",
    "     -4.5 & -4.5 & -4.5 \\\\\n",
    "     -1.5 & -1.5 & -1.5 \\\\\n",
    "     1.5 & 1.5 & 1.5 \\\\\n",
    "     4.5 & 4.5 & 4.5\n",
    "     \\end{bmatrix}\n",
    "     $\n",
    "   - The result is a $ 3 \\times 3 $ matrix.\n",
    "\n",
    "4. **Divide by $ n-1 $** (for sample covariance):\n",
    "   $\n",
    "   \\Sigma = \\frac{1}{4-1} \\cdot A^T A\n",
    "   $\n",
    "\n",
    "---\n",
    "\n",
    "### Key Takeaways:\n",
    "- The **covariance matrix** summarizes the relationships between variables in a multivariate dataset.\n",
    "- Diagonal elements represent **variances**, and off-diagonal elements represent **covariances**.\n",
    "- Use Python libraries like `numpy` or `pandas` for efficient computation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the covariance matrix of a multivariate dataset\n",
    "\n",
    "# Define the dataset\n",
    "X = np.array([\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6],\n",
    "    [7, 8, 9],\n",
    "    [10, 11, 12]\n",
    "])\n",
    "\n",
    "# Compute the covariance matrix\n",
    "covarianceMatrix = np.cov(X, rowvar=False)  # rowvar=False means columns are variables\n",
    "print(\"Covariance Matrix:\")\n",
    "print(covarianceMatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardizing a dataset to have a **mean of 0** and a **variance of 1** is a common preprocessing step in data analysis and machine learning. This process is also known as **z-score normalization**. Standardization ensures that all features are on the same scale, which is particularly important for algorithms that are sensitive to the magnitude of features (e.g., PCA, k-means, SVM).\n",
    "\n",
    "---\n",
    "\n",
    "### Steps to Standardize a Dataset:\n",
    "1. **Compute the mean** of each feature (column) in the dataset.\n",
    "2. **Compute the standard deviation** of each feature.\n",
    "3. **Standardize each value** using the formula:\n",
    "   $\n",
    "   z = \\frac{x - \\mu}{\\sigma}\n",
    "   $\n",
    "   Where:\n",
    "   - $ x $ is the original value,\n",
    "   - $ \\mu $ is the mean of the feature,\n",
    "   - $ \\sigma $ is the standard deviation of the feature.\n",
    "\n",
    "---\n",
    "\n",
    "### Example:\n",
    "Consider the following dataset with 2 features ($ X_1 $ and $ X_2 $):\n",
    "\n",
    "| $ X_1 $ | $ X_2 $ |\n",
    "|----------|----------|\n",
    "| 1        | 2        |\n",
    "| 2        | 3        |\n",
    "| 3        | 4        |\n",
    "| 4        | 5        |\n",
    "\n",
    "1. **Compute the mean** of each feature:\n",
    "   - $ \\mu_{X_1} = \\frac{1 + 2 + 3 + 4}{4} = 2.5 $,\n",
    "   - $ \\mu_{X_2} = \\frac{2 + 3 + 4 + 5}{4} = 3.5 $.\n",
    "\n",
    "2. **Compute the standard deviation** of each feature:\n",
    "   - For $ X_1 $:\n",
    "     $\n",
    "     \\sigma_{X_1} = \\sqrt{\\frac{(1-2.5)^2 + (2-2.5)^2 + (3-2.5)^2 + (4-2.5)^2}{4}} = \\sqrt{\\frac{2.25 + 0.25 + 0.25 + 2.25}{4}} = \\sqrt{1.25} \\approx 1.118\n",
    "     $\n",
    "   - For $ X_2 $:\n",
    "     $\n",
    "     \\sigma_{X_2} = \\sqrt{\\frac{(2-3.5)^2 + (3-3.5)^2 + (4-3.5)^2 + (5-3.5)^2}{4}} = \\sqrt{\\frac{2.25 + 0.25 + 0.25 + 2.25}{4}} = \\sqrt{1.25} \\approx 1.118\n",
    "     $\n",
    "\n",
    "3. **Standardize each value**:\n",
    "   - For $ X_1 $:\n",
    "     $\n",
    "     z_{X_1} = \\frac{x - \\mu_{X_1}}{\\sigma_{X_1}}\n",
    "     $\n",
    "   - For $ X_2 $:\n",
    "     $\n",
    "     z_{X_2} = \\frac{x - \\mu_{X_2}}{\\sigma_{X_2}}\n",
    "     $\n",
    "\n",
    "   Applying this to each value:\n",
    "\n",
    "   | $ X_1 $ | $ X_2 $ | $ z_{X_1} $ | $ z_{X_2} $ |\n",
    "   |----------|----------|---------------|---------------|\n",
    "   | 1        | 2        | $\\frac{1-2.5}{1.118} \\approx -1.34$ | $\\frac{2-3.5}{1.118} \\approx -1.34$ |\n",
    "   | 2        | 3        | $\\frac{2-2.5}{1.118} \\approx -0.45$ | $\\frac{3-3.5}{1.118} \\approx -0.45$ |\n",
    "   | 3        | 4        | $\\frac{3-2.5}{1.118} \\approx 0.45$ | $\\frac{4-3.5}{1.118} \\approx 0.45$ |\n",
    "   | 4        | 5        | $\\frac{4-2.5}{1.118} \\approx 1.34$ | $\\frac{5-3.5}{1.118} \\approx 1.34$ |\n",
    "\n",
    "   The standardized dataset is:\n",
    "\n",
    "   | $ z_{X_1} $ | $ z_{X_2} $ |\n",
    "   |---------------|---------------|\n",
    "   | -1.34         | -1.34         |\n",
    "   | -0.45         | -0.45         |\n",
    "   | 0.45          | 0.45          |\n",
    "   | 1.34          | 1.34          |\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### Key Takeaways:\n",
    "- Standardization transforms the data to have a **mean of 0** and a **standard deviation of 1**.\n",
    "- It is essential for algorithms that are sensitive to feature scales.\n",
    "- Use Python libraries like `scikit-learn` or `numpy` for efficient standardization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize a dataset to have a mean of 0 and variance of 1\n",
    "\n",
    "# Define the dataset\n",
    "X = np.array([\n",
    "    [1, 2],\n",
    "    [2, 3],\n",
    "    [3, 4],\n",
    "    [4, 5]\n",
    "])\n",
    "\n",
    "# Compute mean and standard deviation\n",
    "mean = np.mean(X, axis=0)\n",
    "std = np.std(X, axis=0)\n",
    "\n",
    "# Standardize the data\n",
    "xStandardized = (X - mean) / std\n",
    "\n",
    "print(\"Standardized Dataset:\")\n",
    "print(xStandardized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -- Probability Basics --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a rolling a fair six sided die and compute probabilities of  of outcomes\n",
    "\n",
    "\n",
    "def rollDice()-> int:\n",
    "    return random.randint(1, 6)\n",
    "def simulateRolls(rolls=10)-> list:\n",
    "    return [rollDice() for _ in range(rolls)]\n",
    "total: list = simulateRolls()\n",
    "print(\"TotalRolls:\", total)\n",
    "\n",
    "\n",
    "def largeRollSimulation(rolls=1000):\n",
    "    rolling: list = simulateRolls(rolls=rolls)\n",
    "    outcome: dict = Counter(rolling)\n",
    "    # Compute probabilities\n",
    "    probabilities: dict = {outcome: count / rolls for outcome, count in outcome.items()}\n",
    "    print(\"Outcome Counts:\", outcome)\n",
    "    print(\"Probabilities:\", probabilities)\n",
    "\n",
    "largeRollSimulation()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Formula for Relative Frequency:\n",
    "The probability of an event $ A $ using relative frequency is given by:\n",
    "\n",
    "$\n",
    "P(A) = \\frac{\\text{Number of times event } A \\text{ occurs}}{\\text{Total number of trials}}\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "### Steps to Calculate Relative Frequency:\n",
    "1. **Perform the experiment** or simulation multiple times (e.g., roll a die, flip a coin).\n",
    "2. **Count the number of times** the event of interest occurs.\n",
    "3. **Divide by the total number of trials** to get the relative frequency.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Key Takeaways:\n",
    "- Relative frequency is an empirical way to estimate probabilities based on observed data.\n",
    "- It is particularly useful when theoretical probabilities are unknown or difficult to compute.\n",
    "- Use Python to simulate experiments and calculate relative frequencies efficiently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Simulate rolling a die\n",
    "def rollDie():\n",
    "    return random.randint(1, 6)\n",
    "\n",
    "# Simulate multiple die rolls\n",
    "def simulateRolls(num_rolls):\n",
    "    return [rollDie() for _ in range(num_rolls)]\n",
    "\n",
    "# Number of trials\n",
    "numRolls = 1000\n",
    "rolls = simulateRolls(numRolls)\n",
    "\n",
    "# Count the number of times a 4 appears\n",
    "eventCount = rolls.count(3)\n",
    "\n",
    "# Calculate relative frequency\n",
    "relativeFrequency = eventCount / numRolls\n",
    "\n",
    "print(\"Number of times 4 appears:\", eventCount)\n",
    "print(\"Relative Frequency of rolling a 4:\", relativeFrequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate flipping a biased coin and estimate it's probability of heads\n",
    "\n",
    "# Define the bias (probability of heads)\n",
    "p = 0.7  # Example: 70% chance of heads\n",
    "\n",
    "# Simulate a single biased coin flip\n",
    "def biasedCoinFlip(p):\n",
    "    return \"Heads\" if random.random() < p else \"Tails\"\n",
    "\n",
    "# Simulate multiple biased coin flips\n",
    "def simulateFlips(nFlips, p):\n",
    "    return [biasedCoinFlip(p) for _ in range(nFlips)]\n",
    "\n",
    "def probabilityHead(nFlips:int=1000):\n",
    "\n",
    "# Number of flips\n",
    "    flips:list[str] = simulateFlips(nFlips, p)\n",
    "\n",
    "# Count the number of heads\n",
    "    headsCount:int = flips.count(\"Heads\")\n",
    "\n",
    "# Calculate the relative frequency of heads\n",
    "    relativeFrequencyHeads:float = headsCount / nFlips\n",
    "\n",
    "    print(\"Number of heads:\", headsCount)\n",
    "    print(\"Relative Frequency of heads:\", relativeFrequencyHeads)\n",
    "\n",
    "probabilityHead()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **complement of an event** is a fundamental concept in probability. The complement of an event $ A $, denoted as $ A^c $ or $ \\overline{A} $, represents all outcomes that are **not** in $ A $. The probability of the complement of an event is given by:\n",
    "\n",
    "$\n",
    "P(A^c) = 1 - P(A)\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "### Key Properties of Complements:\n",
    "1. **Mutually Exclusive**:\n",
    "   - An event $ A $ and its complement $ A^c $ cannot occur simultaneously.\n",
    "   - $ A \\cap A^c = \\emptyset $ (they are disjoint).\n",
    "\n",
    "2. **Exhaustive**:\n",
    "   - Either $ A $ or $ A^c $ must occur.\n",
    "   - $ A \\cup A^c = S $, where $ S $ is the sample space.\n",
    "\n",
    "3. **Probability**:\n",
    "   - The sum of the probabilities of an event and its complement is always 1:\n",
    "     $\n",
    "     P(A) + P(A^c) = 1\n",
    "     $\n",
    "\n",
    "---\n",
    "\n",
    "### Steps to Calculate the Complement of an Event:\n",
    "1. **Identify the event $ A $** and its probability $ P(A) $.\n",
    "2. **Use the complement formula**:\n",
    "   $\n",
    "   P(A^c) = 1 - P(A)\n",
    "   $\n",
    "\n",
    "---\n",
    "\n",
    "### Example 1: Simple Event\n",
    "Suppose you roll a fair six-sided die. Let $ A $ be the event of rolling a **6**. The probability of $ A $ is:\n",
    "$\n",
    "P(A) = \\frac{1}{6}\n",
    "$\n",
    "\n",
    "The complement $ A^c $ is the event of **not rolling a 6**. The probability of $ A^c $ is:\n",
    "$\n",
    "P(A^c) = 1 - P(A) = 1 - \\frac{1}{6} = \\frac{5}{6}\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "### Example 2: Compound Event\n",
    "Suppose you draw a card from a standard deck of 52 cards. Let $ A $ be the event of drawing a **heart**. The probability of $ A $ is:\n",
    "$\n",
    "P(A) = \\frac{13}{52} = \\frac{1}{4}\n",
    "$\n",
    "\n",
    "The complement $ A^c $ is the event of **not drawing a heart**. The probability of $ A^c $ is:\n",
    "$\n",
    "P(A^c) = 1 - P(A) = 1 - \\frac{1}{4} = \\frac{3}{4}\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "### Example 3: Real-World Scenario\n",
    "Suppose the probability of rain tomorrow is $ 0.3 $. Let $ A $ be the event of **rain tomorrow**. Then:\n",
    "$\n",
    "P(A) = 0.3\n",
    "$\n",
    "\n",
    "The complement $ A^c $ is the event of **no rain tomorrow**. The probability of $ A^c $ is:\n",
    "$\n",
    "P(A^c) = 1 - P(A) = 1 - 0.3 = 0.7\n",
    "$\n",
    "---\n",
    "\n",
    "### Key Takeaways:\n",
    "- The complement of an event $ A $ represents all outcomes **not** in $ A $.\n",
    "- The probability of the complement is $ P(A^c) = 1 - P(A) $.\n",
    "- Complements are useful for simplifying probability calculations, especially when itâ€™s easier to calculate $ P(A^c) $ than $ P(A) $.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the complement of an event\n",
    "\n",
    "def complementEvent():\n",
    "    # P = probability of rain\n",
    "    P:float = 0.6\n",
    "    pComplement = 1 - P\n",
    "    print(\"Probability of event A:\", P)\n",
    "    print(\"Probability of complement of event A:\", pComplement)\n",
    "\n",
    "complementEvent()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conditional probability** is the probability of an event $ A $ occurring given that another event $ B $ has already occurred. It is denoted as $ P(A|B) $ and is calculated using the formula:\n",
    "\n",
    "$\n",
    "P(A|B) = \\frac{P(A \\cap B)}{P(B)}\n",
    "$\n",
    "\n",
    "Where:\n",
    "- $ P(A \\cap B) $ is the probability of both $ A $ and $ B $ occurring,\n",
    "- $ P(B) $ is the probability of event $ B $.\n",
    "\n",
    "If you have **sample data**, you can estimate $ P(A|B) $ using relative frequencies.\n",
    "\n",
    "---\n",
    "\n",
    "### Steps to Compute Conditional Probability from Sample Data:\n",
    "1. **Identify the relevant events**:\n",
    "   - Let $ A $ and $ B $ be two events of interest.\n",
    "\n",
    "2. **Count the occurrences**:\n",
    "   - $ N $: Total number of observations in the sample.\n",
    "   - $ N_B $: Number of observations where event $ B $ occurs.\n",
    "   - $ N_{A \\cap B} $: Number of observations where both $ A $ and $ B $ occur.\n",
    "\n",
    "3. **Compute the probabilities**:\n",
    "   - $ P(B) = \\frac{N_B}{N} $,\n",
    "   - $ P(A \\cap B) = \\frac{N_{A \\cap B}}{N} $.\n",
    "\n",
    "4. **Calculate the conditional probability**:\n",
    "   $\n",
    "   P(A|B) = \\frac{P(A \\cap B)}{P(B)} = \\frac{N_{A \\cap B}}{N_B}\n",
    "   $\n",
    "\n",
    "---\n",
    "\n",
    "### Example:\n",
    "Suppose you have the following sample data for a survey of 100 people:\n",
    "\n",
    "| Event          | Number of People |\n",
    "|----------------|------------------|\n",
    "| $ B $: Smoker | 30               |\n",
    "| $ A \\cap B $: Smoker and has lung disease | 10 |\n",
    "\n",
    "Here:\n",
    "- $ N = 100 $ (total number of people),\n",
    "- $ N_B = 30 $ (number of smokers),\n",
    "- $ N_{A \\cap B} = 10 $ (number of smokers with lung disease).\n",
    "\n",
    "1. **Compute $ P(B) $**:\n",
    "   $\n",
    "   P(B) = \\frac{N_B}{N} = \\frac{30}{100} = 0.3\n",
    "   $\n",
    "\n",
    "2. **Compute $ P(A \\cap B) $**:\n",
    "   $\n",
    "   P(A \\cap B) = \\frac{N_{A \\cap B}}{N} = \\frac{10}{100} = 0.1\n",
    "   $\n",
    "\n",
    "3. **Compute $ P(A|B) $**:\n",
    "   $\n",
    "   P(A|B) = \\frac{P(A \\cap B)}{P(B)} = \\frac{0.1}{0.3} \\approx 0.333\n",
    "   $\n",
    "\n",
    "So, the probability of having lung disease given that a person is a smoker is approximately **0.333** (or 33.3%).\n",
    "\n",
    "---\n",
    "\n",
    "### Key Takeaways:\n",
    "- Conditional probability $ P(A|B) $ measures the likelihood of event $ A $ occurring given that event $ B $ has occurred.\n",
    "- It is calculated as $ P(A|B) = \\frac{P(A \\cap B)}{P(B)} $.\n",
    "- When working with sample data, you can estimate $ P(A|B) $ using relative frequencies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Conditional Probability P(A|B) given sample data\n",
    "\n",
    "def conditionalProbability(observations=1000, observationB= 20, observationAB= 10):\n",
    "    # P(B)\n",
    "    PB = observationB / observations\n",
    "    # A# Compute P(A âˆ© B)\n",
    "    PAB = observationAB / observations\n",
    "    # Compute P(A|B)\n",
    "    PGivenAB = PAB / PB\n",
    "    print(\"P(A|B):\", PGivenAB)\n",
    "    return PGivenAB\n",
    "\n",
    "conditionalProbability()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Law of Total Probability** is a fundamental rule in probability theory that allows you to calculate the total probability of an event by considering all possible scenarios or partitions of the sample space. It states:\n",
    "\n",
    "If $ B_1, B_2, \\dots, B_n $ are mutually exclusive and exhaustive events (i.e., they partition the sample space), then for any event $ A $:\n",
    "\n",
    "$\n",
    "P(A) = \\sum_{i=1}^n P(A|B_i) \\cdot P(B_i)\n",
    "$\n",
    "\n",
    "To **verify the Law of Total Probability using simulated data**, you can:\n",
    "1. Simulate data that follows a known distribution or process.\n",
    "2. Partition the data into mutually exclusive and exhaustive events $ B_1, B_2, \\dots, B_n $.\n",
    "3. Compute $ P(A|B_i) $ and $ P(B_i) $ for each partition.\n",
    "4. Verify that $ P(A) = \\sum_{i=1}^n P(A|B_i) \\cdot P(B_i) $.\n",
    "\n",
    "---\n",
    "\n",
    "### Example: Verifying the Law of Total Probability\n",
    "Letâ€™s say we have a biased coin and two dice:\n",
    "- The coin has a probability $ P(H) = 0.6 $ of landing heads and $ P(T) = 0.4 $ of landing tails.\n",
    "- If the coin lands heads, we roll a fair 6-sided die.\n",
    "- If the coin lands tails, we roll a biased 6-sided die where the probability of rolling a 6 is $ 0.5 $, and the other numbers are equally likely.\n",
    "\n",
    "We want to verify the Law of Total Probability for the event $ A $: **Rolling a 6**.\n",
    "\n",
    "---\n",
    "\n",
    "### Theoretical Calculation\n",
    "Using the Law of Total Probability:\n",
    "$\n",
    "P(A) = P(A|H) \\cdot P(H) + P(A|T) \\cdot P(T)\n",
    "$\n",
    "\n",
    "- $ P(H) = 0.6 $, $ P(T) = 0.4 $.\n",
    "- If the coin lands heads, we roll a fair die: $ P(A|H) = \\frac{1}{6} $.\n",
    "- If the coin lands tails, we roll a biased die: $ P(A|T) = 0.5 $.\n",
    "\n",
    "So:\n",
    "$\n",
    "P(A) = \\left(\\frac{1}{6}\\right) \\cdot 0.6 + 0.5 \\cdot 0.4 = 0.1 + 0.2 = 0.3\n",
    "$\n",
    "\n",
    "The theoretical probability of rolling a 6 is **0.3**.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "###  Verify the Law of Total Probability\n",
    "From the simulation:\n",
    "- The simulated probability $ P(A) \\approx 0.2998 $.\n",
    "- The theoretical probability $ P(A) = 0.3 $.\n",
    "\n",
    "The results are very close, verifying the Law of Total Probability.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Takeaways:\n",
    "1. The Law of Total Probability allows you to compute $ P(A) $ by considering all possible scenarios ($ B_i $).\n",
    "2. You can verify the law using simulated data by:\n",
    "   - Partitioning the sample space into mutually exclusive and exhaustive events.\n",
    "   - Computing $ P(A|B_i) $ and $ P(B_i) $ for each partition.\n",
    "   - Confirming that $ P(A) = \\sum_{i=1}^n P(A|B_i) \\cdot P(B_i) $.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify Law of Total Probability using simulated Data\n",
    "\n",
    "# Parameters\n",
    "headsProbability = 0.2  # Probability of heads\n",
    "tailsProbability = 0.8  # Probability of tails\n",
    "numOfTrials = 100000  # Number of simulations\n",
    "\n",
    "# Simulate the process\n",
    "countA = 0  # Count of event A (rolling a 6)\n",
    "\n",
    "for _ in range(numOfTrials):\n",
    "    # Flip the coin\n",
    "    if random.random() < headsProbability:\n",
    "        # Roll a fair die\n",
    "        roll = random.randint(1, 6)\n",
    "    else:\n",
    "        # Roll a biased die\n",
    "        if random.random() < 0.5:\n",
    "            roll = 6\n",
    "        else:\n",
    "            roll = random.randint(1, 5)\n",
    "    \n",
    "    # Check if event A occurs\n",
    "    if roll == 6:\n",
    "        countA += 1\n",
    "\n",
    "# Compute P(A) from simulation\n",
    "PASimulated = countA / numOfTrials\n",
    "\n",
    "print(\"Simulated P(A):\", PASimulated)\n",
    "print(\"Theoretical P(A):\", 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate and Calculate the probability of drawing specific card from deck\n",
    "\n",
    "# Specific card to draw (e.g., Ace of Spades)\n",
    "def randomDeckOfCards( specificCard = \"Ace of Spades\", simulations = 100000):\n",
    "\n",
    "# Define the deck of cards\n",
    "    suits: list[str] = [\"Hearts\", \"Diamonds\", \"Clubs\", \"Spades\"]\n",
    "    ranks:list[str] = [\"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"Jack\", \"Queen\", \"King\", \"Ace\"]\n",
    "    deck = [f\"{rank} of {suit}\" for suit in suits for rank in ranks]\n",
    "    countSpecificCard = 0\n",
    "\n",
    "# Simulate drawing a card\n",
    "    for _ in range(simulations):\n",
    "        # Shuffle the deck\n",
    "        random.shuffle(deck)\n",
    "        # Draw the top card\n",
    "        drawnCard = deck[0]\n",
    "        # Check if it's the specific card\n",
    "        if drawnCard == specificCard:\n",
    "            countSpecificCard += 1\n",
    "\n",
    "    # Calculate the empirical probability\n",
    "    empiricalProbability = countSpecificCard / simulations\n",
    "\n",
    "    print(\"Specific Card:\", specificCard)\n",
    "    print(\"Theoretical Probability:\", 1/52)\n",
    "    print(\"Empirical Probability:\", empiricalProbability)\n",
    "\n",
    "randomDeckOfCards(specificCard=\"Queen of Hearts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Joint probability** refers to the probability of two events occurring together. It is denoted as $ P(A \\cap B) $ or $ P(A, B) $, and it represents the likelihood of both events $ A $ and $ B $ happening simultaneously.\n",
    "\n",
    "---\n",
    "\n",
    "### Formula for Joint Probability:\n",
    "The joint probability of two events $ A $ and $ B $ is given by:\n",
    "\n",
    "$\n",
    "P(A \\cap B) = P(A) \\cdot P(B|A)\n",
    "$\n",
    "\n",
    "Or equivalently:\n",
    "\n",
    "$\n",
    "P(A \\cap B) = P(B) \\cdot P(A|B)\n",
    "$\n",
    "\n",
    "Where:\n",
    "- $ P(A) $ is the probability of event $ A $,\n",
    "- $ P(B|A) $ is the probability of event $ B $ given that $ A $ has occurred,\n",
    "- $ P(B) $ is the probability of event $ B $,\n",
    "- $ P(A|B) $ is the probability of event $ A $ given that $ B $ has occurred.\n",
    "\n",
    "If events $ A $ and $ B $ are **independent**, then:\n",
    "$\n",
    "P(A \\cap B) = P(A) \\cdot P(B)\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "### Steps to Compute Joint Probability:\n",
    "1. **Identify the events**:\n",
    "   - Define events $ A $ and $ B $.\n",
    "\n",
    "2. **Determine if the events are independent**:\n",
    "   - If $ A $ and $ B $ are independent, use $ P(A \\cap B) = P(A) \\cdot P(B) $.\n",
    "   - If $ A $ and $ B $ are dependent, use $ P(A \\cap B) = P(A) \\cdot P(B|A) $ or $ P(A \\cap B) = P(B) \\cdot P(A|B) $.\n",
    "\n",
    "3. **Compute the probabilities**:\n",
    "   - Calculate $ P(A) $, $ P(B) $, and (if necessary) $ P(B|A) $ or $ P(A|B) $.\n",
    "\n",
    "4. **Calculate the joint probability**:\n",
    "   - Use the appropriate formula to compute $ P(A \\cap B) $.\n",
    "\n",
    "---\n",
    "\n",
    "### Example 1: Independent Events\n",
    "Suppose you roll a fair six-sided die and flip a fair coin. Let:\n",
    "- $ A $: Rolling a **3** on the die.\n",
    "- $ B $: Flipping **heads** on the coin.\n",
    "\n",
    "Since the die roll and coin flip are independent:\n",
    "$\n",
    "P(A) = \\frac{1}{6}, \\quad P(B) = \\frac{1}{2}\n",
    "$\n",
    "\n",
    "The joint probability is:\n",
    "$\n",
    "P(A \\cap B) = P(A) \\cdot P(B) = \\frac{1}{6} \\cdot \\frac{1}{2} = \\frac{1}{12} \\approx 0.0833\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "### Example 2: Dependent Events\n",
    "Suppose you draw two cards from a standard deck of 52 cards without replacement. Let:\n",
    "- $ A $: First card is an **Ace**.\n",
    "- $ B $: Second card is also an **Ace**.\n",
    "\n",
    "Here, $ A $ and $ B $ are dependent events.\n",
    "\n",
    "1. Compute $ P(A) $:\n",
    "   $\n",
    "   P(A) = \\frac{4}{52} = \\frac{1}{13}\n",
    "   $\n",
    "\n",
    "2. Compute $ P(B|A) $:\n",
    "   - If the first card is an Ace, there are now 3 Aces left in the remaining 51 cards.\n",
    "   $\n",
    "   P(B|A) = \\frac{3}{51} = \\frac{1}{17}\n",
    "   $\n",
    "\n",
    "3. Compute the joint probability:\n",
    "   $\n",
    "   P(A \\cap B) = P(A) \\cdot P(B|A) = \\frac{1}{13} \\cdot \\frac{1}{17} = \\frac{1}{221} \\approx 0.0045\n",
    "   $\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### Key Takeaways:\n",
    "- Joint probability measures the likelihood of two events occurring together.\n",
    "- For **independent events**, $ P(A \\cap B) = P(A) \\cdot P(B) $.\n",
    "- For **dependent events**, $ P(A \\cap B) = P(A) \\cdot P(B|A) $ or $ P(A \\cap B) = P(B) \\cdot P(A|B) $.\n",
    "- Use Python to compute joint probabilities efficiently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Joint Probabilities of Two Events\n",
    "\n",
    "\n",
    "\n",
    "def independentEvent()->float:\n",
    "    # Probabilities\n",
    "    pA: float = 1 / 6  # Probability of rolling a 3\n",
    "    pB: float = 1 / 2  # Probability of flipping heads\n",
    "\n",
    "# Joint probability for independent events\n",
    "    pApB:float = pA * pB\n",
    "\n",
    "    print(\"Joint Probability P(A âˆ© B):\", pApB)\n",
    "    return pApB\n",
    "def dependentEvent()->float:\n",
    "    # Probabilities\n",
    "    pA:float = 4 / 52  # Probability of first card being an Ace\n",
    "    pBgivenA:float = 3 / 51  # Probability of second card being an Ace given the first was an Ace\n",
    "\n",
    "    # Joint probability for dependent events\n",
    "    pApB:float = pA * pBgivenA\n",
    "\n",
    "    print(\"Joint Probability P(A âˆ© B):\", pApB)\n",
    "    return pApB\n",
    "\n",
    "independentEvent()\n",
    "dependentEvent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bayes' Theorem** is a fundamental concept in probability that allows us to update the probability of an event based on new information. It is stated as:\n",
    "\n",
    "$\n",
    "P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}\n",
    "$\n",
    "\n",
    "Where:\n",
    "- $ P(A|B) $: Posterior probability of event $ A $ given event $ B $,\n",
    "- $ P(B|A) $: Likelihood of event $ B $ given event $ A $,\n",
    "- $ P(A) $: Prior probability of event $ A $,\n",
    "- $ P(B) $: Total probability of event $ B $.\n",
    "\n",
    "To **verify Bayes' Theorem with a real-world example**, letâ€™s use a medical testing scenario.\n",
    "\n",
    "---\n",
    "\n",
    "### Real-World Example: Medical Testing\n",
    "Suppose:\n",
    "- A disease affects **1%** of the population ($ P(A) = 0.01 $).\n",
    "- A test for the disease is **99% accurate**:\n",
    "  - If a person has the disease, the test is positive **99%** of the time ($ P(B|A) = 0.99 $).\n",
    "  - If a person does not have the disease, the test is negative **99%** of the time ($ P(B^c|A^c) = 0.99 $).\n",
    "\n",
    "We want to find:\n",
    "- The probability that a person has the disease given that they tested positive ($ P(A|B) $).\n",
    "\n",
    "---\n",
    "\n",
    "### Step 1: Theoretical Calculation Using Bayes' Theorem\n",
    "1. **Compute $ P(B|A) $**:\n",
    "   - $ P(B|A) = 0.99 $.\n",
    "\n",
    "2. **Compute $ P(A) $**:\n",
    "   - $ P(A) = 0.01 $.\n",
    "\n",
    "3. **Compute $ P(B|A^c) $**:\n",
    "   - The probability of a false positive is $ 1 - 0.99 = 0.01 $.\n",
    "\n",
    "4. **Compute $ P(B) $**:\n",
    "   - $ P(B) = P(B|A) \\cdot P(A) + P(B|A^c) \\cdot P(A^c) $,\n",
    "   - $ P(B) = (0.99 \\cdot 0.01) + (0.01 \\cdot 0.99) = 0.0099 + 0.0099 = 0.0198 $.\n",
    "\n",
    "5. **Apply Bayes' Theorem**:\n",
    "   $\n",
    "   P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)} = \\frac{0.99 \\cdot 0.01}{0.0198} = \\frac{0.0099}{0.0198} = 0.5\n",
    "   $\n",
    "\n",
    "So, the probability that a person has the disease given that they tested positive is **50%**.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Key Takeaways:\n",
    "1. **Bayes' Theorem** allows us to update probabilities based on new evidence.\n",
    "2. In this example:\n",
    "   - Even with a **99% accurate test**, the probability of having the disease after testing positive is only **50%** due to the low prevalence of the disease.\n",
    "3. Simulations can be used to verify theoretical results and build intuition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify Bayes Theorem with a Real World Example\n",
    "\n",
    "def realWorldBayesTheorem():\n",
    "\n",
    "    # Parameters\n",
    "    pA = 0.01  # Prevalence of the disease\n",
    "    pBgivenA = 0.99  # Probability of testing positive given the disease\n",
    "    pBgivenNotA = 0.01  # Probability of testing positive given no disease\n",
    "\n",
    "    # Number of simulations\n",
    "    trials = 100000\n",
    "\n",
    "    # Counters\n",
    "    countAandB = 0  # Number of people with the disease and testing positive\n",
    "    countB = 0  # Number of people testing positive\n",
    "\n",
    "# Simulate\n",
    "    for _ in range(trials):\n",
    "      # Determine if the person has the disease\n",
    "        hasDisease = random.random() < pA\n",
    "     # Determine if the test is positive\n",
    "        if hasDisease:\n",
    "            testPositive = random.random() < pBgivenA\n",
    "        else:\n",
    "            testPositive = random.random() < pBgivenNotA\n",
    "     # Update counters\n",
    "        if testPositive:\n",
    "            countB += 1\n",
    "            if hasDisease:\n",
    "                countAandB += 1\n",
    "\n",
    "    # Compute P(A|B) from simulation\n",
    "    pAgivenBSimulated = countAandB / countB\n",
    "\n",
    "    # Theoretical P(A|B)\n",
    "    pB = pBgivenA * pA + pBgivenNotA * (1 - pA)\n",
    "    pAgivenBTheoretical = (pBgivenA * pA) / pB\n",
    "\n",
    "    print(\"Simulated P(A|B):\", pAgivenBSimulated)\n",
    "    print(\"Theoretical P(A|B):\", pAgivenBTheoretical)\n",
    "\n",
    "\n",
    "realWorldBayesTheorem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -- Discrete and Continuous Random Variables --\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **binomial random variable** models the number of successes in a fixed number of independent trials, each with the same probability of success. It is characterized by two parameters:\n",
    "- $ n $: Number of trials,\n",
    "- $ p $: Probability of success in each trial.\n",
    "\n",
    "The **mean** and **variance** of a binomial random variable are given by:\n",
    "- Mean: $ \\mu = n \\cdot p $,\n",
    "- Variance: $ \\sigma^2 = n \\cdot p \\cdot (1 - p) $.\n",
    "\n",
    "---\n",
    "\n",
    "### Steps to Generate a Binomial Random Variable and Compute Its Mean and Variance:\n",
    "1. **Generate binomial random variables**:\n",
    "   - Use a random number generator to simulate $ n $ independent trials, each with probability $ p $ of success.\n",
    "   - Count the number of successes.\n",
    "\n",
    "2. **Compute the mean and variance**:\n",
    "   - Use the formulas $ \\mu = n \\cdot p $ and $ \\sigma^2 = n \\cdot p \\cdot (1 - p) $.\n",
    "\n",
    "---\n",
    "\n",
    "### Example:\n",
    "Suppose:\n",
    "- $ n = 10 $ (number of trials),\n",
    "- $ p = 0.5 $ (probability of success in each trial).\n",
    "\n",
    "We want to:\n",
    "1. Generate a binomial random variable.\n",
    "2. Compute its mean and variance.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Takeaways:\n",
    "- A binomial random variable represents the number of successes in $ n $ independent trials, each with probability $ p $ of success.\n",
    "- The mean is $ \\mu = n \\cdot p $, and the variance is $ \\sigma^2 = n \\cdot p \\cdot (1 - p) $.\n",
    "- Use Python libraries like `numpy` or `scipy.stats` to generate binomial random variables and compute their properties.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Generate Binomial random variable and compute it's mean and variance\n",
    "\n",
    "def binomialMeanVariance(trials=10, successProbability= 0.5, iterations =10000):\n",
    "    randomBinomialVariable = np.random.binomial(trials, successProbability, iterations)\n",
    "    empiricalMean = np.mean(randomBinomialVariable)\n",
    "    empiricalVariance = np.var(randomBinomialVariable)\n",
    "    theoreticalMean = trials * successProbability\n",
    "    theoreticalVariance = trials * successProbability * (1 - successProbability)\n",
    "    print(\"Empirical Mean:\", empiricalMean)\n",
    "    print(\"Theoretical Mean:\", theoreticalMean)\n",
    "    print(\"Empirical Variance:\", empiricalVariance)\n",
    "    print(\"Theoretical Variance:\", theoreticalVariance)\n",
    "\n",
    "binomialMeanVariance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Poisson process** is a stochastic process that models events occurring randomly over time (or space) at a constant average rate. It is characterized by:\n",
    "- $ \\lambda $: The average rate of events per unit time (or space),\n",
    "- $ k $: The number of events in a given interval.\n",
    "\n",
    "The probability of observing $ k $ events in a time interval $ t $ is given by the **Poisson distribution**:\n",
    "\n",
    "$\n",
    "P(X = k) = \\frac{(\\lambda t)^k e^{-\\lambda t}}{k!}\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "### Steps to Simulate a Poisson Process and Calculate Probabilities:\n",
    "1. **Simulate the Poisson process**:\n",
    "   - Generate the times at which events occur using the exponential distribution (since inter-arrival times in a Poisson process are exponentially distributed).\n",
    "\n",
    "2. **Calculate probabilities of specific events**:\n",
    "   - Use the Poisson distribution formula to compute the probability of observing $ k $ events in a given interval.\n",
    "\n",
    "---\n",
    "\n",
    "### Example:\n",
    "Suppose:\n",
    "- $ \\lambda = 2 $ (average rate of 2 events per unit time),\n",
    "- $ t = 3 $ (time interval of interest).\n",
    "\n",
    "We want to:\n",
    "1. Simulate a Poisson process over the interval $ [0, 3] $.\n",
    "2. Calculate the probability of observing $ k = 5 $ events in this interval.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate the Poison Process and calculate the probabilities of specific events\n",
    "\n",
    "\n",
    "def poisonProcess()-> int:\n",
    "\n",
    "# Parameters\n",
    "    lambda_:int = 2  # Average rate of events per unit time\n",
    "    t:int = 3  # Time interval\n",
    "\n",
    "# Simulate inter-arrival times (exponentially distributed)\n",
    "    interArrivalTimes = np.random.exponential(scale=1/lambda_, size=1000)\n",
    "\n",
    "# Simulate event times\n",
    "    eventTimes = np.cumsum(interArrivalTimes)\n",
    "\n",
    "# Filter events within the interval [0, t]\n",
    "    eventsInInterval = eventTimes[eventTimes <= t]\n",
    "\n",
    "# Number of events in the interval\n",
    "    numEvents:int = len(eventsInInterval)\n",
    "\n",
    "    print(\"Number of events in [0, 3]:\", numEvents)\n",
    "    return numEvents\n",
    "\n",
    "poisonProcess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **probability mass function (PMF)** of a discrete random variable gives the probability that the variable takes on a specific value. For a discrete random variable $ X $, the PMF is defined as:\n",
    "\n",
    "$\n",
    "P(X = x) = f(x)\n",
    "$\n",
    "\n",
    "Where:\n",
    "- $ f(x) $ is the probability that $ X $ takes the value $ x $,\n",
    "- $ \\sum_{x} f(x) = 1 $ (the probabilities sum to 1).\n",
    "\n",
    "---\n",
    "\n",
    "### Steps to Implement the PMF:\n",
    "1. **Define the random variable**:\n",
    "   - Identify the possible values $ x $ that the random variable $ X $ can take.\n",
    "\n",
    "2. **Define the probabilities**:\n",
    "   - Assign probabilities $ f(x) $ to each value $ x $, ensuring that $ \\sum_{x} f(x) = 1 $.\n",
    "\n",
    "3. **Implement the PMF**:\n",
    "   - Create a function that takes a value $ x $ as input and returns $ P(X = x) $.\n",
    "\n",
    "---\n",
    "\n",
    "### Example: PMF of a Fair Six-Sided Die\n",
    "For a fair six-sided die:\n",
    "- The random variable $ X $ can take values $ \\{1, 2, 3, 4, 5, 6\\} $.\n",
    "- The PMF is $ P(X = x) = \\frac{1}{6} $ for $ x \\in \\{1, 2, 3, 4, 5, 6\\} $.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Takeaways:\n",
    "- The **PMF** of a discrete random variable gives the probability that the variable takes on a specific value.\n",
    "- You can implement the PMF using conditional statements or a dictionary for more flexibility.\n",
    "- Ensure that the probabilities sum to 1 for all possible values of the random variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the probability mass function (PMF) of a discrete random variable\n",
    "\n",
    "def probabilityMassFunction(x, pmfDictionary):\n",
    "    for value in x:\n",
    "        if value in pmfDictionary:\n",
    "            print(f\"P(X = {value}) = {pmfDictionary[value]}\")\n",
    "        else:\n",
    "            print(f\"P(X = {value}) is not defined in the PMF.\")\n",
    "    return pmfDictionary\n",
    "\n",
    "# Define the PMF as a dictionary\n",
    "pmf_dict = {\n",
    "    0: 0.2,\n",
    "    1: 0.5,\n",
    "    2: 0.3,\n",
    "    3: 0.4\n",
    "}\n",
    "\n",
    "# Example usage\n",
    "x_values = [0, 1, 2, 3]\n",
    "\n",
    "probabilityMassFunction(x_values ,pmf_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Normal Random Variable: Definition**\n",
    "A **Normal random variable** follows the **Normal (Gaussian) Distribution**, defined by two parameters:\n",
    "- **Mean (Î¼):** The center of the distribution.\n",
    "- **Standard deviation (Ïƒ):** Controls the spread of the distribution.\n",
    "\n",
    "The probability density function (PDF) is given by:\n",
    "\n",
    "$\n",
    "f(x; \\mu, \\sigma) = \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-\\frac{(x - \\mu)^2}{2\\sigma^2}}\n",
    "$\n",
    "\n",
    "This describes the probability of a continuous variable $ X $ taking on a specific value $ x $. The area under the curve of the PDF over an interval represents the probability for that interval.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### **Summary**\n",
    "1. Use `numpy.random.normal` to generate random variables.\n",
    "2. Use `scipy.stats.norm.pdf` for PDF values and `norm.cdf` for probabilities.\n",
    "3. Calculate probabilities over an interval with $ F(b) - F(a) $.\n",
    "4. Visualize the distribution using `matplotlib`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate normal random variable and compute probabilities at specific intervals\n",
    "def probabilitiesAtSpecificIntervals(mean=0, standerDeviation= 1, sampleSize=1000):\n",
    "    # Normal RandomVariables\n",
    "    data = np.random.normal(mean, standerDeviation, sampleSize)\n",
    "    # PDF at specific points\n",
    "    point = 1\n",
    "    pdfValues = sciStats.norm.pdf(point, loc=mean, scale=standerDeviation)\n",
    "    print(f\"PDF at X={point}: {pdfValues}\")\n",
    "    # 3. Compute probabilities for an interval [a, b]\n",
    "    intervalA, intervalB = -1, 1   \n",
    "    probability = sciStats.norm.cdf(intervalB, loc=mean, scale=standerDeviation) -sciStats.norm.cdf(intervalA, loc=mean, scale=standerDeviation)\n",
    "    print(f\"P({intervalA} <= X <= {intervalB}): {probability}\")\n",
    "    # Proportion of samples in the interval [a, b]\n",
    "    proportion = np.mean((data >= intervalA) & (data <= intervalB))\n",
    "    print(f\"Proportion of samples in [{intervalA}, {intervalB}]: {proportion}\")\n",
    "\n",
    "\n",
    "probabilitiesAtSpecificIntervals()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### **What Is the Cumulative Distribution Function (CDF)?**\n",
    "\n",
    "The **cumulative distribution function (CDF)** of a random variable $ X $ represents the probability that $ X $ takes on a value less than or equal to a specific value $ x $. \n",
    "\n",
    "Mathematically, it is defined as:\n",
    "\n",
    "$\n",
    "F(x) = P(X \\leq x)\n",
    "$\n",
    "\n",
    "For a **normal distribution**, this means $ F(x) $ is the probability that a randomly selected value from the distribution is less than or equal to $ x $.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Properties of the CDF:**\n",
    "\n",
    "1. **Range:** The CDF value $ F(x) $ is always between 0 and 1:\n",
    "   $\n",
    "   0 \\leq F(x) \\leq 1\n",
    "   $\n",
    "   - $ F(x) = 0 $: All values are smaller than $ x $ (unlikely).\n",
    "   - $ F(x) = 1 $: All values are less than or equal to $ x $ (certain).\n",
    "\n",
    "2. **Monotonicity:** The CDF is a non-decreasing function, meaning $ F(x_1) \\leq F(x_2) $ for $ x_1 < x_2 $.\n",
    "\n",
    "3. **Asymptotes for Normal Distribution:**\n",
    "   - As $ x \\to -\\infty $, $ F(x) \\to 0 $.\n",
    "   - As $ x \\to +\\infty $, $ F(x) \\to 1 $.\n",
    "\n",
    "---\n",
    "\n",
    "### **How It Relates to a Normal Distribution**\n",
    "\n",
    "For a **normal distribution** with mean $ \\mu $ and standard deviation $ \\sigma $, the cumulative distribution function $ F(x) $ cannot be solved in closed form. Instead, it is computed numerically using integration of the **probability density function (PDF):**\n",
    "\n",
    "$\n",
    "F(x) = \\int_{-\\infty}^{x} \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-\\frac{(t-\\mu)^2}{2\\sigma^2}} dt\n",
    "$\n",
    "\n",
    "The area under the curve of the PDF from $ -\\infty $ to $ x $ gives the value of $ F(x) $.\n",
    "\n",
    "---\n",
    "\n",
    "### **Why Is CDF Useful?**\n",
    "1. **Probability over an Interval:**\n",
    "   - You can compute the probability that $ X $ lies between two values, $ a $ and $ b $, using:\n",
    "     $\n",
    "     P(a \\leq X \\leq b) = F(b) - F(a)\n",
    "     $\n",
    "\n",
    "2. **Comparison of Values:**\n",
    "   - The CDF tells us how likely a value is relative to the rest of the distribution. For example, if $ F(x) = 0.75 $, then $ x $ is greater than 75% of the values in the distribution.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the cumulative distribution function of a normal distribution\n",
    "\n",
    "def cumulativeDistributionFunction(mean= 0,standardDeviation=1, value=1.5)-> float:\n",
    "    cdfValue:float = sciStats.norm.cdf(value, loc=mean, scale=standardDeviation)\n",
    "    print(f\"CDF at X={value} for N({mean}, {standardDeviation}): {cdfValue}\")\n",
    "    return cdfValue\n",
    "cumulativeDistributionFunction()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **What is the Central Limit Theorem (CLT)?**\n",
    "\n",
    "The **Central Limit Theorem (CLT)** is a fundamental theorem in probability and statistics. It states that:\n",
    "\n",
    "> The sum (or average) of a large number of independent and identically distributed (i.i.d.) random variables, regardless of their original distribution, will tend to follow a **normal distribution**, provided the number of variables is sufficiently large.\n",
    "\n",
    "#### **Key Points:**\n",
    "1. The mean of the resulting normal distribution is the sum of the means of the individual random variables.\n",
    "   $\n",
    "   \\mu_{sum} = n \\cdot \\mu\n",
    "   $\n",
    "2. The variance of the resulting normal distribution is the sum of the variances of the individual random variables.\n",
    "   $\n",
    "   \\sigma^2_{sum} = n \\cdot \\sigma^2\n",
    "   $\n",
    "3. This approximation becomes more accurate as $ n $, the number of random variables, increases.\n",
    "\n",
    "---\n",
    "\n",
    "### **Why Is CLT Important?**\n",
    "\n",
    "The CLT allows us to:\n",
    "- Approximate the sum (or average) of random variables using a normal distribution.\n",
    "- Make statistical inferences about sample means even when the population distribution is not normal.\n",
    "\n",
    "---\n",
    "\n",
    "### **How to Use CLT to Approximate the Sum of Random Variables**\n",
    "\n",
    "#### **1. Identify the Random Variables**\n",
    "   - Ensure the random variables are:\n",
    "     - Independent\n",
    "     - Identically distributed (same mean $ \\mu $ and variance $ \\sigma^2 $).\n",
    "\n",
    "#### **2. Compute the Parameters for the Sum**\n",
    "   - If you are summing $ n $ random variables with:\n",
    "     - Mean $ \\mu $,\n",
    "     - Standard deviation $ \\sigma $,\n",
    "   - Then:\n",
    "     $\n",
    "     \\mu_{sum} = n \\cdot \\mu, \\quad \\sigma_{sum} = \\sqrt{n} \\cdot \\sigma\n",
    "     $\n",
    "\n",
    "#### **3. Approximate Using the Normal Distribution**\n",
    "   - Treat the sum as a random variable following:\n",
    "     $\n",
    "     S \\sim N(\\mu_{sum}, \\sigma_{sum}^2)\n",
    "     $\n",
    "\n",
    "#### **4. Calculate Probabilities**\n",
    "   - Use the normal distribution's **CDF** or **PDF** to compute probabilities about the sum.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step-by-Step Example**\n",
    "\n",
    "#### Problem: \n",
    "Suppose we roll a fair 6-sided die 50 times. Each roll is a random variable $ X $, with:\n",
    "- $ \\mu = 3.5 $ (average of die outcomes),\n",
    "- $ \\sigma^2 = \\frac{35}{12} \\approx 2.92 $ (variance of die outcomes).\n",
    "\n",
    "We want to approximate the probability that the sum of the 50 rolls is less than 200.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step-by-Step Solution**\n",
    "\n",
    "1. **Identify the Parameters of the Individual Random Variables**:\n",
    "   - $ \\mu = 3.5 $\n",
    "   - $ \\sigma = \\sqrt{\\frac{35}{12}} \\approx 1.71 $\n",
    "\n",
    "2. **Compute the Parameters for the Sum**:\n",
    "   - $ \\mu_{sum} = n \\cdot \\mu = 50 \\cdot 3.5 = 175 $\n",
    "   - $ \\sigma_{sum} = \\sqrt{n} \\cdot \\sigma = \\sqrt{50} \\cdot 1.71 \\approx 12.09 $\n",
    "\n",
    "3. **Approximate Using the Normal Distribution**:\n",
    "   - The sum $ S $ follows approximately:\n",
    "     $\n",
    "     S \\sim N(175, 12.09^2)\n",
    "     $\n",
    "\n",
    "4. **Standardize the Value $ x = 200 $ Using the Z-Score**:\n",
    "   - The Z-score is given by:\n",
    "     $\n",
    "     Z = \\frac{x - \\mu_{sum}}{\\sigma_{sum}}\n",
    "     $\n",
    "   - For $ x = 200 $:\n",
    "     $\n",
    "     Z = \\frac{200 - 175}{12.09} \\approx 2.07\n",
    "     $\n",
    "\n",
    "5. **Find the Probability Using the CDF**:\n",
    "   - Using the standard normal distribution table or Python:\n",
    "     $\n",
    "     P(S \\leq 200) = P(Z \\leq 2.07)\n",
    "     $\n",
    "   - From the CDF of the standard normal distribution:\n",
    "     $\n",
    "     P(Z \\leq 2.07) \\approx 0.9808\n",
    "     $\n",
    "\n",
    "So, the probability that the sum of the dice rolls is less than 200 is approximately **98.08%**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Takeaways:**\n",
    "- The Central Limit Theorem approximates the sum of random variables as a normal distribution.\n",
    "- Use the CLT when:\n",
    "  - Random variables are independent and identically distributed.\n",
    "  - The sample size $ n $ is large (usually $ n > 30 $ is sufficient).\n",
    "- Approximate probabilities using the mean and standard deviation of the sum.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Central Limit Theorem to approximate the the sum of random variables \n",
    "\n",
    "\n",
    "\n",
    "def approximateSumViaCLT(mean=3.5, standardDeviation=0, trials= 50)-> np.float32:\n",
    "    standardDeviation = np.sqrt(35/12)\n",
    "    print(standardDeviation)\n",
    "    meanSum:float = trials * mean\n",
    "    standardDeviationSum:np.float32 = np.sqrt(trials) * standardDeviation\n",
    "    # Compute the probability for the sum being less than 200\n",
    "    x:int = 200\n",
    "    z = (x - meanSum) / standardDeviationSum   \n",
    "    probability:np.float32 =sciStats.norm.cdf(z)\n",
    "    print(f\"P(Sum â‰¤ {x}): {probability}\")\n",
    "    return probability\n",
    "\n",
    "approximateSumViaCLT()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **What is an Exponential Distribution?**\n",
    "\n",
    "The **exponential distribution** is a continuous probability distribution used to model the time until an event occurs (e.g., time between arrivals of customers at a store). It is widely used in reliability analysis, queuing theory, and survival analysis.\n",
    "\n",
    "---\n",
    "\n",
    "### **Mathematical Definition**\n",
    "\n",
    "1. **Probability Density Function (PDF):**\n",
    "   $\n",
    "   f(x; \\lambda) = \\lambda e^{-\\lambda x}, \\quad x \\geq 0\n",
    "   $\n",
    "   - $ \\lambda > 0 $ is the rate parameter (mean time between events is $ \\frac{1}{\\lambda} $).\n",
    "\n",
    "2. **Cumulative Distribution Function (CDF):**\n",
    "   $\n",
    "   F(x; \\lambda) = P(X \\leq x) = 1 - e^{-\\lambda x}, \\quad x \\geq 0\n",
    "   $\n",
    "\n",
    "3. **Key Properties:**\n",
    "   - The mean of the distribution is $ \\mu = \\frac{1}{\\lambda} $.\n",
    "   - The variance is $ \\sigma^2 = \\frac{1}{\\lambda^2} $.\n",
    "\n",
    "---\n",
    "\n",
    "### **How to Simulate and Compute Probabilities**\n",
    "\n",
    "Hereâ€™s how you can simulate an exponential distribution and compute probabilities:\n",
    "\n",
    "---\n",
    "\n",
    "### **Step-by-Step Guide**\n",
    "\n",
    "1. **Define the Rate Parameter ($ \\lambda $):**\n",
    "   - Choose a positive value for $ \\lambda $, which represents the event rate per unit time.\n",
    "\n",
    "2. **Simulate Random Variables:**\n",
    "   - Use `numpy.random.exponential(scale, size)` where:\n",
    "     - $ \\text{scale} = \\frac{1}{\\lambda} $,\n",
    "     - `size` is the number of random variables to generate.\n",
    "\n",
    "3. **Compute the PDF at a Specific Point:**\n",
    "   - Use the formula $ f(x; \\lambda) = \\lambda e^{-\\lambda x} $.\n",
    "\n",
    "4. **Compute the CDF at a Specific Point:**\n",
    "   - Use the formula $ F(x; \\lambda) = 1 - e^{-\\lambda x} $.\n",
    "\n",
    "5. **Calculate Probabilities for Intervals:**\n",
    "   - For $ P(a \\leq X \\leq b) $, use:\n",
    "     $\n",
    "     P(a \\leq X \\leq b) = F(b; \\lambda) - F(a; \\lambda)\n",
    "     $\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### **Key Takeaways**\n",
    "- The exponential distribution is memory-less: $ P(X > a + b | X > a) = P(X > b) $.\n",
    "- Use $ \\lambda $ to define the event rate and compute probabilities or simulate values.\n",
    "- CDF helps calculate probabilities over intervals easily.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate and Compute the probabilities of an exponential distribution\n",
    "\n",
    "def probabilityOfExponentialDistribution(rate=2, sample=1000)->np.float32:\n",
    "    np.random.exponential(scale=1/rate, size=sample)\n",
    "    pointX:int = 1\n",
    "    pdfValue:float =  rate * np.exp(-rate * pointX)\n",
    "    print(f\"PDF at X={pointX}: {pdfValue}\")\n",
    "    cdfValue:float = 1 - np.exp(-rate * pointX)\n",
    "    print(f\"CDF at X={pointX}: {cdfValue}\")\n",
    "    intervalA, intervalB = 0.5, 2\n",
    "    cdfA:np.float32 = 1 - np.exp(-rate * intervalA)\n",
    "    cdfB:np.float32 = 1 - np.exp(-rate * intervalB)\n",
    "    probabilityInterval:np.float32 = cdfB - cdfA\n",
    "    print(f\"P({intervalA} <= X <= {intervalB}): {probabilityInterval}\")\n",
    "    return probabilityInterval\n",
    "\n",
    "probabilityOfExponentialDistribution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a Normal Distribution to a give dataset and estimate it's parameters\n",
    "\n",
    "def fitDatasetToNormalDistribution():\n",
    "    # Step 1: Sample dataset (replace with your actual data)\n",
    "    data = np.random.normal(loc=5, scale=2, size=1000)  # Example data (mean=5, std=2)\n",
    "\n",
    "# Step 2: Estimate the parameters (mean and std)\n",
    "    estimatedMean = np.mean(data)\n",
    "    estimatedStandardDeviation = np.std(data, ddof=1)  # Sample standard deviation (use ddof=1)\n",
    "\n",
    "# Step 3: Print the estimated parameters\n",
    "    print(f\"Estimated Mean (Î¼): {estimatedMean:.2f}\")\n",
    "    print(f\"Estimated Standard Deviation (Ïƒ): {estimatedStandardDeviation:.2f}\")\n",
    "\n",
    "fitDatasetToNormalDistribution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Comparison of Discrete and Continuous Random Variables**\n",
    "\n",
    "Random variables are fundamental in probability theory and statistics. They represent the outcomes of random processes or experiments. The distinction between **discrete** and **continuous** random variables is based on the nature of their possible values.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Definition:**\n",
    "\n",
    "- **Discrete Random Variable:**\n",
    "  - A discrete random variable can take on only a **finite or countably infinite** number of distinct values.\n",
    "  - The values are typically integers or whole numbers, but they could also be any countable set.\n",
    "  - Examples: Number of heads in 10 coin tosses, number of cars passing a traffic light in one minute.\n",
    "\n",
    "- **Continuous Random Variable:**\n",
    "  - A continuous random variable can take on an **infinite number of possible values** within a certain range or interval.\n",
    "  - These values are not countable and can take any value within a real number interval.\n",
    "  - Examples: Height of individuals, time taken to run a race, temperature.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Probability Distribution:**\n",
    "\n",
    "- **Discrete Random Variable:**\n",
    "  - The probability distribution is given by the **probability mass function (PMF)**.\n",
    "  - The sum of the probabilities for all possible outcomes is always 1:\n",
    "    $\n",
    "    P(X = x_i) = p(x_i), \\quad \\sum p(x_i) = 1\n",
    "    $\n",
    "  - For discrete random variables, the probabilities of specific outcomes can be computed directly.\n",
    "\n",
    "- **Continuous Random Variable:**\n",
    "  - The probability distribution is given by the **probability density function (PDF)**.\n",
    "  - The probability that the variable takes any specific value is **zero**. Instead, we compute the probability that the variable falls within an interval:\n",
    "    $\n",
    "    P(a \\leq X \\leq b) = \\int_{a}^{b} f(x) \\, dx\n",
    "    $\n",
    "  - For continuous random variables, the probability density is a function, and the total area under the PDF curve is 1.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Example Distributions:**\n",
    "\n",
    "- **Discrete Random Variable:**\n",
    "  - **Binomial Distribution**: Models the number of successes in a fixed number of independent Bernoulli trials.\n",
    "  - **Poisson Distribution**: Models the number of events occurring in a fixed interval of time or space.\n",
    "  \n",
    "- **Continuous Random Variable:**\n",
    "  - **Normal Distribution**: Describes data that clusters around a mean value and is symmetric.\n",
    "  - **Exponential Distribution**: Models the time between events in a Poisson process.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Probability Calculation:**\n",
    "\n",
    "- **Discrete Random Variable:**\n",
    "  - We can directly calculate the probability of specific outcomes (e.g., $ P(X = 3) $) using the PMF.\n",
    "\n",
    "- **Continuous Random Variable:**\n",
    "  - We calculate probabilities over intervals (e.g., $ P(3 \\leq X \\leq 5) $) using the area under the PDF curve between the bounds of the interval.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Notation:**\n",
    "\n",
    "- **Discrete Random Variable:**\n",
    "  - $ X $ takes values from a finite or countably infinite set: $ X \\in \\{x_1, x_2, x_3, \\dots\\} $.\n",
    "\n",
    "- **Continuous Random Variable:**\n",
    "  - $ X $ can take any value from a continuous interval: $ X \\in [a, b] $, or $ X \\in (-\\infty, \\infty) $.\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Examples:**\n",
    "\n",
    "- **Discrete Random Variable Example:**\n",
    "  - **Tossing a Die**: When rolling a fair six-sided die, the possible outcomes are $ 1, 2, 3, 4, 5, 6 $. The probability of rolling each specific number is $ \\frac{1}{6} $.\n",
    "  \n",
    "- **Continuous Random Variable Example:**\n",
    "  - **Height of a Person**: The height can take any value within a range (e.g., between 140 cm and 200 cm), and the probability of a person being exactly 180 cm is technically zero. We instead calculate the probability of being within a range (e.g., 170 cm to 180 cm).\n",
    "\n",
    "---\n",
    "\n",
    "### **7. Mathematical Expectation:**\n",
    "\n",
    "- **Discrete Random Variable:**\n",
    "  - The expected value (mean) is computed as a weighted average of the possible outcomes:\n",
    "    $\n",
    "    E(X) = \\sum_{i} x_i \\cdot P(X = x_i)\n",
    "    $\n",
    "\n",
    "- **Continuous Random Variable:**\n",
    "  - The expected value (mean) is computed as an integral:\n",
    "    $\n",
    "    E(X) = \\int_{-\\infty}^{\\infty} x \\cdot f(x) \\, dx\n",
    "    $\n",
    "\n",
    "---\n",
    "\n",
    "### **8. Variance and Standard Deviation:**\n",
    "\n",
    "- **Discrete Random Variable:**\n",
    "  - Variance is calculated as:\n",
    "    $\n",
    "    Var(X) = \\sum_{i} (x_i - E(X))^2 \\cdot P(X = x_i)\n",
    "    $\n",
    "  \n",
    "- **Continuous Random Variable:**\n",
    "  - Variance is calculated as:\n",
    "    $\n",
    "    Var(X) = \\int_{-\\infty}^{\\infty} (x - E(X))^2 \\cdot f(x) \\, dx\n",
    "    $\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Differences Summary**\n",
    "\n",
    "| Feature                        | Discrete Random Variable          | Continuous Random Variable        |\n",
    "|---------------------------------|-----------------------------------|-----------------------------------|\n",
    "| **Possible Values**             | Countable (finite or infinite)    | Uncountable (in a range/interval) |\n",
    "| **Probability Function**        | Probability Mass Function (PMF)   | Probability Density Function (PDF)|\n",
    "| **Specific Value Probability**  | Can calculate $ P(X = x) $      | $ P(X = x) = 0 $, calculate over intervals |\n",
    "| **Example**                     | Binomial, Poisson                 | Normal, Exponential               |\n",
    "| **Probability Calculation**     | Directly for specific values      | Over an interval (area under curve) |\n",
    "| **Mathematical Expectation**    | $ E(X) = \\sum x_i \\cdot P(X = x_i) $ | $ E(X) = \\int x \\cdot f(x) \\, dx $ |\n",
    "\n",
    "---\n",
    "\n",
    "### **Conclusion:**\n",
    "\n",
    "- **Discrete random variables** are more straightforward in terms of calculating probabilities and expectations, as the possible outcomes are countable and distinct.\n",
    "- **Continuous random variables** deal with probabilities over intervals and require the use of integrals for calculations, making them more complex but useful for modeling real-world phenomena like time, distance, or measurements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the behavior of Discrete and Continuous random variables\n",
    "\n",
    "\n",
    "# Discrete Random Variable Example: Rolling a Die (Uniform Distribution)\n",
    "# Simulating 1000 rolls of a fair six-sided die\n",
    "discreteData = np.random.choice([1, 2, 3, 4, 5, 6], size=1000, p=[1/6]*6)\n",
    "\n",
    "# Calculate probability of rolling a 3\n",
    "probRoll3 = np.sum(discreteData == 3) / len(discreteData)\n",
    "print(f\"Probability of rolling a 3: {probRoll3:.2f}\")\n",
    "\n",
    "# Calculate the expected value (mean) and variance for the discrete data\n",
    "expectedValueDiscrete = np.mean(discreteData)\n",
    "discreteVariance = np.var(discreteData)\n",
    "print(f\"Discrete Expected Value (Mean): {expectedValueDiscrete:.2f}\")\n",
    "print(f\"Discrete Variance: {discreteVariance:.2f}\")\n",
    "\n",
    "# Continuous Random Variable Example: Generating data from a Normal Distribution\n",
    "# Simulating 1000 random variables from a normal distribution (mean=5, std=2)\n",
    "mu = 5\n",
    "sigma = 2\n",
    "n = 1000\n",
    "continuousData = np.random.normal(mu, sigma, n)\n",
    "\n",
    "# Calculate probability of being between 4 and 6 (CDF approach)\n",
    "cdf4 = sciStats.norm.cdf(4, mu, sigma)\n",
    "cdf6 = sciStats.norm.cdf(6, mu, sigma)\n",
    "probBetween4And6 = cdf6 - cdf4\n",
    "print(f\"Probability of being between 4 and 6: {probBetween4And6:.2f}\")\n",
    "\n",
    "# Calculate the expected value (mean) and variance for the continuous data\n",
    "expectedValueContinuous = np.mean(continuousData)\n",
    "continuousVariance = np.var(continuousData)\n",
    "print(f\"Continuous Expected Value (Mean): {expectedValueContinuous:.2f}\")\n",
    "print(f\"Continuous Variance: {continuousVariance:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -- Sampling & Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Sampling on dataset\n",
    "\n",
    "def randomSamplingOnDataSet(dataset, sampleSize:int):\n",
    "    randomSampleWithOutReplace = np.random.choice(dataset, size=sampleSize, replace=False)\n",
    "    print(f\"Random Sample (without replacement): {randomSampleWithOutReplace}\")\n",
    "    randomSampleWithReplace = np.random.choice(dataset, size=sampleSize, replace=True)\n",
    "    print(f\"Random Sample (with replacement): {randomSampleWithReplace}\")\n",
    "    return randomSampleWithReplace, randomSampleWithOutReplace\n",
    "\n",
    "data = np.array([1,2,3,4,5,6,7,8,9])\n",
    "randomSamplingOnDataSet(data, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Sample mean and Variance\n",
    "\n",
    "def sampleMeanAndVariance(data):\n",
    "    sampleMean = np.mean(data)\n",
    "    sampleVariance= np.var(data, ddof=1)\n",
    "    print(f\"Sample Mean: {sampleMean}\")\n",
    "    print(f\"Sample Variance: {sampleVariance}\")\n",
    "    return sampleMean, sampleVariance\n",
    "\n",
    "sampleMeanAndVariance([12,4,5,6,57,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate Analyze sample Distribution of mean\n",
    "\n",
    "def analyzeSampleDistributionMean():\n",
    "\n",
    "# Parameters for the population\n",
    "    populationMean = 50\n",
    "    populationStandard = 10\n",
    "    populationSize = 10000\n",
    "\n",
    "# Create the population (e.g., normal distribution)\n",
    "    populationData = np.random.normal(populationMean, populationStandard, populationSize)\n",
    "\n",
    "# Parameters for sampling\n",
    "    sampleSize = 30  # Size of each sample\n",
    "    totalSamples = 1000  # Number of samples to draw\n",
    "\n",
    "# List to store the sample means\n",
    "    sampleMeans = []\n",
    "\n",
    "# Draw samples and calculate their means\n",
    "    for _ in range(totalSamples):\n",
    "        sample = np.random.choice(populationData, size=sampleSize, replace=False)\n",
    "        sampleMeans.append(np.mean(sample))\n",
    "\n",
    "# Convert sample means to a numpy array for easier analysis\n",
    "    sampleMeans = np.array(sampleMeans)\n",
    "\n",
    "# Compute sample mean and sample standard deviation of the sample means\n",
    "    meanSampleMeans = np.mean(sampleMeans)\n",
    "    standardSampleMeans = np.std(sampleMeans)\n",
    "\n",
    "    print(f\"Mean of sample means: {meanSampleMeans}\")\n",
    "    print(f\"Standard deviation of sample means: {standardSampleMeans}\")\n",
    "\n",
    "\n",
    "analyzeSampleDistributionMean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Stratified Sampling**\n",
    "\n",
    "**Stratified sampling** is a method of sampling that involves dividing the population into distinct subgroups or **strata** that share a specific characteristic. Then, random samples are taken from each of these subgroups. This method ensures that each subgroup is represented in the sample, making it particularly useful when the population contains subgroups that may vary significantly from each other.\n",
    "\n",
    "### **Steps for Stratified Sampling**:\n",
    "1. **Divide the population into strata** based on a characteristic (e.g., age, income, gender).\n",
    "2. **Randomly sample** from each stratum.\n",
    "3. Combine the samples from each stratum to form the final sample.\n",
    "\n",
    "The main goal of stratified sampling is to ensure that important subgroups are adequately represented in the sample.\n",
    "\n",
    "---\n",
    "\n",
    "### **Mathematical Concept**:\n",
    "In stratified sampling, the population is divided into $ k $ strata, and a sample is drawn from each stratum.\n",
    "\n",
    "#### Formula for Stratified Sampling:\n",
    "- **Proportional Stratified Sampling**: The number of samples selected from each stratum is proportional to the size of the stratum in the population.\n",
    "  \n",
    "  $\n",
    "  n_i = \\frac{N_i}{N} \\times n\n",
    "  $\n",
    "  Where:\n",
    "  - $ n_i $ = number of samples to be drawn from stratum $ i $\n",
    "  - $ N_i $ = size of stratum $ i $\n",
    "  - $ N $ = total population size\n",
    "  - $ n $ = total sample size\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### **Key Notes**:\n",
    "1. **Proportional Stratified Sampling** ensures that each subgroup is represented according to its size in the population.\n",
    "2. **Equal Allocation** (an alternative approach) can be used where the same number of samples are drawn from each stratum, regardless of the strata's size in the population.\n",
    "\n",
    "---\n",
    "\n",
    "### **Applications of Stratified Sampling**:\n",
    "- **Ensuring Representation**: When certain subgroups are smaller but important, stratified sampling ensures that they are not overlooked in the sample.\n",
    "- **Improved Precision**: When the variability within strata is lower than the variability in the overall population, stratified sampling can result in more precise estimates of population parameters.\n",
    "- **Market Research**: It can be used in market research to ensure all customer segments (e.g., by age or income) are represented in the sample.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified Sampling on dataset\n",
    "\n",
    "def stratifiedSampling(data):\n",
    "# Step 1: Define the strata (we will stratify by 'Gender')\n",
    "    maleData = [data['Age'][i] for i in range(len(data['Gender'])) if data['Gender'][i] == 'Male']\n",
    "    femaleData = [data['Age'][i] for i in range(len(data['Gender'])) if data['Gender'][i] == 'Female']\n",
    "\n",
    "# Step 2: Define the total sample size\n",
    "    totalSampleSize = 6\n",
    "\n",
    "# Step 3: Calculate the number of samples for each stratum (proportional allocation)\n",
    "    maleSampleSize = int(np.rint(len(maleData) / len(data['Gender']) * totalSampleSize))\n",
    "    female_sample_size = totalSampleSize - maleSampleSize  # Remaining samples for females\n",
    "\n",
    "# Step 4: Randomly sample from each stratum\n",
    "    maleSample = np.random.choice(maleData, size=maleSampleSize, replace=False)\n",
    "    femaleSample = np.random.choice(femaleData, size=female_sample_size, replace=False)\n",
    "\n",
    "# Step 5: Combine the samples from each stratum\n",
    "    stratifiedSample = np.concatenate((maleSample, femaleSample))\n",
    "\n",
    "    print(\"Stratified Sample:\")\n",
    "    print(stratifiedSample)\n",
    "    return stratifiedSample\n",
    "# Sample dataset (replace with your actual dataset)\n",
    "data = {\n",
    "    'Age': [25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80],\n",
    "    'Gender': ['Male', 'Female', 'Male', 'Female', 'Male', 'Female', 'Male', 'Female', 'Male', 'Female', 'Male', 'Female'],\n",
    "}\n",
    "stratifiedSampling(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Maximum Likelihood Estimation (MLE)**\n",
    "\n",
    "**Maximum Likelihood Estimation (MLE)** is a method for estimating the parameters of a statistical model. The idea is to find the parameter values that maximize the likelihood of observing the given sample data.\n",
    "\n",
    "In MLE, we define the **likelihood function** for the data given the parameters of the model. Then, we seek the parameter values that maximize this likelihood.\n",
    "\n",
    "### **Steps for Maximum Likelihood Estimation**:\n",
    "1. **Define the Likelihood Function**: Given a statistical model, the likelihood function represents the probability of observing the sample data as a function of the model parameters.\n",
    "2. **Maximize the Likelihood**: We estimate the parameters by finding the values that maximize the likelihood function. Often, we work with the **log-likelihood** because it simplifies the maximization process.\n",
    "3. **Estimate the Parameters**: The values of the parameters that maximize the likelihood are the MLE estimates.\n",
    "\n",
    "---\n",
    "\n",
    "### **Mathematical Formulation of MLE**:\n",
    "\n",
    "Suppose we have a dataset $ X = \\{x_1, x_2, ..., x_n\\} $, and we are trying to estimate the parameters $ \\theta $ of a probability distribution $ f(x|\\theta) $.\n",
    "\n",
    "1. **Likelihood Function**:\n",
    "   The likelihood function $ L(\\theta) $ is the joint probability of observing the data:\n",
    "   $\n",
    "   L(\\theta) = \\prod_{i=1}^{n} f(x_i|\\theta)\n",
    "   $\n",
    "\n",
    "2. **Log-Likelihood Function**:\n",
    "   The log-likelihood is the logarithm of the likelihood function, which is easier to work with:\n",
    "   $\n",
    "   \\ell(\\theta) = \\log L(\\theta) = \\sum_{i=1}^{n} \\log f(x_i|\\theta)\n",
    "   $\n",
    "\n",
    "3. **Maximizing the Likelihood**:\n",
    "   To find the maximum likelihood estimate, we differentiate the log-likelihood with respect to $ \\theta $ and set it equal to zero:\n",
    "   $\n",
    "   \\frac{d}{d\\theta} \\ell(\\theta) = 0\n",
    "   $\n",
    "   Solving this equation gives us the MLE for the parameter $ \\theta $.\n",
    "\n",
    "---\n",
    "\n",
    "### **MLE Example: Estimating the Parameters of a Normal Distribution**\n",
    "\n",
    "For simplicity, let's consider the case of estimating the parameters of a **Normal Distribution** with unknown mean $ \\mu $ and standard deviation $ \\sigma $ using MLE.\n",
    "\n",
    "- **Normal Distribution**: The probability density function (PDF) for a normal distribution is:\n",
    "  $\n",
    "  f(x|\\mu, \\sigma) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right)\n",
    "  $\n",
    "\n",
    "- **Likelihood Function**:\n",
    "  For a dataset $ X = \\{x_1, x_2, ..., x_n\\} $, the likelihood function is:\n",
    "  $\n",
    "  L(\\mu, \\sigma) = \\prod_{i=1}^{n} \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left(-\\frac{(x_i - \\mu)^2}{2\\sigma^2}\\right)\n",
    "  $\n",
    "\n",
    "- **Log-Likelihood**:\n",
    "  The log-likelihood function is:\n",
    "  $\n",
    "  \\ell(\\mu, \\sigma) = -n\\log(\\sigma) - \\frac{n}{2}\\log(2\\pi) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (x_i - \\mu)^2\n",
    "  $\n",
    "\n",
    "- **Maximizing the Log-Likelihood**:\n",
    "  To estimate the parameters $ \\mu $ and $ \\sigma $, we maximize the log-likelihood. The solutions are:\n",
    "  - $ \\hat{\\mu} = \\frac{1}{n} \\sum_{i=1}^{n} x_i $ (Sample mean)\n",
    "  - $ \\hat{\\sigma} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\hat{\\mu})^2} $ (Sample standard deviation)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **Conclusion**:\n",
    "\n",
    "Using MLE, we can estimate the parameters of a statistical model, such as the **mean** and **standard deviation** of a normal distribution, by finding the values that maximize the likelihood of the observed data.\n",
    "\n",
    "In this example, we applied MLE to estimate the parameters of a normal distribution, but the same concept can be applied to other distributions by defining the likelihood function for those distributions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate population parameters via maximum likelihood estimation\n",
    "\n",
    "def populationMaximumLikelihoodEstimation(data):\n",
    "    MLEHat = np.mean(data)\n",
    "    MLEStandardDeviation = np.std(  data, ddof=0)\n",
    "    print(f\"Estimated Mean (mu): {MLEHat}\")\n",
    "    print(f\"Estimated Standard Deviation (sigma): {MLEStandardDeviation}\")\n",
    "\n",
    "data = np.array([50, 52, 53, 55, 48, 49, 50, 51, 55, 54])\n",
    "populationMaximumLikelihoodEstimation(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Bootstrap Sampling for Confidence Interval**\n",
    "\n",
    "**Bootstrap sampling** is a powerful statistical technique that involves resampling with replacement from a given dataset to estimate the sampling distribution of a statistic. This method can be used to estimate confidence intervals for any statistic (mean, median, variance, etc.).\n",
    "\n",
    "### **Steps for Bootstrap Sampling to Compute Confidence Interval**:\n",
    "\n",
    "1. **Resample**: Randomly draw samples from the original dataset with replacement. The number of samples drawn is the same as the size of the original dataset.\n",
    "2. **Calculate Statistic**: For each resample, compute the statistic of interest (e.g., mean, median, etc.).\n",
    "3. **Repeat**: Repeat the resampling process many times (typically 1,000 or more) to build a distribution of the statistic.\n",
    "4. **Compute Confidence Interval**: From the distribution of the statistic, compute the confidence interval by finding the appropriate percentiles.\n",
    "\n",
    "The most commonly used confidence intervals are **percentile-based** intervals. For example, to compute a 95% confidence interval, we would take the 2.5th and 97.5th percentiles of the resampled statistic values.\n",
    "\n",
    "---\n",
    "\n",
    "### **Mathematical Formulation**:\n",
    "\n",
    "Given a sample $ X = \\{x_1, x_2, ..., x_n\\} $, we want to estimate the confidence interval for a statistic $ \\theta $ (e.g., the mean).\n",
    "\n",
    "1. **Resample** the data with replacement to create $ B $ bootstrap samples $ X^* = \\{X_1^*, X_2^*, ..., X_B^*\\} $.\n",
    "2. **Compute the statistic** $ \\hat{\\theta}^* $ for each resample.\n",
    "3. **Determine the percentiles** of the distribution of $ \\hat{\\theta}^* $.\n",
    "\n",
    "For a 95% confidence interval:\n",
    "$\n",
    "\\left[\\text{percentile}(2.5\\%), \\text{percentile}(97.5\\%)\\right]\n",
    "$\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Points**:\n",
    "\n",
    "1. **Resampling with Replacement**: In each iteration, we sample with replacement from the dataset to create a bootstrap sample.\n",
    "2. **Distribution of the Statistic**: By repeating the sampling process many times (typically 1,000 or more), we create a distribution of the statistic (mean in this case).\n",
    "3. **Percentile Confidence Interval**: The confidence interval is obtained by taking the appropriate percentiles of the bootstrap distribution.\n",
    "\n",
    "---\n",
    "\n",
    "### **Applications of Bootstrap Sampling**:\n",
    "- **Non-parametric Estimation**: It doesn't assume a particular distribution for the data and can be used for almost any statistic.\n",
    "- **Confidence Intervals**: It can be used to estimate confidence intervals for complex statistics that are difficult to handle analytically.\n",
    "- **Model Evaluation**: It's commonly used in machine learning to assess the performance of models, especially when sample sizes are small.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bootstrap sampling to compute confidence interval\n",
    "\n",
    "\n",
    "# Function to perform bootstrap sampling and compute confidence interval\n",
    "def bootstrapConfidenceInterval(data, samples, confidenceLevel):\n",
    "    # Array to store the mean of each bootstrap sample\n",
    "    bootstrapMeans = []\n",
    "\n",
    "    # Perform bootstrap sampling\n",
    "    for _ in range(samples):\n",
    "        sample = np.random.choice(data, size=len(data), replace=True)  # Resampling with replacement\n",
    "        bootstrapMeans.append(np.mean(sample))  # Compute the statistic (mean in this case)\n",
    "\n",
    "    # Calculate the confidence interval\n",
    "    lowerPercentile = (1 - confidenceLevel) / 2 * 100\n",
    "    upperPercentile = (1 + confidenceLevel) / 2 * 100\n",
    "\n",
    "    # Compute the percentiles for the confidence interval\n",
    "    lowerBound = np.percentile(bootstrapMeans, lowerPercentile)\n",
    "    upperBound = np.percentile(bootstrapMeans, upperPercentile)\n",
    "    print(f\"{int(confidenceLevel * 100)}% Confidence Interval for the Mean: ({lowerBound}, {upperBound})\")\n",
    "    return lowerBound, upperBound\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Sample dataset (replace with your actual dataset)\n",
    "data = np.array([50, 52, 53, 55, 48, 49, 50, 51, 55, 54])\n",
    "\n",
    "# Number of bootstrap samples\n",
    "samples = 1000\n",
    "\n",
    "# Confidence level\n",
    "confidenceLevel = 0.95\n",
    "\n",
    "# Compute the 95% confidence interval for the mean\n",
    "bootstrapConfidenceInterval(data, samples, confidenceLevel)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Computing Bias and Variance of an Estimator Using Simulation**\n",
    "\n",
    "In statistical estimation, the **bias** and **variance** of an estimator provide important information about how well the estimator performs in approximating the true population parameter.\n",
    "\n",
    "- **Bias** of an estimator $ \\hat{\\theta} $ is the difference between the expected value of the estimator and the true value of the parameter:\n",
    "  $\n",
    "  \\text{Bias}(\\hat{\\theta}) = E[\\hat{\\theta}] - \\theta\n",
    "  $\n",
    "  If the bias is zero, the estimator is said to be **unbiased**.\n",
    "\n",
    "- **Variance** of an estimator measures how much the estimator varies from its expected value:\n",
    "  $\n",
    "  \\text{Var}(\\hat{\\theta}) = E[(\\hat{\\theta} - E[\\hat{\\theta}])^2]\n",
    "  $\n",
    "\n",
    "We can estimate both bias and variance using **simulation**. In a simulation, we repeat the process of estimating the parameter from multiple random samples and then compute the average bias and variance of the estimators.\n",
    "\n",
    "### **Steps to Compute Bias and Variance Using Simulation**:\n",
    "\n",
    "1. **Define the True Parameter**: Specify the true population parameter $ \\theta $ that you want to estimate (e.g., mean, variance).\n",
    "2. **Simulate Samples**: Generate multiple random samples from a population with the true parameter.\n",
    "3. **Estimate the Parameter**: For each sample, compute the estimator (e.g., sample mean, sample variance).\n",
    "4. **Compute Bias**: Calculate the difference between the average of the estimators and the true parameter.\n",
    "5. **Compute Variance**: Calculate the variance of the estimators.\n",
    "\n",
    "\n",
    "### **Explanation of Results**:\n",
    "\n",
    "- **Bias**: \n",
    "  - If the bias is close to 0, the estimator is approximately unbiased. In this case, the bias is very small (close to zero), indicating that the sample mean is an **unbiased estimator** of the population mean.\n",
    "  \n",
    "- **Variance**: \n",
    "  - The variance indicates how much the sample mean varies from the true population mean across different simulations. A smaller variance means that the estimator is more stable and reliable.\n",
    "\n",
    "---\n",
    "\n",
    "### **General Steps to Apply This Method**:\n",
    "\n",
    "1. **Choose an Estimator**: You can apply this approach to any estimator (e.g., sample median, sample variance) by defining the true parameter and estimating it from the sample.\n",
    "2. **Repeat the Process**: Run the simulation multiple times (e.g., 1000 simulations) to get a reliable estimate of the bias and variance.\n",
    "3. **Calculate the Metrics**: Use the average of the estimator values to compute the bias, and the variance formula to compute the estimator's variability.\n",
    "\n",
    "---\n",
    "\n",
    "### **Applications of Bias and Variance Estimation**:\n",
    "\n",
    "- **Model Evaluation**: Understanding the bias and variance of estimators helps in evaluating the performance of machine learning models, especially when considering trade-offs like **bias-variance tradeoff**.\n",
    "- **Simulation Studies**: Used in research to simulate and evaluate the properties of statistical estimators before applying them to real-world data.\n",
    "- **Algorithm Comparison**: By simulating different estimators, you can choose the one with the smallest bias and variance for more accurate and stable parameter estimation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the bias and variance of estimator using simulation\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# Function to calculate bias and variance of the sample mean estimator\n",
    "def biasVarianceSimulation(trueMean, trueStandardDeviation, sampleSize, nSimulations):\n",
    "    # Store the sample means\n",
    "    sampleMeans = []\n",
    "\n",
    "    for _ in range(nSimulations):\n",
    "        # Simulate a random sample from the population\n",
    "        sample = np.random.normal(trueMean, trueStandardDeviation, sampleSize)\n",
    "\n",
    "        # Estimate the parameter (mean of the sample)\n",
    "        sampleMean = np.mean(sample)\n",
    "        sampleMeans.append(sampleMean)\n",
    "\n",
    "    # Compute the bias of the estimator\n",
    "    estimatedMean = np.mean(sampleMeans)\n",
    "    bias = estimatedMean - trueMean\n",
    "\n",
    "    # Compute the variance of the estimator\n",
    "    variance = np.var(sampleMeans)\n",
    "\n",
    "    return bias, variance\n",
    "\n",
    "\n",
    "# Set the true population mean (parameter)\n",
    "trueMean = 50\n",
    "trueStd = 10\n",
    "sampleSize = 30\n",
    "numSimulations = 1000\n",
    "# Run the simulation\n",
    "bias, variance = biasVarianceSimulation(trueMean, trueStd, sampleSize, numSimulations)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Bias of the sample mean estimator: {bias}\")\n",
    "print(f\"Variance of the sample mean estimator: {variance}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Verifying the Law of Large Numbers (LLN)**\n",
    "\n",
    "The **Law of Large Numbers (LLN)** is a fundamental theorem in probability theory and statistics. It states that as the size of a sample increases, the sample mean (or average) of the observed values gets closer to the true population mean.\n",
    "\n",
    "There are two main versions of LLN:\n",
    "1. **Weak Law of Large Numbers (WLLN)**: This states that for a sequence of independent and identically distributed (i.i.d.) random variables, the sample mean converges in probability to the expected value (population mean) as the sample size increases.\n",
    "2. **Strong Law of Large Numbers (SLLN)**: This states that the sample mean almost surely converges to the population mean as the sample size increases.\n",
    "\n",
    "The idea behind verifying LLN is to simulate how the sample mean approaches the true population mean as the sample size increases.\n",
    "\n",
    "### **Steps to Verify the Law of Large Numbers**:\n",
    "\n",
    "1. **Define the True Population Mean**: Choose a probability distribution with a known mean (e.g., normal distribution with known mean).\n",
    "2. **Generate Random Samples**: Simulate random samples from the chosen distribution.\n",
    "3. **Compute Sample Mean**: For increasing sample sizes, compute the sample mean at each step.\n",
    "4. **Observe Convergence**: Plot or track how the sample mean converges to the true population mean as the sample size increases.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **Mathematical Formulation of LLN**:\n",
    "\n",
    "- Let $ X_1, X_2, ..., X_n $ be a sequence of i.i.d. random variables with a population mean $ \\mu = E[X_i] $.\n",
    "- The sample mean $ \\hat{\\mu}_n $ for a sample of size $ n $ is:\n",
    "  $\n",
    "  \\hat{\\mu}_n = \\frac{1}{n} \\sum_{i=1}^{n} X_i\n",
    "  $\n",
    "- The Law of Large Numbers states that as $ n \\to \\infty $:\n",
    "  $\n",
    "  \\hat{\\mu}_n \\xrightarrow{P} \\mu\n",
    "  $\n",
    "  This means that as the sample size increases, the sample mean $ \\hat{\\mu}_n $ converges to the true mean $ \\mu $ in probability.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step-by-Step Guide to Verify LLN Using Simulation**:\n",
    "\n",
    "1. **Choose a Distribution**: Pick a distribution, such as the **normal distribution**, with known parameters (e.g., mean = 50, standard deviation = 10).\n",
    "\n",
    "2. **Simulate Random Samples**: Generate random samples from this distribution using a fixed random seed for reproducibility.\n",
    "\n",
    "3. **Calculate Sample Mean**: As you simulate increasing sample sizes, compute the sample mean for each sample size.\n",
    "\n",
    "4. **Observe Convergence**: Track how the sample mean approaches the true mean.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### **How It Verifies LLN**:\n",
    "\n",
    "- **Convergence of Sample Mean**: The sample mean fluctuates for small sample sizes, but as the sample size grows, it gets closer to the true mean. This demonstrates the **convergence** predicted by the Law of Large Numbers.\n",
    "  \n",
    "- **Accuracy of Estimator**: With larger sample sizes, the sample mean becomes a better estimate of the population mean.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Takeaways**:\n",
    "\n",
    "1. **LLN Convergence**: As the sample size increases, the sample mean converges to the true population mean. This confirms the Law of Large Numbers.\n",
    "2. **Simulation**: The simulation method allows us to empirically observe the behavior of the sample mean.\n",
    "3. **Practical Implication**: In practice, we can rely on large samples to accurately estimate population parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the Law of Large Numbers\n",
    "\n",
    "# Function to simulate LLN\n",
    "def verifyLawOfLargeNumber(trueMean, maxSampleSize):\n",
    "    sampleMeans = []\n",
    "    \n",
    "    for n in range(1, maxSampleSize + 1):\n",
    "        # Generate a random sample of size n\n",
    "        sample = np.random.normal(trueMean, trueStd, n)\n",
    "        \n",
    "        # Compute the sample mean\n",
    "        sampleMean = np.mean(sample)\n",
    "        sampleMeans.append(sampleMean)\n",
    "    \n",
    "    return sampleMeans\n",
    "trueMean = 50\n",
    "trueStd = 10\n",
    "maxSampleSize = 10000\n",
    "# Run the simulation\n",
    "sampleMeans = verifyLawOfLargeNumber(trueMean, maxSampleSize)\n",
    "\n",
    "# Print some results (you can remove the visualization code if not needed)\n",
    "print(f\"True mean: {trueMean}\")\n",
    "print(f\"Sample mean for large sample size: {sampleMeans[-1]}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Simulating and Computing the Impact of Sample Size on Estimation Accuracy**\n",
    "\n",
    "In statistics, the accuracy of an estimator improves as the sample size increases. This is a fundamental concept, as larger samples tend to produce more reliable estimates of population parameters.\n",
    "\n",
    "To simulate and compute the impact of sample size on estimation accuracy, we typically look at how the **sample mean** (or any other estimator) approaches the **true population mean** as the sample size increases.\n",
    "\n",
    "### **Steps to Simulate and Compute Impact of Sample Size on Estimation Accuracy**:\n",
    "\n",
    "1. **Define the True Population Parameter**: Choose a distribution with known parameters (e.g., population mean and standard deviation).\n",
    "2. **Generate Random Samples**: Simulate random samples of increasing sizes.\n",
    "3. **Estimate the Parameter**: Compute the estimator (e.g., sample mean) for each sample.\n",
    "4. **Compute Estimation Accuracy**: For each sample size, compute the **bias** and **mean squared error (MSE)** to measure how well the estimator approximates the true parameter.\n",
    "\n",
    "---\n",
    "\n",
    "### **Mathematical Formulation**:\n",
    "\n",
    "1. **Bias** of an estimator $ \\hat{\\theta}_n $ for sample size $ n $:\n",
    "   $\n",
    "   \\text{Bias}(\\hat{\\theta}_n) = E[\\hat{\\theta}_n] - \\theta\n",
    "   $\n",
    "   Where $ \\theta $ is the true population parameter.\n",
    "\n",
    "2. **Mean Squared Error (MSE)** of an estimator $ \\hat{\\theta}_n $ is a measure of the average of the squared differences between the estimator and the true parameter:\n",
    "   $\n",
    "   \\text{MSE}(\\hat{\\theta}_n) = E[(\\hat{\\theta}_n - \\theta)^2]\n",
    "   $\n",
    "   MSE combines both **variance** and **bias**:\n",
    "   $\n",
    "   \\text{MSE}(\\hat{\\theta}_n) = \\text{Var}(\\hat{\\theta}_n) + \\text{Bias}^2(\\hat{\\theta}_n)\n",
    "   $\n",
    "\n",
    "3. **Accuracy of Estimation**: As the sample size $ n $ increases, both the **bias** and **MSE** decrease, demonstrating that larger samples yield more accurate estimates.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step-by-Step Guide for Simulation**:\n",
    "\n",
    "1. **Choose a Distribution**: Select a distribution, e.g., a **normal distribution** with known mean and standard deviation.\n",
    "2. **Vary Sample Sizes**: Choose different sample sizes, e.g., 10, 100, 1000, etc.\n",
    "3. **Simulate Samples**: For each sample size, simulate random samples and compute the sample mean.\n",
    "4. **Compute Accuracy Metrics**:\n",
    "   - **Bias**: Calculate the difference between the true mean and the sample mean.\n",
    "   - **Mean Squared Error (MSE)**: Calculate the MSE for each sample size.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### **Interpretation of Results**:\n",
    "\n",
    "1. **Bias**: As the sample size increases, the bias tends to decrease, indicating that the estimator becomes more accurate.\n",
    "   \n",
    "2. **MSE**: The **Mean Squared Error** also decreases as the sample size increases, showing that larger samples yield more precise estimates of the true population mean.\n",
    "\n",
    "   - For smaller sample sizes, the sample mean fluctuates more, leading to higher MSE.\n",
    "   - As the sample size increases, the estimator stabilizes, and MSE decreases.\n",
    "\n",
    "### **Key Takeaways**:\n",
    "\n",
    "- **Larger Sample Sizes = More Accurate Estimation**: As the sample size increases, the sample mean (or any estimator) becomes a more accurate estimate of the true population parameter.\n",
    "- **Bias and MSE**: Both bias and MSE decrease with increasing sample size, confirming that larger samples are more reliable.\n",
    "- **Statistical Confidence**: This shows that with larger sample sizes, we can have greater confidence in our estimates.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate and compute the impact of the sample size  on estimation accuracy\n",
    "\n",
    "\n",
    "# Function to simulate and compute bias and MSE for different sample sizes\n",
    "def sampleSizeImpactSimulation(trueMean, trueStandardDeviation, sampleSizes, nSimulations=1000):\n",
    "    biasValues = []\n",
    "    meanEstimatedValues = []\n",
    "\n",
    "    # Run simulations for each sample size\n",
    "    for n in sampleSizes:\n",
    "        sampleMeans = []\n",
    "        \n",
    "        # Perform multiple simulations for the current sample size\n",
    "        for _ in range(nSimulations):\n",
    "            sample = np.random.normal(trueMean, trueStandardDeviation, n)  # Generate random sample\n",
    "            sample_mean = np.mean(sample)  # Calculate sample mean\n",
    "            sampleMeans.append(sample_mean)\n",
    "        \n",
    "        # Compute the bias (difference between average sample mean and true mean)\n",
    "        estimatedMean = np.mean(sampleMeans)\n",
    "        bias = estimatedMean - trueMean\n",
    "        \n",
    "        # Compute the MSE (mean squared error)\n",
    "        mse = np.mean((np.array(sampleMeans) - trueMean) ** 2)\n",
    "        \n",
    "        # Store results\n",
    "        biasValues.append(bias)\n",
    "        meanEstimatedValues.append(mse)\n",
    "    \n",
    "    return biasValues, meanEstimatedValues\n",
    "\n",
    "\n",
    "# Define the true population parameters\n",
    "trueMean = 50\n",
    "trueStd = 10\n",
    "\n",
    "# Define the maximum sample sizes to test\n",
    "sampleSizes = [10, 50, 100, 500, 1000, 5000]\n",
    "# Run the simulation for different sample sizes\n",
    "biasValues, mse_values = sampleSizeImpactSimulation(trueMean, trueStd, sampleSizes)\n",
    "\n",
    "# Print the results\n",
    "for i, n in enumerate(sampleSizes):\n",
    "    print(f\"Sample Size: {n}\")\n",
    "    print(f\"  Bias: {biasValues[i]}\")\n",
    "    print(f\"  MSE: {mse_values[i]}\")\n",
    "    print(\"-\" * 30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Standard Error of the Sample Mean**\n",
    "\n",
    "The **Standard Error of the Sample Mean (SE)** measures the variability of the sample mean as an estimate of the population mean. It quantifies the uncertainty in the sample mean due to random sampling.\n",
    "\n",
    "---\n",
    "\n",
    "### **Formula for Standard Error**:\n",
    "\n",
    "The Standard Error is calculated using the formula:\n",
    "\n",
    "$\n",
    "SE = \\frac{\\sigma}{\\sqrt{n}}\n",
    "$\n",
    "\n",
    "Where:\n",
    "- $ \\sigma $: Population standard deviation (if known) or sample standard deviation (if population standard deviation is unknown).\n",
    "- $ n $: Sample size.\n",
    "\n",
    "---\n",
    "\n",
    "### **Steps to Compute Standard Error**:\n",
    "\n",
    "1. **Determine the Standard Deviation**:\n",
    "   - If the population standard deviation $ \\sigma $ is known, use it directly.\n",
    "   - If not, estimate it using the sample standard deviation $ s $, calculated as:\n",
    "     $\n",
    "     s = \\sqrt{\\frac{\\sum_{i=1}^n (x_i - \\bar{x})^2}{n-1}}\n",
    "     $\n",
    "     where $ \\bar{x} $ is the sample mean, and $ x_i $ are the data points.\n",
    "\n",
    "2. **Determine the Sample Size $ n $**:\n",
    "   - Count the number of observations in the sample.\n",
    "\n",
    "3. **Calculate the Standard Error**:\n",
    "   - Plug the values of $ s $ (or $ \\sigma $) and $ n $ into the formula:\n",
    "     $\n",
    "     SE = \\frac{s}{\\sqrt{n}}\n",
    "     $\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Insights**:\n",
    "\n",
    "1. **Smaller Standard Error**:\n",
    "   - A smaller $ SE $ indicates that the sample mean is a more precise estimate of the population mean.\n",
    "   - $ SE $ decreases as the sample size $ n $ increases.\n",
    "\n",
    "2. **Variability**:\n",
    "   - If the sample standard deviation $ s $ is large, $ SE $ will also be larger, indicating greater variability in the data.\n",
    "\n",
    "3. **Use in Confidence Intervals**:\n",
    "   - The $ SE $ is often used to construct confidence intervals for the population mean:\n",
    "     $\n",
    "     \\text{Confidence Interval} = \\bar{x} \\pm z \\cdot SE\n",
    "     $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Standard error for sample mean\n",
    "\n",
    "# Function to calculate standard error\n",
    "def standardError(sample):\n",
    "    n = len(sample)  # Sample size\n",
    "    sampleStandardDeviation = np.std(sample, ddof=1)  # Sample standard deviation (ddof=1 for unbiased estimate)\n",
    "    standardError = sampleStandardDeviation / np.sqrt(n)  # Standard Error formula\n",
    "    return standardError\n",
    "\n",
    "# Example usage\n",
    "data = [12, 15, 14, 10, 13, 15, 11, 14, 13, 12]  # Example dataset\n",
    "stdError = standardError(data)\n",
    "\n",
    "print(f\"Sample Data: {data}\")\n",
    "print(f\"Standard Error: {stdError}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -- Hypothesis Testing --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **One-Sample t-Test**\n",
    "\n",
    "A **one-sample t-test** is used to determine whether the mean of a sample differs significantly from a known or hypothesized value.\n",
    "\n",
    "---\n",
    "\n",
    "### **Mathematical Formulation**\n",
    "\n",
    "1. **Null Hypothesis ($ H_0 $)**: The sample mean ($ \\bar{x} $) is equal to the hypothesized value ($ \\mu_0 $).\n",
    "   $\n",
    "   H_0: \\mu = \\mu_0\n",
    "   $\n",
    "\n",
    "2. **Alternative Hypothesis ($ H_1 $)**: The sample mean ($ \\bar{x} $) is not equal to ($ \\mu_0 $) (two-tailed), or is greater/less than ($ \\mu_0 $) (one-tailed).\n",
    "\n",
    "3. **Test Statistic** ($ t $):\n",
    "   $\n",
    "   t = \\frac{\\bar{x} - \\mu_0}{SE}\n",
    "   $\n",
    "   Where:\n",
    "   - $ \\bar{x} $: Sample mean\n",
    "   - $ \\mu_0 $: Known or hypothesized value\n",
    "   - $ SE $: Standard error of the sample mean:\n",
    "     $\n",
    "     SE = \\frac{s}{\\sqrt{n}}\n",
    "     $\n",
    "     - $ s $: Sample standard deviation\n",
    "     - $ n $: Sample size\n",
    "\n",
    "4. **Degrees of Freedom (df)**:\n",
    "   $\n",
    "   df = n - 1\n",
    "   $\n",
    "\n",
    "5. **p-value**: Compare the test statistic to the t-distribution to compute the p-value.\n",
    "\n",
    "---\n",
    "\n",
    "### **Steps for Performing a One-Sample t-Test**\n",
    "\n",
    "1. **Formulate the Hypotheses**:\n",
    "   - Null hypothesis ($ H_0 $): Sample mean equals the hypothesized value.\n",
    "   - Alternative hypothesis ($ H_1 $): Sample mean differs from the hypothesized value.\n",
    "\n",
    "2. **Compute the t-statistic**:\n",
    "   - Calculate the sample mean, sample standard deviation, and standard error.\n",
    "   - Plug these into the formula for the t-statistic.\n",
    "\n",
    "3. **Determine the p-value**:\n",
    "   - Use the t-distribution with $ n-1 $ degrees of freedom to compute the p-value.\n",
    "\n",
    "4. **Compare the p-value to the significance level ($ \\alpha $)**:\n",
    "   - If $ p \\leq \\alpha $, reject the null hypothesis.\n",
    "   - If $ p > \\alpha $, fail to reject the null hypothesis.\n",
    "\n",
    "---\n",
    "\n",
    "### **Interpretation of Results**\n",
    "\n",
    "1. **Sample Mean**: The mean of the given sample.\n",
    "2. **t-Statistic**: The test statistic calculated based on the sample data.\n",
    "3. **p-Value**:\n",
    "   - If $ p \\leq \\alpha $: Reject $ H_0 $. The sample mean significantly differs from the hypothesized mean.\n",
    "   - If $ p > \\alpha $: Fail to reject $ H_0 $. The sample mean does not significantly differ from the hypothesized mean.\n",
    "\n",
    "4. **Result**: For the example, since $ p > 0.05 $, we fail to reject the null hypothesis, meaning the sample mean does not significantly differ from 13.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a one sample t-test to check if a sample mean differ from a known value\n",
    "\n",
    "\n",
    "def oneSampleTTest(sample, hypothesizedMean, alpha=0.05)->dict:\n",
    "    # Step 1: Calculate sample statistics\n",
    "    sampleMean = np.mean(sample)\n",
    "    sampleStd = np.std(sample, ddof=1)  # Unbiased standard deviation\n",
    "    n = len(sample)\n",
    "    standardError = sampleStd / np.sqrt(n)\n",
    "\n",
    "    # Step 2: Compute the t-statistic\n",
    "    tStatistic = (sampleMean - hypothesizedMean) / standardError\n",
    "\n",
    "    # Step 3: Compute degrees of freedom\n",
    "    degreesOfFreedom = n - 1\n",
    "\n",
    "    # Step 4: Compute the p-value (two-tailed test)\n",
    "    pValue = 2 * sciStats.t.sf(abs(tStatistic), df=degreesOfFreedom)\n",
    "\n",
    "    # Step 5: Compare the p-value with the significance level\n",
    "    if pValue < alpha:\n",
    "        result = \"Reject the null hypothesis (significant difference)\"\n",
    "    else:\n",
    "        result = \"Fail to reject the null hypothesis (no significant difference)\"\n",
    "    \n",
    "    # Return results\n",
    "    return {\n",
    "        \"Sample Mean\": sampleMean,\n",
    "        \"T-Statistic\": tStatistic,\n",
    "        \"P-Value\": pValue,\n",
    "        \"Result\": result\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "data = [12, 15, 14, 10, 13, 15, 11, 14, 13, 12]\n",
    "hypothesizedMean = 13\n",
    "alpha = 0.05\n",
    "\n",
    "testResult = oneSampleTTest(data, hypothesizedMean, alpha)\n",
    "\n",
    "# Print the results\n",
    "for key, value in testResult.items():\n",
    "    print(f\"{key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Two-Sample t-Test**\n",
    "\n",
    "The **two-sample t-test** is used to compare the means of two independent datasets to determine whether there is a significant difference between them.\n",
    "\n",
    "---\n",
    "\n",
    "### **Mathematical Formulation**\n",
    "\n",
    "1. **Hypotheses**:\n",
    "   - **Null Hypothesis ($H_0$)**: The means of the two datasets are equal.\n",
    "     $\n",
    "     H_0: \\mu_1 = \\mu_2\n",
    "     $\n",
    "   - **Alternative Hypothesis ($H_1$)**: The means of the two datasets are not equal (two-tailed test) or one is greater/less than the other (one-tailed test).\n",
    "     $\n",
    "     H_1: \\mu_1 \\neq \\mu_2 \\quad \\text{(two-tailed)}\n",
    "     $\n",
    "\n",
    "2. **Test Statistic ($ t $)**:\n",
    "   $\n",
    "   t = \\frac{\\bar{x}_1 - \\bar{x}_2}{\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}}\n",
    "   $\n",
    "   Where:\n",
    "   - $ \\bar{x}_1, \\bar{x}_2 $: Means of the two samples\n",
    "   - $ s_1^2, s_2^2 $: Variances of the two samples\n",
    "   - $ n_1, n_2 $: Sizes of the two samples\n",
    "\n",
    "3. **Degrees of Freedom (df)**:\n",
    "   If sample variances are not assumed to be equal (Welch's t-test):\n",
    "   $\n",
    "   df = \\frac{\\left(\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}\\right)^2}{\\frac{\\left(\\frac{s_1^2}{n_1}\\right)^2}{n_1 - 1} + \\frac{\\left(\\frac{s_2^2}{n_2}\\right)^2}{n_2 - 1}}\n",
    "   $\n",
    "\n",
    "4. **p-value**: Compare the test statistic to the t-distribution to compute the p-value.\n",
    "\n",
    "---\n",
    "\n",
    "### **Steps to Perform Two-Sample t-Test**\n",
    "\n",
    "1. **Formulate Hypotheses**:\n",
    "   - $ H_0 $: The two means are equal.\n",
    "   - $ H_1 $: The two means are not equal (or greater/less than).\n",
    "\n",
    "2. **Compute Sample Statistics**:\n",
    "   - Calculate the means and variances of the two samples.\n",
    "   - Compute the standard error of the difference in means.\n",
    "\n",
    "3. **Compute the t-statistic**:\n",
    "   - Plug the values into the t-statistic formula.\n",
    "\n",
    "4. **Determine Degrees of Freedom**:\n",
    "   - If variances are unequal, use Welch's formula for degrees of freedom.\n",
    "\n",
    "5. **Determine the p-value**:\n",
    "   - Compute the p-value based on the t-statistic and degrees of freedom.\n",
    "\n",
    "6. **Make a Decision**:\n",
    "   - Compare the p-value with the significance level $ \\alpha $ (typically 0.05).\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Insights**\n",
    "\n",
    "1. **Reject $ H_0 $**:\n",
    "   - If $ p < \\alpha $, the means are significantly different.\n",
    "\n",
    "2. **Fail to Reject $ H_0 $**:\n",
    "   - If $ p > \\alpha $, there is no significant difference between the means.\n",
    "\n",
    "3. **Equal vs. Unequal Variances**:\n",
    "   - Use Welchâ€™s t-test (default) if you suspect the variances are unequal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform two sample t-test to compare the mean of two datasets \n",
    "\n",
    "\n",
    "\n",
    "def twoSampleTTest(sample1, sample2, alpha=0.05, equalVariance=False):\n",
    "    # Step 1: Calculate sample statistics\n",
    "    mean1, mean2 = np.mean(sample1), np.mean(sample2)\n",
    "    var1, var2 = np.var(sample1, ddof=1), np.var(sample2, ddof=1)\n",
    "    n1, n2 = len(sample1), len(sample2)\n",
    "\n",
    "    # Step 2: Compute the pooled or un-pooled standard error\n",
    "    if equalVariance:  # Assume equal variances (pooled variance)\n",
    "        pooledVariance = ((n1 - 1) * var1 + (n2 - 1) * var2) / (n1 + n2 - 2)\n",
    "        standardError = np.sqrt(pooledVariance * (1/n1 + 1/n2))\n",
    "        degreesOfFreedom = n1 + n2 - 2\n",
    "    else:  # Unequal variances (Welch's t-test)\n",
    "        standardError = np.sqrt(var1/n1 + var2/n2)\n",
    "        degreesOfFreedom = ((var1/n1 + var2/n2) ** 2) / (\n",
    "            (var1/n1) ** 2 / (n1 - 1) + (var2/n2) ** 2 / (n2 - 1)\n",
    "        )\n",
    "\n",
    "    # Step 3: Compute the t-statistic\n",
    "    tStatistic = (mean1 - mean2) / standardError\n",
    "\n",
    "    # Step 4: Compute the p-value (two-tailed test)\n",
    "    pValue = 2 * sciStats.t.sf(abs(tStatistic), df=degreesOfFreedom)\n",
    "\n",
    "    # Step 5: Compare the p-value with the significance level\n",
    "    if pValue < alpha:\n",
    "        result = \"Reject the null hypothesis (means are significantly different)\"\n",
    "    else:\n",
    "        result = \"Fail to reject the null hypothesis (no significant difference)\"\n",
    "    \n",
    "    # Return results\n",
    "    return {\n",
    "        \"Sample 1 Mean\": mean1,\n",
    "        \"Sample 2 Mean\": mean2,\n",
    "        \"T-Statistic\": tStatistic,\n",
    "        \"P-Value\": pValue,\n",
    "        \"Degrees of Freedom\": degreesOfFreedom,\n",
    "        \"Result\": result\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "sample1 = [12, 15, 14, 10, 13, 15, 11, 14, 13, 12]\n",
    "sample2 = [22, 25, 20, 19, 24, 23, 21, 22, 20, 23]\n",
    "\n",
    "testResult = twoSampleTTest(sample1, sample2, alpha=0.05, equalVariance=False)\n",
    "\n",
    "# Print the results\n",
    "for key, value in testResult.items():\n",
    "    print(f\"{key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Chi-Square Test for Independence**\n",
    "\n",
    "The **chi-square test for independence** determines whether two categorical variables are independent or have an association. It is based on comparing the observed frequencies in a contingency table to the expected frequencies.\n",
    "\n",
    "---\n",
    "\n",
    "### **Mathematical Formulation**\n",
    "\n",
    "1. **Hypotheses**:\n",
    "   - **Null Hypothesis ($H_0$)**: The two variables are independent.\n",
    "   - **Alternative Hypothesis ($H_1$)**: The two variables are dependent (associated).\n",
    "\n",
    "2. **Chi-Square Test Statistic**:\n",
    "   $\n",
    "   \\chi^2 = \\sum \\frac{(O - E)^2}{E}\n",
    "   $\n",
    "   Where:\n",
    "   - $O$: Observed frequency in each cell\n",
    "   - $E$: Expected frequency in each cell, calculated as:\n",
    "     $\n",
    "     E = \\frac{\\text{Row Total} \\times \\text{Column Total}}{\\text{Grand Total}}\n",
    "     $\n",
    "\n",
    "3. **Degrees of Freedom (df)**:\n",
    "   $\n",
    "   df = (r - 1) \\times (c - 1)\n",
    "   $\n",
    "   Where $r$ is the number of rows and $c$ is the number of columns in the contingency table.\n",
    "\n",
    "4. **p-value**: Compare the calculated $ \\chi^2 $ value to the chi-square distribution with the given degrees of freedom to obtain the p-value.\n",
    "\n",
    "---\n",
    "\n",
    "### **Steps to Perform Chi-Square Test**\n",
    "\n",
    "1. **Construct a Contingency Table**:\n",
    "   - A table summarizing the frequencies of two categorical variables.\n",
    "\n",
    "2. **Calculate Expected Frequencies**:\n",
    "   - Use the formula $ E = \\frac{\\text{Row Total} \\times \\text{Column Total}}{\\text{Grand Total}} $ for each cell.\n",
    "\n",
    "3. **Compute the Chi-Square Statistic**:\n",
    "   - Use the formula $ \\chi^2 = \\sum \\frac{(O - E)^2}{E} $ for all cells.\n",
    "\n",
    "4. **Determine the Degrees of Freedom**:\n",
    "   - Use $ df = (r - 1) \\times (c - 1) $.\n",
    "\n",
    "5. **Compute the p-value**:\n",
    "   - Compare the test statistic $ \\chi^2 $ to the chi-square distribution.\n",
    "\n",
    "6. **Make a Decision**:\n",
    "   - If $ p \\leq \\alpha $ (e.g., 0.05), reject the null hypothesis ($H_0$): The variables are dependent.\n",
    "   - If $ p > \\alpha $, fail to reject $H_0$: The variables are independent.\n",
    "\n",
    "---\n",
    "\n",
    "### **Interpretation**\n",
    "\n",
    "1. **Chi-Square Statistic**:\n",
    "   - Represents how much the observed frequencies deviate from the expected frequencies.\n",
    "\n",
    "2. **Degrees of Freedom**:\n",
    "   - The number of independent comparisons available.\n",
    "\n",
    "3. **p-Value**:\n",
    "   - If $ p \\leq \\alpha $: There is evidence to suggest the variables are dependent (associated).\n",
    "   - If $ p > \\alpha $: No evidence to suggest an association; the variables are likely independent.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement and interpret a chi-square test for independence\n",
    "\n",
    "\n",
    "\n",
    "def chiSquareTest(contingency_table, alpha=0.05):\n",
    "    # Step 1: Compute observed frequencies\n",
    "    observed = np.array(contingency_table)\n",
    "\n",
    "    # Step 2: Compute expected frequencies\n",
    "    rowTotals = observed.sum(axis=1).reshape(-1, 1)\n",
    "    columnTotals = observed.sum(axis=0)\n",
    "    grandTotal = observed.sum()\n",
    "    expected = (rowTotals @ columnTotals.reshape(1, -1)) / grandTotal\n",
    "\n",
    "    # Step 3: Compute chi-square statistic\n",
    "    chiSquareStatistic = np.sum((observed - expected) ** 2 / expected)\n",
    "\n",
    "    # Step 4: Degrees of freedom\n",
    "    mRows, nColumns = observed.shape\n",
    "    degreesOfFreedom = (mRows - 1) * (nColumns - 1)\n",
    "\n",
    "    # Step 5: Compute p-value\n",
    "    pValue = 1 - sciStats.chi2.cdf(chiSquareStatistic, degreesOfFreedom)\n",
    "\n",
    "    # Step 6: Hypothesis testing\n",
    "    if pValue < alpha:\n",
    "        result = \"Reject the null hypothesis (variables are dependent)\"\n",
    "    else:\n",
    "        result = \"Fail to reject the null hypothesis (variables are independent)\"\n",
    "\n",
    "    # Return results\n",
    "    return {\n",
    "        \"Chi-Square Statistic\": chiSquareStatistic,\n",
    "        \"Degrees of Freedom\": degreesOfFreedom,\n",
    "        \"P-Value\": pValue,\n",
    "        \"Result\": result\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "contingencyTable = [\n",
    "    [10, 20, 30],  # Row 1: Category 1 of Variable A\n",
    "    [15, 25, 20],  # Row 2: Category 2 of Variable A\n",
    "    [35, 30, 25]   # Row 3: Category 3 of Variable A\n",
    "]\n",
    "\n",
    "testResult = chiSquareTest(contingencyTable, alpha=0.05)\n",
    "\n",
    "# Print the results\n",
    "for key, value in testResult.items():\n",
    "    print(f\"{key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ANOVA (Analysis of Variance)**\n",
    "\n",
    "ANOVA (Analysis of Variance) is used to compare the means of **three or more groups** to determine if at least one group mean is significantly different from the others.\n",
    "\n",
    "---\n",
    "\n",
    "### **Mathematical Formulation**\n",
    "\n",
    "1. **Hypotheses**:\n",
    "   - **Null Hypothesis ($H_0$)**: All group means are equal.\n",
    "     $\n",
    "     H_0: \\mu_1 = \\mu_2 = \\mu_3 = \\ldots = \\mu_k\n",
    "     $\n",
    "   - **Alternative Hypothesis ($H_1$)**: At least one group mean is different.\n",
    "\n",
    "2. **Test Statistic**:\n",
    "   ANOVA is based on the **F-statistic**, which is the ratio of two variances:\n",
    "   $\n",
    "   F = \\frac{\\text{Between-Group Variance}}{\\text{Within-Group Variance}}\n",
    "   $\n",
    "   - **Between-Group Variance**: Variance due to differences between group means.\n",
    "   - **Within-Group Variance**: Variance due to differences within groups.\n",
    "\n",
    "3. **Degrees of Freedom**:\n",
    "   - Between-group: $df_b = k - 1$, where $k$ is the number of groups.\n",
    "   - Within-group: $df_w = N - k$, where $N$ is the total number of observations.\n",
    "\n",
    "4. **p-value**:\n",
    "   - Use the F-statistic and degrees of freedom to compute the p-value.\n",
    "\n",
    "---\n",
    "\n",
    "### **Steps to Perform ANOVA**\n",
    "\n",
    "1. **Formulate Hypotheses**:\n",
    "   - $H_0$: All group means are equal.\n",
    "   - $H_1$: At least one group mean is different.\n",
    "\n",
    "2. **Calculate Group Means and Variances**:\n",
    "   - Compute the mean for each group and the overall mean.\n",
    "\n",
    "3. **Calculate Between-Group Variance**:\n",
    "   - Measure how much the group means differ from the overall mean.\n",
    "\n",
    "4. **Calculate Within-Group Variance**:\n",
    "   - Measure the variability within each group.\n",
    "\n",
    "5. **Compute the F-statistic**:\n",
    "   - $ F = \\frac{\\text{Between-Group Variance}}{\\text{Within-Group Variance}} $.\n",
    "\n",
    "6. **Determine the p-value**:\n",
    "   - Compare the F-statistic to the F-distribution.\n",
    "\n",
    "7. **Make a Decision**:\n",
    "   - If $p \\leq \\alpha$, reject $H_0$: At least one group mean is different.\n",
    "\n",
    "---\n",
    "\n",
    "### **Interpreting Results**\n",
    "\n",
    "1. **Reject $H_0$**:\n",
    "   - If $p \\leq \\alpha$, there is evidence that at least one group mean is different.\n",
    "\n",
    "2. **Fail to Reject $H_0$**:\n",
    "   - If $p > \\alpha$, there is no evidence to suggest significant differences between the group means.\n",
    "\n",
    "---\n",
    "\n",
    "### **Limitations of ANOVA**\n",
    "\n",
    "- ANOVA only tells if there is a significant difference but does not specify which groups differ.\n",
    "- To identify specific group differences, use **post-hoc tests** (e.g., Tukeyâ€™s test).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conduct ANOVA test to compare mean of multiple groups\n",
    "\n",
    "\n",
    "def oneWayANOVA(*groups, alpha=0.05):\n",
    "    # Step 1: Perform ANOVA\n",
    "    fStatistic, pValue = sciStats.f_oneway(*groups)\n",
    "    \n",
    "    # Step 2: Interpret Results\n",
    "    if pValue < alpha:\n",
    "        result = \"Reject the null hypothesis (at least one group mean is different)\"\n",
    "    else:\n",
    "        result = \"Fail to reject the null hypothesis (no significant difference between group means)\"\n",
    "    \n",
    "    return {\n",
    "        \"F-Statistic\": fStatistic,\n",
    "        \"P-Value\": pValue,\n",
    "        \"Result\": result\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "group1 = [15, 20, 25, 30, 35]\n",
    "group2 = [22, 25, 30, 35, 40]\n",
    "group3 = [28, 32, 35, 40, 45]\n",
    "\n",
    "testResult = oneWayANOVA(group1, group2, group3, alpha=0.05)\n",
    "\n",
    "# Print the results\n",
    "for key, value in testResult.items():\n",
    "    print(f\"{key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Permutation Test for Hypothesis Testing**\n",
    "\n",
    "The **permutation test** is a non-parametric statistical method used to test the null hypothesis by comparing the observed test statistic to a distribution of test statistics generated by randomly permuting the data. It is especially useful when the assumptions of parametric tests (e.g., t-test) are not satisfied.\n",
    "\n",
    "---\n",
    "\n",
    "### **Steps of Permutation Test**\n",
    "\n",
    "1. **Define Hypotheses**:\n",
    "   - **Null Hypothesis ($H_0$)**: The two groups have the same distribution (no significant difference between means).\n",
    "   - **Alternative Hypothesis ($H_1$)**: The two groups have different distributions (there is a significant difference between means).\n",
    "\n",
    "2. **Calculate Observed Test Statistic**:\n",
    "   - Compute a metric like the difference in means or medians between the two groups.\n",
    "\n",
    "3. **Generate Permutation Distribution**:\n",
    "   - Combine the two datasets.\n",
    "   - Randomly shuffle (permute) the combined dataset and split it back into two groups.\n",
    "   - Recalculate the test statistic for each permutation.\n",
    "\n",
    "4. **Compute p-value**:\n",
    "   - The p-value is the proportion of permuted test statistics that are as extreme as or more extreme than the observed test statistic.\n",
    "\n",
    "5. **Decision**:\n",
    "   - If $p \\leq \\alpha$ (e.g., 0.05), reject the null hypothesis $H_0$.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Points to Note**\n",
    "\n",
    "1. **Permutation Test Strengths**:\n",
    "   - No assumptions about the distribution of the data.\n",
    "   - Useful for small sample sizes or non-parametric data.\n",
    "\n",
    "2. **Permutation Test Limitations**:\n",
    "   - Computationally expensive for large datasets with many permutations.\n",
    "   - Only applicable when data can be shuffled meaningfully under $H_0$.\n",
    "\n",
    "3. **Interpretation**:\n",
    "   - A small p-value indicates evidence against the null hypothesis ($H_0$).\n",
    "   - If $p > \\alpha$, there is no evidence to suggest the groups are different.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permutation test for Hypothesis\n",
    "\n",
    "\n",
    "def permutationTestHypothesis(group1, group2, nPermutations=1000, alpha=0.05):\n",
    "    # Step 1: Calculate observed test statistic (difference in means)\n",
    "    observedStatistic = np.mean(group1) - np.mean(group2)\n",
    "    \n",
    "    # Step 2: Combine both groups into one dataset\n",
    "    combined = np.concatenate([group1, group2])\n",
    "    \n",
    "    # Step 3: Generate permutation distribution\n",
    "    permutedStatistics = []\n",
    "    for _ in range(nPermutations):\n",
    "        np.random.shuffle(combined)  # Shuffle the combined dataset\n",
    "        permutedGroup1 = combined[:len(group1)]\n",
    "        permutedGroup2 = combined[len(group1):]\n",
    "        permutedStatistic = np.mean(permutedGroup1) - np.mean(permutedGroup2)\n",
    "        permutedStatistics.append(permutedStatistic)\n",
    "    \n",
    "    # Step 4: Calculate p-value\n",
    "    permutedStatistics = np.array(permutedStatistics)\n",
    "    pValue = np.sum(np.abs(permutedStatistics) >= np.abs(observedStatistic)) / nPermutations\n",
    "    \n",
    "    # Step 5: Decision\n",
    "    if pValue <= alpha:\n",
    "        result = \"Reject the null hypothesis (the groups are significantly different)\"\n",
    "    else:\n",
    "        result = \"Fail to reject the null hypothesis (no significant difference between the groups)\"\n",
    "    \n",
    "    return {\n",
    "        \"Observed Statistic\": observedStatistic,\n",
    "        \"P-Value\": pValue,\n",
    "        \"Result\": result\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "group1 = [5.2, 5.8, 6.1, 5.9, 6.3]\n",
    "group2 = [6.5, 6.7, 7.0, 6.8, 7.2]\n",
    "\n",
    "testResult = permutationTestHypothesis(group1, group2, nPermutations=1000, alpha=0.05)\n",
    "\n",
    "# Print the results\n",
    "for key, value in testResult.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Mann-Whitney U Test**\n",
    "\n",
    "The **Mann-Whitney U Test** is a non-parametric test used to determine whether there is a significant difference between two independent groups when the data does not meet the assumptions of a parametric test like the t-test (e.g., non-normality or ordinal data).\n",
    "\n",
    "---\n",
    "\n",
    "### **When to Use**\n",
    "- Data is **non-parametric** or ordinal (not normally distributed).\n",
    "- Two groups are **independent**.\n",
    "- Compares the **medians** or general distribution of two groups.\n",
    "\n",
    "---\n",
    "\n",
    "### **Mathematical Formulation**\n",
    "\n",
    "1. **Hypotheses**:\n",
    "   - **Null Hypothesis ($H_0$)**: The two groups come from the same population (no difference in medians).\n",
    "   - **Alternative Hypothesis ($H_1$)**: The two groups come from different populations (difference in medians).\n",
    "\n",
    "2. **Test Statistic**:\n",
    "   - Based on the ranks of the combined data:\n",
    "     $\n",
    "     U = R_1 - \\frac{n_1(n_1 + 1)}{2}\n",
    "     $\n",
    "     Where:\n",
    "     - $R_1$: Sum of ranks for the first group.\n",
    "     - $n_1$: Number of observations in the first group.\n",
    "\n",
    "3. **p-value**:\n",
    "   - The p-value is derived from the U statistic based on the Mann-Whitney distribution.\n",
    "\n",
    "---\n",
    "\n",
    "### **Steps to Perform the Mann-Whitney U Test**\n",
    "\n",
    "1. **Define Hypotheses**:\n",
    "   - $H_0$: The two groups come from the same distribution.\n",
    "   - $H_1$: The two groups come from different distributions.\n",
    "\n",
    "2. **Rank Data**:\n",
    "   - Combine the two groups, assign ranks to all data points, and compute the rank sums for each group.\n",
    "\n",
    "3. **Calculate the U Statistic**:\n",
    "   - Use the formula for $U$ to compute the test statistic.\n",
    "\n",
    "4. **Compute p-value**:\n",
    "   - Use the U statistic to find the p-value.\n",
    "\n",
    "5. **Decision**:\n",
    "   - If $p \\leq \\alpha$ (e.g., 0.05), reject $H_0$.\n",
    "\n",
    "---\n",
    "\n",
    "### **Interpretation**\n",
    "\n",
    "1. **Reject $H_0$**:\n",
    "   - A small p-value ($p \\leq \\alpha$) indicates that the distributions of the two groups are significantly different.\n",
    "\n",
    "2. **Fail to Reject $H_0$**:\n",
    "   - A large p-value ($p > \\alpha$) suggests there is no evidence to conclude the groups are different.\n",
    "\n",
    "---\n",
    "\n",
    "### **Advantages of Mann-Whitney U Test**\n",
    "\n",
    "1. **Non-parametric**: Does not assume normality.\n",
    "2. **Robust**: Works well for ordinal data or when variances differ between groups.\n",
    "\n",
    "---\n",
    "\n",
    "### **Limitations**\n",
    "\n",
    "1. **Independent Groups**: Assumes the two groups are independent.\n",
    "2. **Equal Shape Distributions**: Assumes the two groups have the same shape under the null hypothesis.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement and interpret a Mann-Whitney U test for non parametric data\n",
    "\n",
    "\n",
    "def mannWhitneyUTest(group1, group2, alpha=0.05, alternative='two-sided')->dict:\n",
    "    # Step 1: Perform the Mann-Whitney U Test\n",
    "    uStatistic, pValue = sciStats.mannwhitneyu(group1, group2, alternative=alternative)\n",
    "    \n",
    "    # Step 2: Interpret Results\n",
    "    if pValue < alpha:\n",
    "        result = \"Reject the null hypothesis (the groups have different distributions)\"\n",
    "    else:\n",
    "        result = \"Fail to reject the null hypothesis (no significant difference between the groups)\"\n",
    "    \n",
    "    return {\n",
    "        \"U-Statistic\": uStatistic,\n",
    "        \"P-Value\": pValue,\n",
    "        \"Result\": result\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "group1 = [55, 60, 65, 70, 75]  # Example non-parametric data\n",
    "group2 = [80, 85, 90, 95, 100]\n",
    "\n",
    "testResult = mannWhitneyUTest(group1, group2, alpha=0.05, alternative='two-sided')\n",
    "\n",
    "# Print the results\n",
    "for key, value in testResult.items():\n",
    "    print(f\"{key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Simulating Type-1 and Type-2 Errors for Hypothesis Tests**\n",
    "\n",
    "In hypothesis testing, **Type-1 and Type-2 errors** are crucial concepts that describe the accuracy of the test. Hereâ€™s how to understand and simulate them:\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Concepts**\n",
    "\n",
    "1. **Type-1 Error ($\\alpha$)**:\n",
    "   - Occurs when the null hypothesis ($H_0$) is rejected even though it is true.\n",
    "   - Probability of Type-1 error is equal to the significance level ($\\alpha$) of the test, usually 0.05.\n",
    "\n",
    "2. **Type-2 Error ($\\beta$)**:\n",
    "   - Occurs when the null hypothesis ($H_0$) is not rejected even though the alternative hypothesis ($H_1$) is true.\n",
    "   - $1 - \\beta$ is the **power of the test**, indicating its ability to detect a true effect.\n",
    "\n",
    "---\n",
    "\n",
    "### **Steps to Simulate Type-1 and Type-2 Errors**\n",
    "\n",
    "1. **Generate Data**:\n",
    "   - Simulate data from a distribution under the null hypothesis ($H_0$).\n",
    "   - Simulate data under the alternative hypothesis ($H_1$).\n",
    "\n",
    "2. **Conduct Hypothesis Testing**:\n",
    "   - Use a statistical test (e.g., t-test).\n",
    "   - Compare the p-value with the significance level ($\\alpha$) to make decisions.\n",
    "\n",
    "3. **Record Outcomes**:\n",
    "   - Record whether a Type-1 error occurs for $H_0$-data.\n",
    "   - Record whether a Type-2 error occurs for $H_1$-data.\n",
    "\n",
    "4. **Calculate Error Rates**:\n",
    "   - Type-1 error rate = (Type-1 errors) / (Total $H_0$-tests).\n",
    "   - Type-2 error rate = (Type-2 errors) / (Total $H_1$-tests).\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Points to Interpret**\n",
    "\n",
    "1. **Type-1 Error Rate**:\n",
    "   - Should be close to the predefined significance level ($\\alpha$).\n",
    "\n",
    "2. **Type-2 Error Rate**:\n",
    "   - Higher for smaller sample sizes or smaller effect sizes.\n",
    "   - Decreases as the sample size or effect size increases.\n",
    "\n",
    "3. **Power**:\n",
    "   - Power ($1 - \\beta$) increases with larger sample sizes and effect sizes.\n",
    "\n",
    "---\n",
    "\n",
    "### **Practical Applications**\n",
    "\n",
    "- **Type-1 Error**: Controlling false positives, e.g., in medical trials where rejecting a true null can lead to unnecessary treatments.\n",
    "- **Type-2 Error**: Minimizing false negatives, e.g., ensuring a test detects real effects when they exist.\n",
    "- **Power Analysis**: Used to design studies with adequate sample size for detecting true effects.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate Type-1 and Type-2 error for hypothesis tests\n",
    "\n",
    "\n",
    "def simulateType1Type2Errors(\n",
    "    sampleSize=30, \n",
    "    nSimulations=1000, \n",
    "    alpha=0.05, \n",
    "    effectSize=0.5\n",
    "):\n",
    "    # Step 1: Initialize counters for errors\n",
    "    type1Errors = 0\n",
    "    type2Errors = 0\n",
    "\n",
    "    # Step 2: Simulate for null hypothesis (H0 is true)\n",
    "    for _ in range(nSimulations):\n",
    "        # Generate two groups with the same mean under H0\n",
    "        group1 = np.random.normal(loc=0, scale=1, size=sampleSize)\n",
    "        group2 = np.random.normal(loc=0, scale=1, size=sampleSize)\n",
    "        _, pValue = sciStats.ttest_ind(group1, group2)\n",
    "\n",
    "        # Record Type-1 error if we reject H0\n",
    "        if pValue < alpha:\n",
    "            type1Errors += 1\n",
    "\n",
    "    # Step 3: Simulate for alternative hypothesis (H1 is true)\n",
    "    for _ in range(nSimulations):\n",
    "        # Generate two groups with different means under H1\n",
    "        group1 = np.random.normal(loc=0, scale=1, size=sampleSize)\n",
    "        group2 = np.random.normal(loc=effectSize, scale=1, size=sampleSize)\n",
    "        _, pValue = sciStats.ttest_ind(group1, group2)\n",
    "\n",
    "        # Record Type-2 error if we fail to reject H0\n",
    "        if pValue >= alpha:\n",
    "            type2Errors += 1\n",
    "\n",
    "    # Step 4: Calculate error rates\n",
    "    type1ErrorRate = type1Errors / nSimulations\n",
    "    type2ErrorRate = type2Errors / nSimulations\n",
    "\n",
    "    return {\n",
    "        \"Type-1 Error Rate (Î±)\": type1ErrorRate,\n",
    "        \"Type-2 Error Rate (Î²)\": type2ErrorRate,\n",
    "        \"Power (1 - Î²)\": 1 - type2ErrorRate\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "results = simulateType1Type2Errors(\n",
    "    sampleSize=30, \n",
    "    nSimulations=1000, \n",
    "    alpha=0.05, \n",
    "    effectSize=0.5\n",
    ")\n",
    "\n",
    "# Print the results\n",
    "for key, value in results.items():\n",
    "    print(f\"{key}: {value:.4f}\")\n",
    "\n",
    "# * second to execute - LOL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Computing p-values from Test Statistics**\n",
    "\n",
    "In hypothesis testing, the **p-value** is the probability of obtaining test results at least as extreme as the observed results, assuming the null hypothesis ($H_0$) is true. You can compute p-values from test statistics using the corresponding probability distribution (e.g., Normal, t, Chi-Square).\n",
    "\n",
    "---\n",
    "\n",
    "### **Steps to Compute p-values**\n",
    "\n",
    "1. **Determine the Type of Test**:\n",
    "   - Identify the appropriate distribution (e.g., normal, t-distribution, chi-square).\n",
    "\n",
    "2. **Obtain the Test Statistic**:\n",
    "   - Compute the test statistic using the formula for the specific hypothesis test.\n",
    "\n",
    "3. **Choose the Alternative Hypothesis**:\n",
    "   - **Two-tailed**: Tests for differences in both directions.\n",
    "   - **One-tailed**: Tests for differences in one direction (greater or less).\n",
    "\n",
    "4. **Compute the p-value**:\n",
    "   - Use the cumulative distribution function (CDF) of the relevant distribution to calculate the p-value.\n",
    "\n",
    "---\n",
    "\n",
    "### **Interpreting the p-value**\n",
    "\n",
    "1. **Compare p-value to Significance Level ($\\alpha$)**:\n",
    "   - If $p \\leq \\alpha$, reject the null hypothesis ($H_0$).\n",
    "   - If $p > \\alpha$, fail to reject $H_0$.\n",
    "\n",
    "2. **Common $\\alpha$ Levels**:\n",
    "   - $0.05$ (5% significance): Standard in many fields.\n",
    "   - $0.01$ (1% significance): Used for stricter confidence.\n",
    "\n",
    "---\n",
    "\n",
    "### **Examples of Use**\n",
    "\n",
    "1. **z-test**: Compare the mean of a sample to a population mean.\n",
    "2. **t-test**: Compare sample means when the sample size is small or the population standard deviation is unknown.\n",
    "3. **Chi-Square Test**: Test for independence or goodness of fit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute pValues from test statistics\n",
    "\n",
    "\n",
    "def pValueZTest(zStat, alternative=\"two-sided\"):\n",
    "    if alternative == \"two-sided\":\n",
    "        p_value = 2 * (1 - sciStats.norm.cdf(abs(zStat)))\n",
    "    elif alternative == \"greater\":\n",
    "        p_value = 1 -sciStats.norm.cdf(zStat)\n",
    "    elif alternative == \"less\":\n",
    "        p_value =sciStats.norm.cdf(zStat)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid alternative hypothesis. Choose 'two-sided', 'greater', or 'less'.\")\n",
    "    return p_value\n",
    "\n",
    "# Example usage\n",
    "zStat = 2.5  # Example test statistic\n",
    "pValue = pValueZTest(zStat, alternative=\"two-sided\")\n",
    "print(f\"p-value: {pValue:.4f}\")\n",
    "\n",
    "\n",
    "# ---\n",
    "\n",
    "\n",
    "def pValueTTest(tStat, df, alternative=\"two-sided\"):\n",
    "    if alternative == \"two-sided\":\n",
    "        p_value = 2 * (1 - sciStats.t.cdf(abs(tStat), df))\n",
    "    elif alternative == \"greater\":\n",
    "        p_value = 1 - sciStats.t.cdf(tStat, df)\n",
    "    elif alternative == \"less\":\n",
    "        p_value = sciStats.t.cdf(tStat, df)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid alternative hypothesis. Choose 'two-sided', 'greater', or 'less'.\")\n",
    "    return p_value\n",
    "\n",
    "# Example usage\n",
    "tStat = 2.0  # Example test statistic\n",
    "df = 10       # Degrees of freedom\n",
    "pValue = pValueTTest(tStat, df, alternative=\"two-sided\")\n",
    "print(f\"p-value: {pValue:.4f}\")\n",
    "\n",
    "\n",
    "# ----\n",
    "\n",
    "\n",
    "def pValueChi2Test(chi2Stat, df):\n",
    "    p_value = 1 - sciStats.chi2.cdf(chi2Stat, df)\n",
    "    return p_value\n",
    "\n",
    "# Example usage\n",
    "chi2Stat = 5.0  # Example test statistic\n",
    "df = 3           # Degrees of freedom\n",
    "pValue = pValueChi2Test(chi2Stat, df)\n",
    "print(f\"p-value: {pValue:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the rejection region of hypothesis test\n",
    "\n",
    "def rejectionRegionZTest(alpha=0.05, testType=\"two-tailed\"):\n",
    "    if testType == \"two-tailed\":\n",
    "        # Two-tailed critical values\n",
    "        lowerCritical = sciStats.norm.ppf(alpha / 2)\n",
    "        upperCritical = sciStats.norm.ppf(1 - alpha / 2)\n",
    "        return lowerCritical, upperCritical\n",
    "    elif testType == \"right-tailed\":\n",
    "        # One-tailed critical value (right)\n",
    "        criticalValue = sciStats.norm.ppf(1 - alpha)\n",
    "        return criticalValue\n",
    "    elif testType == \"left-tailed\":\n",
    "        # One-tailed critical value (left)\n",
    "        criticalValue =sciStats.norm.ppf(alpha)\n",
    "        return criticalValue\n",
    "    else:\n",
    "        raise ValueError(\"Invalid test type. Choose 'two-tailed', 'right-tailed', or 'left-tailed'.\")\n",
    "\n",
    "# Example usage\n",
    "alpha = 0.05\n",
    "testType = \"two-tailed\"\n",
    "criticalValues = rejectionRegionZTest(alpha, testType)\n",
    "\n",
    "print(f\"Rejection region (z-test, {testType}): {criticalValues}\")\n",
    "\n",
    "\n",
    "\n",
    "# ---\n",
    "\n",
    "\n",
    "def rejectionRegionTTest(alpha=0.05, df=10, testType=\"two-tailed\"):\n",
    "    if testType == \"two-tailed\":\n",
    "        # Two-tailed critical values\n",
    "        lowerCritical = sciStats.t.ppf(alpha / 2, df)\n",
    "        upperCritical = sciStats.t.ppf(1 - alpha / 2, df)\n",
    "        return lowerCritical, upperCritical\n",
    "    elif testType == \"right-tailed\":\n",
    "        # One-tailed critical value (right)\n",
    "        criticalValue = sciStats.t.ppf(1 - alpha, df)\n",
    "        return criticalValue\n",
    "    elif testType == \"left-tailed\":\n",
    "        # One-tailed critical value (left)\n",
    "        criticalValue = sciStats.t.ppf(alpha, df)\n",
    "        return criticalValue\n",
    "    else:\n",
    "        raise ValueError(\"Invalid test type. Choose 'two-tailed', 'right-tailed', or 'left-tailed'.\")\n",
    "\n",
    "# Example usage\n",
    "alpha = 0.05\n",
    "df = 10  # Degrees of freedom\n",
    "testType = \"two-tailed\"\n",
    "criticalValues = rejectionRegionTTest(alpha, df, testType)\n",
    "\n",
    "print(f\"Rejection region (t-test, {testType}, df={df}): {criticalValues}\")\n",
    "\n",
    "#---    \n",
    "\n",
    "\n",
    "def rejectionRegionChi2Test(alpha=0.05, df=10, testType=\"right-tailed\"):\n",
    "    if testType == \"right-tailed\":\n",
    "        # One-tailed critical value (right)\n",
    "        criticalValue = sciStats.chi2.ppf(1 - alpha, df)\n",
    "        return criticalValue\n",
    "    elif testType == \"left-tailed\":\n",
    "        # One-tailed critical value (left)\n",
    "        criticalValue = sciStats.chi2.ppf(alpha, df)\n",
    "        return criticalValue\n",
    "    else:\n",
    "        raise ValueError(\"Chi-square tests are usually one-tailed.\")\n",
    "\n",
    "# Example usage\n",
    "alpha = 0.05\n",
    "df = 10\n",
    "criticalValue = rejectionRegionChi2Test(alpha, df, testType=\"right-tailed\")\n",
    "\n",
    "print(f\"Rejection region (chi-square test, right-tailed, df={df}): Critical value = {criticalValue:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Hypothesis Test to Determine if a Dataset Follows a Normal Distribution**\n",
    "\n",
    "To determine if a dataset follows a normal distribution, you can perform a **Goodness-of-Fit test** using one of the following methods:\n",
    "\n",
    "1. **Shapiro-Wilk Test**:\n",
    "   - A statistical test that checks if a sample comes from a normally distributed population.\n",
    "   \n",
    "2. **Anderson-Darling Test**:\n",
    "   - A more general test for normality that is based on the empirical distribution function.\n",
    "   \n",
    "3. **Kolmogorov-Smirnov Test (KS Test)**:\n",
    "   - Compares the sample's cumulative distribution function (CDF) with the CDF of a normal distribution.\n",
    "\n",
    "4. **Chi-Square Goodness-of-Fit Test**:\n",
    "   - Compares the observed frequency distribution of data against the expected frequencies under the normal distribution.\n",
    "\n",
    "---\n",
    "\n",
    "### **Steps to Perform a Hypothesis Test for Normality**\n",
    "\n",
    "1. **State Hypotheses**:\n",
    "   - Null Hypothesis ($H_0$): The data follows a normal distribution.\n",
    "   - Alternative Hypothesis ($H_1$): The data does not follow a normal distribution.\n",
    "\n",
    "2. **Select Significance Level ($\\alpha$)**:\n",
    "   - Common choice is $\\alpha = 0.05$.\n",
    "\n",
    "3. **Select a Normality Test**:\n",
    "   - Choose one of the tests (Shapiro-Wilk, Anderson-Darling, KS test, etc.).\n",
    "\n",
    "4. **Perform the Test**:\n",
    "   - Compute the test statistic and corresponding p-value.\n",
    "\n",
    "5. **Interpret the p-value**:\n",
    "   - If the p-value $\\leq \\alpha$, reject the null hypothesis ($H_0$); conclude that the data does not follow a normal distribution.\n",
    "   - If the p-value $> \\alpha$, fail to reject $H_0$; conclude that there is insufficient evidence to say the data is not normal.\n",
    "\n",
    "---\n",
    "\n",
    "### **Interpreting Results**\n",
    "\n",
    "- **Shapiro-Wilk, Anderson-Darling, KS test**:\n",
    "  - **Null Hypothesis ($H_0$)**: The data follows a normal distribution.\n",
    "  - **Alternative Hypothesis ($H_1$)**: The data does not follow a normal distribution.\n",
    "  - If the **p-value** $> 0.05$, we fail to reject $H_0$ (the data is normal).\n",
    "  - If the **p-value** $ \\leq 0.05$, we reject $H_0$ (the data is not normal).\n",
    "\n",
    "- **Chi-Square Test**:\n",
    "  - **Null Hypothesis ($H_0$)**: The observed frequencies match the expected frequencies from a normal distribution.\n",
    "  - If the **p-value** $> 0.05$, we fail to reject $H_0$.\n",
    "  - If the **p-value** $ \\leq 0.05$, we reject $H_0$.\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**\n",
    "\n",
    "- You can use these tests to assess whether your dataset follows a normal distribution.\n",
    "- **Shapiro-Wilk** and **Anderson-Darling** are powerful tests specifically for normality.\n",
    "- **Chi-Square** and **Kolmogorov-Smirnov** can also be used but are less commonly applied for normality testing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform A hypothesis test to determine if a dataset follow normal Distribution\n",
    "\n",
    "\n",
    "def ShapiroWilk(data):\n",
    "    stat, pValue = sciStats.shapiro(data)\n",
    "    return stat, pValue\n",
    "\n",
    "# Example usage\n",
    "data = [4.5, 6.7, 8.8, 5.6, 7.9, 8.4, 6.1, 7.3]  # Example data\n",
    "stat, pValue = ShapiroWilk(data)\n",
    "\n",
    "print(f\"Shapiro-Wilk Test Statistic: {stat:.4f}, p-value: {pValue:.4f}\")\n",
    "if pValue > 0.05:\n",
    "    print(\"Fail to reject the null hypothesis: Data follows a normal distribution\")\n",
    "else:\n",
    "    print(\"Reject the null hypothesis: Data does not follow a normal distribution\")\n",
    "\n",
    "\n",
    "# -----\n",
    "\n",
    "\n",
    "\n",
    "def AndersonDarling(data):\n",
    "    result = sciStats.anderson(data)\n",
    "    return result.statistic, result.critical_values, result.significance_level\n",
    "\n",
    "# Example usage\n",
    "data = [4.5, 6.7, 8.8, 5.6, 7.9, 8.4, 6.1, 7.3]  # Example data\n",
    "stat, criticalValues, significanceLevel = AndersonDarling(data)\n",
    "\n",
    "print(f\"Anderson-Darling Test Statistic: {stat:.4f}\")\n",
    "for i in range(len(criticalValues)):\n",
    "    print(f\"Critical value for {significanceLevel[i]}% significance: {criticalValues[i]}\")\n",
    "    if stat > criticalValues[i]:\n",
    "        print(f\"Reject the null hypothesis at {significanceLevel[i]}% significance: Data does not follow a normal distribution\")\n",
    "\n",
    "\n",
    "\n",
    "# ----- \n",
    "\n",
    "\n",
    "def KolmogorovSmirnov(data):\n",
    "    stat, pValue = sciStats.kstest(data, sciStats.norm.cdf, args=(data.mean(), data.std()))\n",
    "    return stat, pValue\n",
    "\n",
    "# Example usage\n",
    "data = [4.5, 6.7, 8.8, 5.6, 7.9, 8.4, 6.1, 7.3]  # Example data\n",
    "stat, pValue = KolmogorovSmirnov(data)\n",
    "\n",
    "print(f\"Kolmogorov-Smirnov Test Statistic: {stat:.4f}, p-value: {pValue:.4f}\")\n",
    "if pValue > 0.05:\n",
    "    print(\"Fail to reject the null hypothesis: Data follows a normal distribution\")\n",
    "else:\n",
    "    print(\"Reject the null hypothesis: Data does not follow a normal distribution\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
